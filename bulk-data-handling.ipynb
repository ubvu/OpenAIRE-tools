{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAIRE Community Data Dump Handling: Extraction, Tranformation and Enrichment\n",
    "\n",
    "In this notebook we will start with one of the [OpenAIRE Community Subgraphs](https://graph.openaire.eu/docs/downloads/subgraphs) to enrich that informatino for further analysis.\n",
    "\n",
    "This data process will extract an [OpenAIRE community data dump from Zenodo](https://doi.org/10.5281/zenodo.3974604), transforms it in to a portable file format .parquet (and updatable with changes for time seires analysis), that can be used to query with DuckDB, to enrich this with additional data  (also in .parquet, for join queries).\n",
    "\n",
    "This additional data can be societal impact data from [Altmetric.com](https://details-page-api-docs.altmetric.com/) or [Overton.io](https://app.overton.io/swagger.php), Gender data using [genderize.io](https://genderize.io/documentation), sdg classification using [aurora-sdg](https://aurora-universities.eu/sdg-research/sdg-api/)\n",
    "\n",
    "This script needs to be written in a way so that it can run every month using  the latest data.\n",
    "\n",
    "## Processing steps\n",
    "\n",
    "* the folder ./data/ is put in .gitignore to prevent that bulk datais sent to a code repository. So make sure that folder exists, and mkdir if not exists. \n",
    "* The script downloads the lastest Data Dump Tar file from one selected community. See https://doi.org/10.5281/zenodo.3974604 for the latest list. In our case the Aurora tar file. https://zenodo.org/records/14887484/files/aurora.tar?download=1\n",
    "  * Use the json record of zenodo to get to the latest record, and fetch the download link of the aurora.tar file. for example : https://zenodo.org/records/14887484/export/json or https://zenodo.org/api/records/14887484/versions/latest \n",
    "  Make the tar filename a variable, so it can be used for multiple community dumps.\n",
    "  Download the tar file in a target folder ./data/{filename+timestamp}/ where a subfolder is created using the filename and the timestamp. Make this also as a  variable to use later on.\n",
    "* Extract the tar file, to the compressed .json.gz files and put these in target folder ./data/{filename+timestamp}/01-extracted/\n",
    "* Transform the compressed .json.gz files into a single .parquet file in target folder ./data/{filename+timestamp}/02-transformed/\n",
    "Use instructions in sections \"Processing JSON files with DuckDB\" and \"Full dataset, bit by bit\" and \"Splitting and Processing JSON Files in Batches\" https://github.com/mosart/OpenAIRE-tools/blob/main/duckdb-querying.ipynb to start with. (be aware of error messages, and fix the issues to get all the data in)\n",
    "* Extract the SQL schema (schema-datadump.sql) from the .parquet file and put it in target folder ./data/{filename+timestamp}/02-transformed/ This is needed for further processing of the records with DuckDB later on.\n",
    "Use instructions in section \"Extracting Schema from Parquet File\" https://github.com/mosart/OpenAIRE-tools/blob/main/duckdb-querying.ipynb to start with.\n",
    "* Query to get all identifiers: openaire id, doi, isbn, hdl, etc.\n",
    "* **Get Altmetric data:**\n",
    "* Extract the Altmetric data using the Identifiers. put that in target folder ./data/{filename+timestamp}/03-altmetric-extracted/\n",
    "* Transform the Altmetric data to a single .parquet file, with the identifiers. put that in target folder ./data/{filename+timestamp}/04-altmetric-transformed/ This way duckDB can make a join when querying over multiple parquet files.\n",
    "* Extract the SQL schema (schema-altmetric.sql) from the .parquet file and put it in target folder ./data/{filename+timestamp}/04-altmetric-transformed/\n",
    "* **Get Overton data:** Repeat the altmetric steps, bun than for Overton.\n",
    "* **Get Gender data** query for the Author names and country codes, and run them over the gerderize api\n",
    "* **Get SDG data** query for the abstracts, and run abstracs larger than 100 tokens over the aurora-SDG api.\n",
    "\n",
    "ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Get the latest Community Dump File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37 .tar files in the dataset.\n",
      "Details of .tar files:\n",
      "[{'filename': 'energy-planning_1.tar', 'size': '6.99 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/energy-planning_1.tar/content', 'checksum': 'md5:0a2f551db46a9e629bb1d0a0098ae5cd'}, {'filename': 'edih-adria_1.tar', 'size': '5.86 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/edih-adria_1.tar/content', 'checksum': 'md5:23559bed5a9023398b431777bdc8a126'}, {'filename': 'uarctic_1.tar', 'size': '9.75 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/uarctic_1.tar/content', 'checksum': 'md5:302e3844ebd041c5f4ed94505eb9a285'}, {'filename': 'netherlands_1.tar', 'size': '3.91 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/netherlands_1.tar/content', 'checksum': 'md5:d1416c058b3961483aac340750ea8726'}, {'filename': 'knowmad_1.tar', 'size': '10.08 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/knowmad_1.tar/content', 'checksum': 'md5:a79573a02f2c9a9d65c33b3f3a2eaab9'}, {'filename': 'argo-france.tar', 'size': '0.00 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/argo-france.tar/content', 'checksum': 'md5:2ce6b0fcc6f876b600207759a0dc9758'}, {'filename': 'civica.tar', 'size': '0.23 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/civica.tar/content', 'checksum': 'md5:d2f24bbef06809a91d124f0b07cb1034'}, {'filename': 'covid-19.tar', 'size': '2.03 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/covid-19.tar/content', 'checksum': 'md5:3b741e8138f39932ca6c13ca106fe5d3'}, {'filename': 'aurora.tar', 'size': '1.73 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/aurora.tar/content', 'checksum': 'md5:9b6a8f38cd6f0ce16a85dfc020c220bf'}, {'filename': 'dh-ch.tar', 'size': '1.16 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/dh-ch.tar/content', 'checksum': 'md5:dbebdcc8ad7fd1dc7894fe03ebe2a978'}, {'filename': 'heritage-science.tar', 'size': '0.03 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/heritage-science.tar/content', 'checksum': 'md5:ffd2537b08c58d78eea4bc23a99b3c07'}, {'filename': 'dth.tar', 'size': '0.01 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/dth.tar/content', 'checksum': 'md5:643894810ac8bfce0f8273cf40d05a7a'}, {'filename': 'egrise.tar', 'size': '0.02 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/egrise.tar/content', 'checksum': 'md5:2f52b49fa8bd983bcf6884d6c4f5e952'}, {'filename': 'lifewatch-eric.tar', 'size': '0.05 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/lifewatch-eric.tar/content', 'checksum': 'md5:213c3f1ac83454ec01560d683a26362d'}, {'filename': 'iperionhs.tar', 'size': '0.00 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/iperionhs.tar/content', 'checksum': 'md5:6ffaf325257d9af5e43daa68505b1797'}, {'filename': 'eutopia.tar', 'size': '1.60 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/eutopia.tar/content', 'checksum': 'md5:f9eb5a1bb86caf6a4f2a7563453fc6df'}, {'filename': 'sdsn-gr.tar', 'size': '0.05 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/sdsn-gr.tar/content', 'checksum': 'md5:696f8b509a12c0bc898e1b9040a53790'}, {'filename': 'north-american-studies.tar', 'size': '0.36 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/north-american-studies.tar/content', 'checksum': 'md5:b677758820184053d9c1e6714394dd70'}, {'filename': 'knowmad_3.tar', 'size': '10.06 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/knowmad_3.tar/content', 'checksum': 'md5:68c5839a6355ea26f979ba781bc26a56'}, {'filename': 'knowmad_2.tar', 'size': '10.07 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/knowmad_2.tar/content', 'checksum': 'md5:95442e14703ba5db573cbf12296d76f4'}, {'filename': 'knowmad_5.tar', 'size': '5.37 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/knowmad_5.tar/content', 'checksum': 'md5:35482782eb621df9ec7365d34ccf3d07'}, {'filename': 'knowmad_4.tar', 'size': '10.04 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/knowmad_4.tar/content', 'checksum': 'md5:d770fcd862d5d7d4d918c59f028e24da'}, {'filename': 'beopen.tar', 'size': '0.20 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/beopen.tar/content', 'checksum': 'md5:447dbe25eaedc20a75568fd340f8e25d'}, {'filename': 'dariah.tar', 'size': '0.02 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/dariah.tar/content', 'checksum': 'md5:814e3a79b29da6b605018009f8575a8c'}, {'filename': 'eu-conexus.tar', 'size': '0.18 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/eu-conexus.tar/content', 'checksum': 'md5:19c86e2b79bde505112fb6b3d6e38eef'}, {'filename': 'elixir-gr.tar', 'size': '0.01 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/elixir-gr.tar/content', 'checksum': 'md5:e1dfe593d40c498e31a6ff5132da5fa4'}, {'filename': 'eut.tar', 'size': '0.21 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/eut.tar/content', 'checksum': 'md5:3da085b997006205c694b023aefafe7c'}, {'filename': 'enermaps.tar', 'size': '1.59 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/enermaps.tar/content', 'checksum': 'md5:b157900f8cb97cf4aa4f82ef59e2ff6e'}, {'filename': 'forthem.tar', 'size': '0.91 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/forthem.tar/content', 'checksum': 'md5:7864a0ddb0b676bb053bbcdf12a12525'}, {'filename': 'inria.tar', 'size': '0.27 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/inria.tar/content', 'checksum': 'md5:66a7e88ddda08626cbf26e182f7eafea'}, {'filename': 'mes.tar', 'size': '0.38 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/mes.tar/content', 'checksum': 'md5:10362f805616e391d350b395d9ae30a3'}, {'filename': 'neanias-underwater.tar', 'size': '0.02 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/neanias-underwater.tar/content', 'checksum': 'md5:4422b9a3be4a1e6cad61da46044a0ca2'}, {'filename': 'neanias-atmospheric.tar', 'size': '0.57 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/neanias-atmospheric.tar/content', 'checksum': 'md5:a3b9edbd956aee813cdfe97655c3fc9e'}, {'filename': 'neanias-space.tar', 'size': '0.73 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/neanias-space.tar/content', 'checksum': 'md5:618fbecca154af288dc1c92246b3d73b'}, {'filename': 'rural-digital-europe.tar', 'size': '0.83 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/rural-digital-europe.tar/content', 'checksum': 'md5:6543f5ead539a8bbad04593b641af0a1'}, {'filename': 'tunet.tar', 'size': '0.05 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/tunet.tar/content', 'checksum': 'md5:178b5a944133ded65d741ea5dc2c5990'}, {'filename': 'ni.tar', 'size': '3.01 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/ni.tar/content', 'checksum': 'md5:e82f580135522acd2da7277ea9389718'}]\n",
      "Publication date: 2025-02-19\n",
      "DOI: 10.5281/zenodo.14887484\n",
      "Title: OpenAIRE Graph: Dataset for research communities and initiatives\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Fetch the JSON data from the URL\n",
    "url = \"https://zenodo.org/api/records/14887484/versions/latest\"\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "# Extract the files information\n",
    "files = data.get(\"files\", [])\n",
    "\n",
    "# Create a list of dictionaries for the .tar files\n",
    "tar_files = []\n",
    "for file in files:\n",
    "    if file[\"key\"].endswith(\".tar\"):\n",
    "        tar_files.append({\n",
    "            \"filename\": file[\"key\"],\n",
    "            \"size\": f\"{file['size'] / (1024**3):.2f} GB\",  # Convert bytes to GB\n",
    "            \"downloadlink\": file[\"links\"][\"self\"],\n",
    "            \"checksum\": file[\"checksum\"]\n",
    "        })\n",
    "\n",
    "# print the tar files\n",
    "# If no tar files found, print a message\n",
    "if not tar_files:\n",
    "    print(\"No .tar files found in the dataset.\")\n",
    "else:\n",
    "    print(f\"Found {len(tar_files)} .tar files in the dataset.\")\n",
    "    print(\"Details of .tar files:\")\n",
    "    print(tar_files)\n",
    "\n",
    "# get and print the publication date\n",
    "publication_date = data.get(\"metadata\", {}).get(\"publication_date\", \"Unknown\")\n",
    "print(f\"Publication date: {publication_date}\")\n",
    "# get and print the DOI\n",
    "doi = data.get(\"doi\", \"Unknown\")\n",
    "print(f\"DOI: {doi}\")\n",
    "# get and print the title\n",
    "title = data.get(\"title\", \"Unknown\")\n",
    "print(f\"Title: {title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      filename      size  \\\n",
      "5              argo-france.tar   0.00 GB   \n",
      "8                   aurora.tar   1.73 GB   \n",
      "22                  beopen.tar   0.20 GB   \n",
      "6                   civica.tar   0.23 GB   \n",
      "7                 covid-19.tar   2.03 GB   \n",
      "23                  dariah.tar   0.02 GB   \n",
      "9                    dh-ch.tar   1.16 GB   \n",
      "11                     dth.tar   0.01 GB   \n",
      "1             edih-adria_1.tar   5.86 GB   \n",
      "12                  egrise.tar   0.02 GB   \n",
      "25               elixir-gr.tar   0.01 GB   \n",
      "0        energy-planning_1.tar   6.99 GB   \n",
      "27                enermaps.tar   1.59 GB   \n",
      "24              eu-conexus.tar   0.18 GB   \n",
      "26                     eut.tar   0.21 GB   \n",
      "15                 eutopia.tar   1.60 GB   \n",
      "28                 forthem.tar   0.91 GB   \n",
      "10        heritage-science.tar   0.03 GB   \n",
      "29                   inria.tar   0.27 GB   \n",
      "14               iperionhs.tar   0.00 GB   \n",
      "4                knowmad_1.tar  10.08 GB   \n",
      "19               knowmad_2.tar  10.07 GB   \n",
      "18               knowmad_3.tar  10.06 GB   \n",
      "21               knowmad_4.tar  10.04 GB   \n",
      "20               knowmad_5.tar   5.37 GB   \n",
      "13          lifewatch-eric.tar   0.05 GB   \n",
      "30                     mes.tar   0.38 GB   \n",
      "32     neanias-atmospheric.tar   0.57 GB   \n",
      "33           neanias-space.tar   0.73 GB   \n",
      "31      neanias-underwater.tar   0.02 GB   \n",
      "3            netherlands_1.tar   3.91 GB   \n",
      "36                      ni.tar   3.01 GB   \n",
      "17  north-american-studies.tar   0.36 GB   \n",
      "34    rural-digital-europe.tar   0.83 GB   \n",
      "16                 sdsn-gr.tar   0.05 GB   \n",
      "35                   tunet.tar   0.05 GB   \n",
      "2                uarctic_1.tar   9.75 GB   \n",
      "\n",
      "                                         downloadlink  \\\n",
      "5   https://zenodo.org/api/records/14887484/files/...   \n",
      "8   https://zenodo.org/api/records/14887484/files/...   \n",
      "22  https://zenodo.org/api/records/14887484/files/...   \n",
      "6   https://zenodo.org/api/records/14887484/files/...   \n",
      "7   https://zenodo.org/api/records/14887484/files/...   \n",
      "23  https://zenodo.org/api/records/14887484/files/...   \n",
      "9   https://zenodo.org/api/records/14887484/files/...   \n",
      "11  https://zenodo.org/api/records/14887484/files/...   \n",
      "1   https://zenodo.org/api/records/14887484/files/...   \n",
      "12  https://zenodo.org/api/records/14887484/files/...   \n",
      "25  https://zenodo.org/api/records/14887484/files/...   \n",
      "0   https://zenodo.org/api/records/14887484/files/...   \n",
      "27  https://zenodo.org/api/records/14887484/files/...   \n",
      "24  https://zenodo.org/api/records/14887484/files/...   \n",
      "26  https://zenodo.org/api/records/14887484/files/...   \n",
      "15  https://zenodo.org/api/records/14887484/files/...   \n",
      "28  https://zenodo.org/api/records/14887484/files/...   \n",
      "10  https://zenodo.org/api/records/14887484/files/...   \n",
      "29  https://zenodo.org/api/records/14887484/files/...   \n",
      "14  https://zenodo.org/api/records/14887484/files/...   \n",
      "4   https://zenodo.org/api/records/14887484/files/...   \n",
      "19  https://zenodo.org/api/records/14887484/files/...   \n",
      "18  https://zenodo.org/api/records/14887484/files/...   \n",
      "21  https://zenodo.org/api/records/14887484/files/...   \n",
      "20  https://zenodo.org/api/records/14887484/files/...   \n",
      "13  https://zenodo.org/api/records/14887484/files/...   \n",
      "30  https://zenodo.org/api/records/14887484/files/...   \n",
      "32  https://zenodo.org/api/records/14887484/files/...   \n",
      "33  https://zenodo.org/api/records/14887484/files/...   \n",
      "31  https://zenodo.org/api/records/14887484/files/...   \n",
      "3   https://zenodo.org/api/records/14887484/files/...   \n",
      "36  https://zenodo.org/api/records/14887484/files/...   \n",
      "17  https://zenodo.org/api/records/14887484/files/...   \n",
      "34  https://zenodo.org/api/records/14887484/files/...   \n",
      "16  https://zenodo.org/api/records/14887484/files/...   \n",
      "35  https://zenodo.org/api/records/14887484/files/...   \n",
      "2   https://zenodo.org/api/records/14887484/files/...   \n",
      "\n",
      "                                checksum  \n",
      "5   md5:2ce6b0fcc6f876b600207759a0dc9758  \n",
      "8   md5:9b6a8f38cd6f0ce16a85dfc020c220bf  \n",
      "22  md5:447dbe25eaedc20a75568fd340f8e25d  \n",
      "6   md5:d2f24bbef06809a91d124f0b07cb1034  \n",
      "7   md5:3b741e8138f39932ca6c13ca106fe5d3  \n",
      "23  md5:814e3a79b29da6b605018009f8575a8c  \n",
      "9   md5:dbebdcc8ad7fd1dc7894fe03ebe2a978  \n",
      "11  md5:643894810ac8bfce0f8273cf40d05a7a  \n",
      "1   md5:23559bed5a9023398b431777bdc8a126  \n",
      "12  md5:2f52b49fa8bd983bcf6884d6c4f5e952  \n",
      "25  md5:e1dfe593d40c498e31a6ff5132da5fa4  \n",
      "0   md5:0a2f551db46a9e629bb1d0a0098ae5cd  \n",
      "27  md5:b157900f8cb97cf4aa4f82ef59e2ff6e  \n",
      "24  md5:19c86e2b79bde505112fb6b3d6e38eef  \n",
      "26  md5:3da085b997006205c694b023aefafe7c  \n",
      "15  md5:f9eb5a1bb86caf6a4f2a7563453fc6df  \n",
      "28  md5:7864a0ddb0b676bb053bbcdf12a12525  \n",
      "10  md5:ffd2537b08c58d78eea4bc23a99b3c07  \n",
      "29  md5:66a7e88ddda08626cbf26e182f7eafea  \n",
      "14  md5:6ffaf325257d9af5e43daa68505b1797  \n",
      "4   md5:a79573a02f2c9a9d65c33b3f3a2eaab9  \n",
      "19  md5:95442e14703ba5db573cbf12296d76f4  \n",
      "18  md5:68c5839a6355ea26f979ba781bc26a56  \n",
      "21  md5:d770fcd862d5d7d4d918c59f028e24da  \n",
      "20  md5:35482782eb621df9ec7365d34ccf3d07  \n",
      "13  md5:213c3f1ac83454ec01560d683a26362d  \n",
      "30  md5:10362f805616e391d350b395d9ae30a3  \n",
      "32  md5:a3b9edbd956aee813cdfe97655c3fc9e  \n",
      "33  md5:618fbecca154af288dc1c92246b3d73b  \n",
      "31  md5:4422b9a3be4a1e6cad61da46044a0ca2  \n",
      "3   md5:d1416c058b3961483aac340750ea8726  \n",
      "36  md5:e82f580135522acd2da7277ea9389718  \n",
      "17  md5:b677758820184053d9c1e6714394dd70  \n",
      "34  md5:6543f5ead539a8bbad04593b641af0a1  \n",
      "16  md5:696f8b509a12c0bc898e1b9040a53790  \n",
      "35  md5:178b5a944133ded65d741ea5dc2c5990  \n",
      "2   md5:302e3844ebd041c5f4ed94505eb9a285  \n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame to hold the tar files information for later use.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df_tar_files = pd.DataFrame(tar_files)\n",
    "\n",
    "# Sort the DataFrame by filename alphabetically\n",
    "df_tar_files = df_tar_files.sort_values(by='filename')\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df_tar_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tar files:\n",
      "    index                    filename      size\n",
      "0       5             argo-france.tar   0.00 GB\n",
      "1       8                  aurora.tar   1.73 GB\n",
      "2      22                  beopen.tar   0.20 GB\n",
      "3       6                  civica.tar   0.23 GB\n",
      "4       7                covid-19.tar   2.03 GB\n",
      "5      23                  dariah.tar   0.02 GB\n",
      "6       9                   dh-ch.tar   1.16 GB\n",
      "7      11                     dth.tar   0.01 GB\n",
      "8       1            edih-adria_1.tar   5.86 GB\n",
      "9      12                  egrise.tar   0.02 GB\n",
      "10     25               elixir-gr.tar   0.01 GB\n",
      "11      0       energy-planning_1.tar   6.99 GB\n",
      "12     27                enermaps.tar   1.59 GB\n",
      "13     24              eu-conexus.tar   0.18 GB\n",
      "14     26                     eut.tar   0.21 GB\n",
      "15     15                 eutopia.tar   1.60 GB\n",
      "16     28                 forthem.tar   0.91 GB\n",
      "17     10        heritage-science.tar   0.03 GB\n",
      "18     29                   inria.tar   0.27 GB\n",
      "19     14               iperionhs.tar   0.00 GB\n",
      "20      4               knowmad_1.tar  10.08 GB\n",
      "21     19               knowmad_2.tar  10.07 GB\n",
      "22     18               knowmad_3.tar  10.06 GB\n",
      "23     21               knowmad_4.tar  10.04 GB\n",
      "24     20               knowmad_5.tar   5.37 GB\n",
      "25     13          lifewatch-eric.tar   0.05 GB\n",
      "26     30                     mes.tar   0.38 GB\n",
      "27     32     neanias-atmospheric.tar   0.57 GB\n",
      "28     33           neanias-space.tar   0.73 GB\n",
      "29     31      neanias-underwater.tar   0.02 GB\n",
      "30      3           netherlands_1.tar   3.91 GB\n",
      "31     36                      ni.tar   3.01 GB\n",
      "32     17  north-american-studies.tar   0.36 GB\n",
      "33     34    rural-digital-europe.tar   0.83 GB\n",
      "34     16                 sdsn-gr.tar   0.05 GB\n",
      "35     35                   tunet.tar   0.05 GB\n",
      "36      2               uarctic_1.tar   9.75 GB\n",
      "Selected file: aurora.tar\n",
      "Download link: https://zenodo.org/api/records/14887484/files/aurora.tar/content\n",
      "Checksum: md5:9b6a8f38cd6f0ce16a85dfc020c220bf\n"
     ]
    }
   ],
   "source": [
    "import signal\n",
    "\n",
    "# Function to handle timeout\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutError\n",
    "\n",
    "# Set the timeout handler for the input\n",
    "signal.signal(signal.SIGALRM, timeout_handler)\n",
    "signal.alarm(10)  # Set the timeout to 10 seconds\n",
    "\n",
    "try:\n",
    "    # Ask the user to select a tar file by its index\n",
    "    print(\"Available tar files:\")\n",
    "    print(df_tar_files[['filename', 'size']].reset_index())\n",
    "    selected_index = int(input(\"Enter the index of the tar file you want to download: \"))\n",
    "except TimeoutError:\n",
    "    print(\"No response received. Defaulting to index 1.\")\n",
    "    selected_index = 1\n",
    "finally:\n",
    "    signal.alarm(0)  # Disable the alarm\n",
    "\n",
    "# Get the selected tar file's download link and checksum\n",
    "selected_file = df_tar_files.iloc[selected_index]\n",
    "downloadlink = selected_file['downloadlink']\n",
    "checksum = selected_file['checksum']\n",
    "\n",
    "print(f\"Selected file: {selected_file['filename']}\")\n",
    "print(f\"Download link: {downloadlink}\")\n",
    "print(f\"Checksum: {checksum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download Path File: ./data/01_input/aurora.tar\n",
      "Extraction Path Folder: ./data/02_extracted\n"
     ]
    }
   ],
   "source": [
    "# Path Variables\n",
    "\n",
    "# Path to save the downloaded tar file using file_name variable\n",
    "download_path = f\"./data/01_input/{selected_file['filename']}\"\n",
    "\n",
    "# Path to save the extracted files\n",
    "extraction_path = \"./data/02_extracted\"\n",
    "\n",
    "print(f\"Download Path File: {download_path}\")\n",
    "print(f\"Extraction Path Folder: {extraction_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the tar file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists: ./data/01_input/aurora.tar\n",
      "Download complete: ./data/01_input/aurora.tar\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Ensure the directory for the download path exists\n",
    "os.makedirs(os.path.dirname(download_path), exist_ok=True)\n",
    "\n",
    "# Check if the file already exists\n",
    "if not os.path.exists(download_path):\n",
    "    # Get the file size in bytes\n",
    "    file_size_bytes = int(selected_file['size'].split()[0]) * (1024**3)  # Convert GB to bytes\n",
    "    print(f\"Downloading file: {selected_file['filename']} ({selected_file['size']})\")\n",
    "    \n",
    "    # Estimate download duration assuming an average speed of 10 MB/s\n",
    "    avg_speed = 10 * (1024**2)  # 10 MB/s in bytes\n",
    "    estimated_duration = file_size_bytes / avg_speed\n",
    "    print(f\"Estimated download time: {estimated_duration:.2f} seconds\")\n",
    "    \n",
    "    # Download the selected tar file\n",
    "    response = requests.get(downloadlink, stream=True)\n",
    "    with open(download_path, 'wb') as file:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            file.write(chunk)\n",
    "else:\n",
    "    print(f\"File already exists: {download_path}\")\n",
    "\n",
    "print(f\"Download complete: {download_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checksum verification passed.\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "# Function to calculate the checksum of a file\n",
    "def calculate_checksum(file_path, algorithm):\n",
    "    hash_func = hashlib.new(algorithm)\n",
    "    with open(file_path, 'rb') as f:\n",
    "        while chunk := f.read(8192):\n",
    "            hash_func.update(chunk)\n",
    "    return hash_func.hexdigest()\n",
    "\n",
    "# Extract the checksum algorithm and value\n",
    "checksum_parts = checksum.split(':', 1)\n",
    "checksum_algorithm = checksum_parts[0]\n",
    "expected_checksum = checksum_parts[1]\n",
    "\n",
    "# Calculate the checksum of the downloaded file\n",
    "calculated_checksum = calculate_checksum(download_path, algorithm=checksum_algorithm)\n",
    "\n",
    "# Compare the calculated checksum with the provided checksum\n",
    "if calculated_checksum == expected_checksum:\n",
    "    print(\"Checksum verification passed.\")\n",
    "else:\n",
    "    print(\"Checksum verification failed.\")\n",
    "    print(f\"Expected: {expected_checksum}\")\n",
    "    print(f\"Calculated: {calculated_checksum}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract the tar file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tar file has already been extracted.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if the extraction directory already exists and contains files\n",
    "if os.path.exists(extraction_path) and os.listdir(extraction_path):\n",
    "    print(\"The tar file has already been extracted.\")\n",
    "else:\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(extraction_path, exist_ok=True)\n",
    "\n",
    "    # Extract the tar file\n",
    "    with tarfile.open(download_path, 'r') as tar:\n",
    "        tar.extractall(path=extraction_path)\n",
    "\n",
    "    print(\"Extraction complete.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 1\n",
      "First 5 files:\n",
      "aurora\n",
      "Subdirectories:\n",
      "aurora\n",
      "Latest subdirectory: aurora\n",
      "Latest extraction path: ./data/02_extracted/aurora\n",
      "  filename  is_directory\n",
      "0   aurora          True\n"
     ]
    }
   ],
   "source": [
    "# List the extracted files\n",
    "extracted_files = os.listdir(extraction_path)\n",
    "\n",
    "# count he number of files in the extracted folder\n",
    "num_files = len(extracted_files)\n",
    "print(f\"Number of files: {num_files}\")\n",
    "\n",
    "# print the first 5 files\n",
    "print(\"First 5 files:\")\n",
    "for file in extracted_files[:5]:\n",
    "    print(file) \n",
    "\n",
    "# print the added subdirectories\n",
    "subdirectories = [file for file in extracted_files if os.path.isdir(os.path.join(extraction_path, file))]\n",
    "print(\"Subdirectories:\")\n",
    "for subdirectory in subdirectories:\n",
    "    print(subdirectory)\n",
    "\n",
    "# print the latest added subdirectory based on date modified\n",
    "latest_subdirectory = sorted(subdirectories, key=lambda x: os.path.getmtime(os.path.join(extraction_path, x)))[-1]\n",
    "print(f\"Latest subdirectory: {latest_subdirectory}\")\n",
    "\n",
    "# make varable for the path to the latest subdirectory\n",
    "latest_extraction_path = os.path.join(extraction_path, latest_subdirectory)\n",
    "\n",
    "# print the path of the latest extraction path\n",
    "print(f\"Latest extraction path: {latest_extraction_path}\")\n",
    "\n",
    "# make a DataFrame for the extracted files\n",
    "df_extracted_files = pd.DataFrame({\n",
    "    'filename': extracted_files,\n",
    "    'is_directory': [os.path.isdir(os.path.join(extraction_path, f)) for f in extracted_files]\n",
    "})\n",
    "# Sort the DataFrame by filename\n",
    "df_extracted_files = df_extracted_files.sort_values(by='filename')\n",
    "# Print the DataFrame\n",
    "print(df_extracted_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Get a data sample to generate parquetfile and the SQL schema\n",
    "We do this before we process the bulk of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output file path: ./data/03_transformed/aurora-sample.parquet\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Ensure the target directory exists\n",
    "os.makedirs(\"./data/03_transformed\", exist_ok=True)\n",
    "\n",
    "# Define the target output file path\n",
    "output_file = f\"./data/03_transformed/{latest_subdirectory}-sample.parquet\"\n",
    "\n",
    "#print the output file path\n",
    "print(f\"Output file path: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below was my original starting point but it failed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Use DuckDB to process the extracted JSON files and save a sample of 1 rows as a Parquet file\n",
    "con.sql(f'''\n",
    "    COPY (\n",
    "        SELECT unnest(items, max_depth:=2)\n",
    "        FROM (\n",
    "            SELECT *\n",
    "            FROM read_json('{latest_extraction_path}/*.json.gz', sample_size=1, union_by_name=true)\n",
    "        )\n",
    "    )\n",
    "    TO '{output_file}' (FORMAT parquet, COMPRESSION gzip)\n",
    "''')\n",
    "\n",
    "print(f\"Transformed data saved to: {output_file}\")\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is my next itteration based on debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing file part-00000-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00001-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00002-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00003-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00004-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00005-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00006-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00007-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00008-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00009-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00010-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00011-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00012-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00013-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00014-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00015-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00016-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00017-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00018-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00019-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00020-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00021-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00022-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00023-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00024-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00025-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00026-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00027-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00028-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00029-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00030-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Invalid Input Error: Malformed JSON in file \"./data/02_extracted/aurora/part-00030-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz\", at byte 91219 in line 1194: unexpected end of data. \n",
      "Error processing file part-00031-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00032-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00033-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00034-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00035-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00036-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00037-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00038-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00039-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00040-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00041-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00042-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00043-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00044-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00045-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00046-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00047-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00048-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00049-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00050-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00051-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00052-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00053-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00054-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00055-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00056-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00057-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00058-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00059-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00060-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00061-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00062-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00063-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00064-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00065-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00066-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00067-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00068-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00069-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00070-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00071-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00072-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00073-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00074-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00075-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00076-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00077-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00078-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00079-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00080-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00081-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00082-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00083-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00084-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00085-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00086-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00087-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00088-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00089-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00090-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00091-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00092-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00093-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00094-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00095-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00096-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00097-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00098-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00099-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00100-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00101-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00102-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00103-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00104-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00105-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00106-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00107-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00108-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00109-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00110-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00111-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00112-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00113-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00114-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00115-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00116-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00117-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00118-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00119-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00120-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00121-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00122-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00123-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00124-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00125-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00126-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00127-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00128-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00129-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00130-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00131-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00132-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n",
      "Error processing file part-00133-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz: Binder Error: table temp_table has 1 columns but 32 values were supplied\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBinderException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Attempt to read the JSON file and append its data to the temporary table\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     \u001b[43mcon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;43m        INSERT INTO temp_table\u001b[39;49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;43m        SELECT *\u001b[39;49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;43m        FROM read_json_auto(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mgz_file_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, compression=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgzip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, union_by_name=true)\u001b[39;49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully processed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgz_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mBinderException\u001b[0m: Binder Error: table temp_table has 1 columns but 32 values were supplied",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m gz_file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(latest_extraction_path, gz_file)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Attempt to read the JSON file and append its data to the temporary table\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     \u001b[43mcon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;43m        INSERT INTO temp_table\u001b[39;49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;43m        SELECT *\u001b[39;49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;43m        FROM read_json_auto(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mgz_file_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, compression=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgzip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, union_by_name=true)\u001b[39;49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully processed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgz_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Log the error and skip the file\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Get the list of gzipped JSON files in the latest_extraction_path\n",
    "gz_files = [file for file in os.listdir(latest_extraction_path) if file.endswith(\".json.gz\")]\n",
    "\n",
    "# Create a temporary table to store data from valid files\n",
    "con.execute(\"CREATE OR REPLACE TEMP TABLE temp_table AS SELECT NULL AS dummy_column WHERE FALSE\")\n",
    "\n",
    "# Process each file individually\n",
    "for gz_file in gz_files:\n",
    "    gz_file_path = os.path.join(latest_extraction_path, gz_file)\n",
    "    try:\n",
    "        # Attempt to read the JSON file and append its data to the temporary table\n",
    "        con.execute(f\"\"\"\n",
    "            INSERT INTO temp_table\n",
    "            SELECT *\n",
    "            FROM read_json_auto('{gz_file_path}', compression='gzip', union_by_name=true)\n",
    "        \"\"\")\n",
    "        print(f\"Successfully processed: {gz_file}\")\n",
    "    except Exception as e:\n",
    "        # Log the error and skip the file\n",
    "        print(f\"Error processing file {gz_file}: {e}\")\n",
    "\n",
    "# Save the combined data from valid files to a Parquet file\n",
    "con.execute(f\"\"\"\n",
    "    COPY temp_table\n",
    "    TO '{output_file}' (FORMAT parquet, COMPRESSION gzip)\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Transformed data saved to: {output_file}\")\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Use DuckDB to process the extracted JSON files and save a sample of 100 rows as a Parquet file\n",
    "con.sql(f'''\n",
    "    COPY (\n",
    "        SELECT *\n",
    "        FROM read_json_auto('{latest_extraction_path}/*.json.gz', sample_size=1, compression='gzip', union_by_name=true)\n",
    "    )\n",
    "    TO '{output_file}' (FORMAT parquet, COMPRESSION gzip)\n",
    "''')\n",
    "\n",
    "print(f\"Transformed data saved to: {output_file}\")\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the target directory exists\n",
    "os.makedirs(\"./data/03_transformed\", exist_ok=True)\n",
    "\n",
    "# Define the target output file path using the latest_subdirectory variable\n",
    "output_file = f\"./data/03_transformed/{latest_subdirectory}-sample.parquet\"\n",
    "\n",
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Get the first 5 gzipped JSON files from the latest_extraction_path\n",
    "gz_files = [file for file in os.listdir(latest_extraction_path) if file.endswith(\".gz\")][:5]\n",
    "\n",
    "# Use DuckDB to process the selected JSON files and save as a Parquet file\n",
    "for gz_file in gz_files:\n",
    "    gz_file_path = os.path.join(latest_extraction_path, gz_file)\n",
    "    con.sql(f'''\n",
    "        COPY (\n",
    "            SELECT unnest(items, max_depth:=2)\n",
    "            FROM (\n",
    "                SELECT *\n",
    "                FROM read_json_auto('{gz_file_path}', compression='gzip')\n",
    "            )\n",
    "        )\n",
    "        TO '{output_file}' (FORMAT parquet, COMPRESSION gzip)\n",
    "    ''')\n",
    "\n",
    "print(f\"Transformed sample data saved to: {output_file}\")\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I WAS HERE REVIEWING THE CODE everything below was not checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import os\n",
    "\n",
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Get the first 3 gzipped JSON files from the latest_extraction_path\n",
    "gz_files = [file for file in os.listdir(latest_extraction_path) if file.endswith(\".gz\")][:3]\n",
    "\n",
    "# Load the gzipped JSON files directly into DuckDB\n",
    "for gz_file in gz_files:\n",
    "    gz_file_path = os.path.join(latest_extraction_path, gz_file)\n",
    "    con.execute(\"CREATE OR REPLACE TEMP TABLE temp_table AS SELECT * FROM read_json_auto(?, compression='gzip')\", [gz_file_path])\n",
    "\n",
    "# Generate the SQL schema from the temporary table\n",
    "schema = con.execute(\"DESCRIBE temp_table\").fetchall()\n",
    "\n",
    "# Print the schema\n",
    "print(\"SQL Schema:\")\n",
    "for column in schema:\n",
    "    print(f\"{column[0]}: {column[1]}\")\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
