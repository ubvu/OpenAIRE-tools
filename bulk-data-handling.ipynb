{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAIRE Community Data Dump Handling: Extraction, Tranformation and Enrichment\n",
    "\n",
    "In this notebook we will start with one of the [OpenAIRE Community Subgraphs](https://graph.openaire.eu/docs/downloads/subgraphs) to enrich that informatino for further analysis.\n",
    "\n",
    "This data process will extract an [OpenAIRE community data dump from Zenodo](https://doi.org/10.5281/zenodo.3974604), transforms it in to a portable file format .parquet (and updatable with changes for time seires analysis), that can be used to query with DuckDB, to enrich this with additional data  (also in .parquet, for join queries).\n",
    "\n",
    "This additional data can be societal impact data from [Altmetric.com](https://details-page-api-docs.altmetric.com/) or [Overton.io](https://app.overton.io/swagger.php), Gender data using [genderize.io](https://genderize.io/documentation), sdg classification using [aurora-sdg](https://aurora-universities.eu/sdg-research/sdg-api/)\n",
    "\n",
    "This script needs to be written in a way so that it can run every month using  the latest data.\n",
    "\n",
    "## Processing steps\n",
    "\n",
    "* the folder ./data/ is put in .gitignore to prevent that bulk datais sent to a code repository. So make sure that folder exists, and mkdir if not exists. \n",
    "* The script downloads the lastest Data Dump Tar file from one selected community. See https://doi.org/10.5281/zenodo.3974604 for the latest list. In our case the Aurora tar file. https://zenodo.org/records/14887484/files/aurora.tar?download=1\n",
    "  * Use the json record of zenodo to get to the latest record, and fetch the download link of the aurora.tar file. for example : https://zenodo.org/records/14887484/export/json or https://zenodo.org/api/records/14887484/versions/latest \n",
    "  Make the tar filename a variable, so it can be used for multiple community dumps.\n",
    "  Download the tar file in a target folder ./data/{filename+timestamp}/ where a subfolder is created using the filename and the timestamp. Make this also as a  variable to use later on.\n",
    "* Extract the tar file, to the compressed .json.gz files and put these in target folder ./data/{filename+timestamp}/01-extracted/\n",
    "* Transform the compressed .json.gz files into a single .parquet file in target folder ./data/{filename+timestamp}/02-transformed/\n",
    "Use instructions in sections \"Processing JSON files with DuckDB\" and \"Full dataset, bit by bit\" and \"Splitting and Processing JSON Files in Batches\" https://github.com/mosart/OpenAIRE-tools/blob/main/duckdb-querying.ipynb to start with. (be aware of error messages, and fix the issues to get all the data in)\n",
    "* Extract the SQL schema (schema-datadump.sql) from the .parquet file and put it in target folder ./data/{filename+timestamp}/02-transformed/ This is needed for further processing of the records with DuckDB later on.\n",
    "Use instructions in section \"Extracting Schema from Parquet File\" https://github.com/mosart/OpenAIRE-tools/blob/main/duckdb-querying.ipynb to start with.\n",
    "* Query to get all identifiers: openaire id, doi, isbn, hdl, etc.\n",
    "* **Get Altmetric data:**\n",
    "* Extract the Altmetric data using the Identifiers. put that in target folder ./data/{filename+timestamp}/03-altmetric-extracted/\n",
    "* Transform the Altmetric data to a single .parquet file, with the identifiers. put that in target folder ./data/{filename+timestamp}/04-altmetric-transformed/ This way duckDB can make a join when querying over multiple parquet files.\n",
    "* Extract the SQL schema (schema-altmetric.sql) from the .parquet file and put it in target folder ./data/{filename+timestamp}/04-altmetric-transformed/\n",
    "* **Get Overton data:** Repeat the altmetric steps, bun than for Overton.\n",
    "* **Get Gender data** query for the Author names and country codes, and run them over the gerderize api\n",
    "* **Get SDG data** query for the abstracts, and run abstracs larger than 100 tokens over the aurora-SDG api.\n",
    "\n",
    "ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Get the latest Community Dump File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37 .tar files in the dataset.\n",
      "Details of .tar files:\n",
      "[{'filename': 'energy-planning_1.tar', 'size': '6.99 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/energy-planning_1.tar/content', 'checksum': 'md5:0a2f551db46a9e629bb1d0a0098ae5cd'}, {'filename': 'edih-adria_1.tar', 'size': '5.86 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/edih-adria_1.tar/content', 'checksum': 'md5:23559bed5a9023398b431777bdc8a126'}, {'filename': 'uarctic_1.tar', 'size': '9.75 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/uarctic_1.tar/content', 'checksum': 'md5:302e3844ebd041c5f4ed94505eb9a285'}, {'filename': 'netherlands_1.tar', 'size': '3.91 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/netherlands_1.tar/content', 'checksum': 'md5:d1416c058b3961483aac340750ea8726'}, {'filename': 'knowmad_1.tar', 'size': '10.08 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/knowmad_1.tar/content', 'checksum': 'md5:a79573a02f2c9a9d65c33b3f3a2eaab9'}, {'filename': 'argo-france.tar', 'size': '0.00 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/argo-france.tar/content', 'checksum': 'md5:2ce6b0fcc6f876b600207759a0dc9758'}, {'filename': 'civica.tar', 'size': '0.23 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/civica.tar/content', 'checksum': 'md5:d2f24bbef06809a91d124f0b07cb1034'}, {'filename': 'covid-19.tar', 'size': '2.03 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/covid-19.tar/content', 'checksum': 'md5:3b741e8138f39932ca6c13ca106fe5d3'}, {'filename': 'aurora.tar', 'size': '1.73 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/aurora.tar/content', 'checksum': 'md5:9b6a8f38cd6f0ce16a85dfc020c220bf'}, {'filename': 'dh-ch.tar', 'size': '1.16 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/dh-ch.tar/content', 'checksum': 'md5:dbebdcc8ad7fd1dc7894fe03ebe2a978'}, {'filename': 'heritage-science.tar', 'size': '0.03 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/heritage-science.tar/content', 'checksum': 'md5:ffd2537b08c58d78eea4bc23a99b3c07'}, {'filename': 'dth.tar', 'size': '0.01 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/dth.tar/content', 'checksum': 'md5:643894810ac8bfce0f8273cf40d05a7a'}, {'filename': 'egrise.tar', 'size': '0.02 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/egrise.tar/content', 'checksum': 'md5:2f52b49fa8bd983bcf6884d6c4f5e952'}, {'filename': 'lifewatch-eric.tar', 'size': '0.05 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/lifewatch-eric.tar/content', 'checksum': 'md5:213c3f1ac83454ec01560d683a26362d'}, {'filename': 'iperionhs.tar', 'size': '0.00 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/iperionhs.tar/content', 'checksum': 'md5:6ffaf325257d9af5e43daa68505b1797'}, {'filename': 'eutopia.tar', 'size': '1.60 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/eutopia.tar/content', 'checksum': 'md5:f9eb5a1bb86caf6a4f2a7563453fc6df'}, {'filename': 'sdsn-gr.tar', 'size': '0.05 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/sdsn-gr.tar/content', 'checksum': 'md5:696f8b509a12c0bc898e1b9040a53790'}, {'filename': 'north-american-studies.tar', 'size': '0.36 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/north-american-studies.tar/content', 'checksum': 'md5:b677758820184053d9c1e6714394dd70'}, {'filename': 'knowmad_3.tar', 'size': '10.06 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/knowmad_3.tar/content', 'checksum': 'md5:68c5839a6355ea26f979ba781bc26a56'}, {'filename': 'knowmad_2.tar', 'size': '10.07 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/knowmad_2.tar/content', 'checksum': 'md5:95442e14703ba5db573cbf12296d76f4'}, {'filename': 'knowmad_5.tar', 'size': '5.37 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/knowmad_5.tar/content', 'checksum': 'md5:35482782eb621df9ec7365d34ccf3d07'}, {'filename': 'knowmad_4.tar', 'size': '10.04 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/knowmad_4.tar/content', 'checksum': 'md5:d770fcd862d5d7d4d918c59f028e24da'}, {'filename': 'beopen.tar', 'size': '0.20 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/beopen.tar/content', 'checksum': 'md5:447dbe25eaedc20a75568fd340f8e25d'}, {'filename': 'dariah.tar', 'size': '0.02 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/dariah.tar/content', 'checksum': 'md5:814e3a79b29da6b605018009f8575a8c'}, {'filename': 'eu-conexus.tar', 'size': '0.18 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/eu-conexus.tar/content', 'checksum': 'md5:19c86e2b79bde505112fb6b3d6e38eef'}, {'filename': 'elixir-gr.tar', 'size': '0.01 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/elixir-gr.tar/content', 'checksum': 'md5:e1dfe593d40c498e31a6ff5132da5fa4'}, {'filename': 'eut.tar', 'size': '0.21 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/eut.tar/content', 'checksum': 'md5:3da085b997006205c694b023aefafe7c'}, {'filename': 'enermaps.tar', 'size': '1.59 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/enermaps.tar/content', 'checksum': 'md5:b157900f8cb97cf4aa4f82ef59e2ff6e'}, {'filename': 'forthem.tar', 'size': '0.91 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/forthem.tar/content', 'checksum': 'md5:7864a0ddb0b676bb053bbcdf12a12525'}, {'filename': 'inria.tar', 'size': '0.27 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/inria.tar/content', 'checksum': 'md5:66a7e88ddda08626cbf26e182f7eafea'}, {'filename': 'mes.tar', 'size': '0.38 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/mes.tar/content', 'checksum': 'md5:10362f805616e391d350b395d9ae30a3'}, {'filename': 'neanias-underwater.tar', 'size': '0.02 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/neanias-underwater.tar/content', 'checksum': 'md5:4422b9a3be4a1e6cad61da46044a0ca2'}, {'filename': 'neanias-atmospheric.tar', 'size': '0.57 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/neanias-atmospheric.tar/content', 'checksum': 'md5:a3b9edbd956aee813cdfe97655c3fc9e'}, {'filename': 'neanias-space.tar', 'size': '0.73 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/neanias-space.tar/content', 'checksum': 'md5:618fbecca154af288dc1c92246b3d73b'}, {'filename': 'rural-digital-europe.tar', 'size': '0.83 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/rural-digital-europe.tar/content', 'checksum': 'md5:6543f5ead539a8bbad04593b641af0a1'}, {'filename': 'tunet.tar', 'size': '0.05 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/tunet.tar/content', 'checksum': 'md5:178b5a944133ded65d741ea5dc2c5990'}, {'filename': 'ni.tar', 'size': '3.01 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/ni.tar/content', 'checksum': 'md5:e82f580135522acd2da7277ea9389718'}]\n",
      "Publication date: 2025-02-19\n",
      "DOI: 10.5281/zenodo.14887484\n",
      "Title: OpenAIRE Graph: Dataset for research communities and initiatives\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Fetch the JSON data from the URL\n",
    "url = \"https://zenodo.org/api/records/14887484/versions/latest\"\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "# Extract the files information\n",
    "files = data.get(\"files\", [])\n",
    "\n",
    "# Create a list of dictionaries for the .tar files\n",
    "tar_files = []\n",
    "for file in files:\n",
    "    if file[\"key\"].endswith(\".tar\"):\n",
    "        tar_files.append({\n",
    "            \"filename\": file[\"key\"],\n",
    "            \"size\": f\"{file['size'] / (1024**3):.2f} GB\",  # Convert bytes to GB\n",
    "            \"downloadlink\": file[\"links\"][\"self\"],\n",
    "            \"checksum\": file[\"checksum\"]\n",
    "        })\n",
    "\n",
    "# print the tar files\n",
    "# If no tar files found, print a message\n",
    "if not tar_files:\n",
    "    print(\"No .tar files found in the dataset.\")\n",
    "else:\n",
    "    print(f\"Found {len(tar_files)} .tar files in the dataset.\")\n",
    "    print(\"Details of .tar files:\")\n",
    "    print(tar_files)\n",
    "\n",
    "# get and print the publication date\n",
    "publication_date = data.get(\"metadata\", {}).get(\"publication_date\", \"Unknown\")\n",
    "print(f\"Publication date: {publication_date}\")\n",
    "# get and print the DOI\n",
    "doi = data.get(\"doi\", \"Unknown\")\n",
    "print(f\"DOI: {doi}\")\n",
    "# get and print the title\n",
    "title = data.get(\"title\", \"Unknown\")\n",
    "print(f\"Title: {title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      filename      size  \\\n",
      "5              argo-france.tar   0.00 GB   \n",
      "8                   aurora.tar   1.73 GB   \n",
      "22                  beopen.tar   0.20 GB   \n",
      "6                   civica.tar   0.23 GB   \n",
      "7                 covid-19.tar   2.03 GB   \n",
      "23                  dariah.tar   0.02 GB   \n",
      "9                    dh-ch.tar   1.16 GB   \n",
      "11                     dth.tar   0.01 GB   \n",
      "1             edih-adria_1.tar   5.86 GB   \n",
      "12                  egrise.tar   0.02 GB   \n",
      "25               elixir-gr.tar   0.01 GB   \n",
      "0        energy-planning_1.tar   6.99 GB   \n",
      "27                enermaps.tar   1.59 GB   \n",
      "24              eu-conexus.tar   0.18 GB   \n",
      "26                     eut.tar   0.21 GB   \n",
      "15                 eutopia.tar   1.60 GB   \n",
      "28                 forthem.tar   0.91 GB   \n",
      "10        heritage-science.tar   0.03 GB   \n",
      "29                   inria.tar   0.27 GB   \n",
      "14               iperionhs.tar   0.00 GB   \n",
      "4                knowmad_1.tar  10.08 GB   \n",
      "19               knowmad_2.tar  10.07 GB   \n",
      "18               knowmad_3.tar  10.06 GB   \n",
      "21               knowmad_4.tar  10.04 GB   \n",
      "20               knowmad_5.tar   5.37 GB   \n",
      "13          lifewatch-eric.tar   0.05 GB   \n",
      "30                     mes.tar   0.38 GB   \n",
      "32     neanias-atmospheric.tar   0.57 GB   \n",
      "33           neanias-space.tar   0.73 GB   \n",
      "31      neanias-underwater.tar   0.02 GB   \n",
      "3            netherlands_1.tar   3.91 GB   \n",
      "36                      ni.tar   3.01 GB   \n",
      "17  north-american-studies.tar   0.36 GB   \n",
      "34    rural-digital-europe.tar   0.83 GB   \n",
      "16                 sdsn-gr.tar   0.05 GB   \n",
      "35                   tunet.tar   0.05 GB   \n",
      "2                uarctic_1.tar   9.75 GB   \n",
      "\n",
      "                                         downloadlink  \\\n",
      "5   https://zenodo.org/api/records/14887484/files/...   \n",
      "8   https://zenodo.org/api/records/14887484/files/...   \n",
      "22  https://zenodo.org/api/records/14887484/files/...   \n",
      "6   https://zenodo.org/api/records/14887484/files/...   \n",
      "7   https://zenodo.org/api/records/14887484/files/...   \n",
      "23  https://zenodo.org/api/records/14887484/files/...   \n",
      "9   https://zenodo.org/api/records/14887484/files/...   \n",
      "11  https://zenodo.org/api/records/14887484/files/...   \n",
      "1   https://zenodo.org/api/records/14887484/files/...   \n",
      "12  https://zenodo.org/api/records/14887484/files/...   \n",
      "25  https://zenodo.org/api/records/14887484/files/...   \n",
      "0   https://zenodo.org/api/records/14887484/files/...   \n",
      "27  https://zenodo.org/api/records/14887484/files/...   \n",
      "24  https://zenodo.org/api/records/14887484/files/...   \n",
      "26  https://zenodo.org/api/records/14887484/files/...   \n",
      "15  https://zenodo.org/api/records/14887484/files/...   \n",
      "28  https://zenodo.org/api/records/14887484/files/...   \n",
      "10  https://zenodo.org/api/records/14887484/files/...   \n",
      "29  https://zenodo.org/api/records/14887484/files/...   \n",
      "14  https://zenodo.org/api/records/14887484/files/...   \n",
      "4   https://zenodo.org/api/records/14887484/files/...   \n",
      "19  https://zenodo.org/api/records/14887484/files/...   \n",
      "18  https://zenodo.org/api/records/14887484/files/...   \n",
      "21  https://zenodo.org/api/records/14887484/files/...   \n",
      "20  https://zenodo.org/api/records/14887484/files/...   \n",
      "13  https://zenodo.org/api/records/14887484/files/...   \n",
      "30  https://zenodo.org/api/records/14887484/files/...   \n",
      "32  https://zenodo.org/api/records/14887484/files/...   \n",
      "33  https://zenodo.org/api/records/14887484/files/...   \n",
      "31  https://zenodo.org/api/records/14887484/files/...   \n",
      "3   https://zenodo.org/api/records/14887484/files/...   \n",
      "36  https://zenodo.org/api/records/14887484/files/...   \n",
      "17  https://zenodo.org/api/records/14887484/files/...   \n",
      "34  https://zenodo.org/api/records/14887484/files/...   \n",
      "16  https://zenodo.org/api/records/14887484/files/...   \n",
      "35  https://zenodo.org/api/records/14887484/files/...   \n",
      "2   https://zenodo.org/api/records/14887484/files/...   \n",
      "\n",
      "                                checksum  \n",
      "5   md5:2ce6b0fcc6f876b600207759a0dc9758  \n",
      "8   md5:9b6a8f38cd6f0ce16a85dfc020c220bf  \n",
      "22  md5:447dbe25eaedc20a75568fd340f8e25d  \n",
      "6   md5:d2f24bbef06809a91d124f0b07cb1034  \n",
      "7   md5:3b741e8138f39932ca6c13ca106fe5d3  \n",
      "23  md5:814e3a79b29da6b605018009f8575a8c  \n",
      "9   md5:dbebdcc8ad7fd1dc7894fe03ebe2a978  \n",
      "11  md5:643894810ac8bfce0f8273cf40d05a7a  \n",
      "1   md5:23559bed5a9023398b431777bdc8a126  \n",
      "12  md5:2f52b49fa8bd983bcf6884d6c4f5e952  \n",
      "25  md5:e1dfe593d40c498e31a6ff5132da5fa4  \n",
      "0   md5:0a2f551db46a9e629bb1d0a0098ae5cd  \n",
      "27  md5:b157900f8cb97cf4aa4f82ef59e2ff6e  \n",
      "24  md5:19c86e2b79bde505112fb6b3d6e38eef  \n",
      "26  md5:3da085b997006205c694b023aefafe7c  \n",
      "15  md5:f9eb5a1bb86caf6a4f2a7563453fc6df  \n",
      "28  md5:7864a0ddb0b676bb053bbcdf12a12525  \n",
      "10  md5:ffd2537b08c58d78eea4bc23a99b3c07  \n",
      "29  md5:66a7e88ddda08626cbf26e182f7eafea  \n",
      "14  md5:6ffaf325257d9af5e43daa68505b1797  \n",
      "4   md5:a79573a02f2c9a9d65c33b3f3a2eaab9  \n",
      "19  md5:95442e14703ba5db573cbf12296d76f4  \n",
      "18  md5:68c5839a6355ea26f979ba781bc26a56  \n",
      "21  md5:d770fcd862d5d7d4d918c59f028e24da  \n",
      "20  md5:35482782eb621df9ec7365d34ccf3d07  \n",
      "13  md5:213c3f1ac83454ec01560d683a26362d  \n",
      "30  md5:10362f805616e391d350b395d9ae30a3  \n",
      "32  md5:a3b9edbd956aee813cdfe97655c3fc9e  \n",
      "33  md5:618fbecca154af288dc1c92246b3d73b  \n",
      "31  md5:4422b9a3be4a1e6cad61da46044a0ca2  \n",
      "3   md5:d1416c058b3961483aac340750ea8726  \n",
      "36  md5:e82f580135522acd2da7277ea9389718  \n",
      "17  md5:b677758820184053d9c1e6714394dd70  \n",
      "34  md5:6543f5ead539a8bbad04593b641af0a1  \n",
      "16  md5:696f8b509a12c0bc898e1b9040a53790  \n",
      "35  md5:178b5a944133ded65d741ea5dc2c5990  \n",
      "2   md5:302e3844ebd041c5f4ed94505eb9a285  \n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame to hold the tar files information for later use.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df_tar_files = pd.DataFrame(tar_files)\n",
    "\n",
    "# Sort the DataFrame by filename alphabetically\n",
    "df_tar_files = df_tar_files.sort_values(by='filename')\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df_tar_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tar files:\n",
      "    index                    filename      size\n",
      "0       5             argo-france.tar   0.00 GB\n",
      "1       8                  aurora.tar   1.73 GB\n",
      "2      22                  beopen.tar   0.20 GB\n",
      "3       6                  civica.tar   0.23 GB\n",
      "4       7                covid-19.tar   2.03 GB\n",
      "5      23                  dariah.tar   0.02 GB\n",
      "6       9                   dh-ch.tar   1.16 GB\n",
      "7      11                     dth.tar   0.01 GB\n",
      "8       1            edih-adria_1.tar   5.86 GB\n",
      "9      12                  egrise.tar   0.02 GB\n",
      "10     25               elixir-gr.tar   0.01 GB\n",
      "11      0       energy-planning_1.tar   6.99 GB\n",
      "12     27                enermaps.tar   1.59 GB\n",
      "13     24              eu-conexus.tar   0.18 GB\n",
      "14     26                     eut.tar   0.21 GB\n",
      "15     15                 eutopia.tar   1.60 GB\n",
      "16     28                 forthem.tar   0.91 GB\n",
      "17     10        heritage-science.tar   0.03 GB\n",
      "18     29                   inria.tar   0.27 GB\n",
      "19     14               iperionhs.tar   0.00 GB\n",
      "20      4               knowmad_1.tar  10.08 GB\n",
      "21     19               knowmad_2.tar  10.07 GB\n",
      "22     18               knowmad_3.tar  10.06 GB\n",
      "23     21               knowmad_4.tar  10.04 GB\n",
      "24     20               knowmad_5.tar   5.37 GB\n",
      "25     13          lifewatch-eric.tar   0.05 GB\n",
      "26     30                     mes.tar   0.38 GB\n",
      "27     32     neanias-atmospheric.tar   0.57 GB\n",
      "28     33           neanias-space.tar   0.73 GB\n",
      "29     31      neanias-underwater.tar   0.02 GB\n",
      "30      3           netherlands_1.tar   3.91 GB\n",
      "31     36                      ni.tar   3.01 GB\n",
      "32     17  north-american-studies.tar   0.36 GB\n",
      "33     34    rural-digital-europe.tar   0.83 GB\n",
      "34     16                 sdsn-gr.tar   0.05 GB\n",
      "35     35                   tunet.tar   0.05 GB\n",
      "36      2               uarctic_1.tar   9.75 GB\n",
      "Selected file: argo-france.tar\n",
      "Download link: https://zenodo.org/api/records/14887484/files/argo-france.tar/content\n",
      "Checksum: md5:2ce6b0fcc6f876b600207759a0dc9758\n"
     ]
    }
   ],
   "source": [
    "# Print a reindexed list of available tar files\n",
    "print(\"Available tar files:\")\n",
    "print(df_tar_files[['filename', 'size']].reset_index())\n",
    "\n",
    "import signal\n",
    "\n",
    "# Function to handle timeout\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutError\n",
    "\n",
    "# Set the timeout handler for the input\n",
    "signal.signal(signal.SIGALRM, timeout_handler)\n",
    "signal.alarm(10)  # Set the timeout to 10 seconds\n",
    "\n",
    "try:\n",
    "    # Ask the user to select a tar file by its index\n",
    "    selected_index = int(input(\"Enter the index of the tar file you want to download: \"))\n",
    "except TimeoutError:\n",
    "    print(\"No response received. Defaulting to index 1.\")\n",
    "    selected_index = 1\n",
    "finally:\n",
    "    signal.alarm(0)  # Disable the alarm\n",
    "\n",
    "# Get the selected tar file's download link and checksum\n",
    "selected_file = df_tar_files.iloc[selected_index]\n",
    "downloadlink = selected_file['downloadlink']\n",
    "checksum = selected_file['checksum']\n",
    "\n",
    "print(f\"Selected file: {selected_file['filename']}\")\n",
    "print(f\"Download link: {downloadlink}\")\n",
    "print(f\"Checksum: {checksum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name: argo-france.tar\n",
      "Download Path File: ./data/2025-02-19/01_input/argo-france.tar\n",
      "Folder Name: argo-france\n",
      "Extraction Path Folder: ./data/2025-02-19/02_extracted/argo-france\n"
     ]
    }
   ],
   "source": [
    "# Path Variables\n",
    "\n",
    "# Extract the file name from the selected file\n",
    "file_name = selected_file['filename']    \n",
    "\n",
    "# Path to save the downloaded tar file using file_name variable\n",
    "download_path = f\"./data/{publication_date}/01_input/{file_name}\"\n",
    "\n",
    "# Create the folder name by removing the .tar extension\n",
    "folder_name = selected_file['filename'].replace('.tar', '')\n",
    "\n",
    "# Path to save the extracted files using the file_name variable without the .tar extension\n",
    "extraction_path = f\"./data/{publication_date}/02_extracted/{folder_name}\"\n",
    "\n",
    "\n",
    "print(f\"File Name: {file_name}\")\n",
    "print(f\"Download Path File: {download_path}\")\n",
    "print(f\"Folder Name: {folder_name}\")\n",
    "print(f\"Extraction Path Folder: {extraction_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the tar file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists: ./data/2025-02-19/01_input/argo-france.tar\n",
      "Download URL: https://zenodo.org/api/records/14887484/files/argo-france.tar/content\n",
      "Download complete: ./data/2025-02-19/01_input/argo-france.tar\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Ensure the directory for the download path exists\n",
    "os.makedirs(os.path.dirname(download_path), exist_ok=True)\n",
    "\n",
    "# Check if the file already exists\n",
    "if not os.path.exists(download_path):\n",
    "    # Get the file size in bytes\n",
    "    file_size_bytes = float(selected_file['size'].split()[0]) * (1024**3)  # Convert GB to bytes\n",
    "    print(f\"Downloading file: {selected_file['filename']} ({selected_file['size']})\")\n",
    "    print(f\"Download URL: {downloadlink}\")\n",
    "    \n",
    "    # Estimate download duration assuming an average speed of 10 MB/s\n",
    "    avg_speed = 10 * (1024**2)  # 10 MB/s in bytes\n",
    "    estimated_duration = file_size_bytes / avg_speed\n",
    "    print(f\"Estimated download time: {estimated_duration:.2f} seconds\")\n",
    "    \n",
    "    # Download the selected tar file\n",
    "    response = requests.get(downloadlink, stream=True)\n",
    "    with open(download_path, 'wb') as file:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            file.write(chunk)\n",
    "else:\n",
    "    print(f\"File already exists: {download_path}\")\n",
    "    print(f\"Download URL: {downloadlink}\")\n",
    "\n",
    "print(f\"Download complete: {download_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checksum verification passed.\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "# Function to calculate the checksum of a file\n",
    "def calculate_checksum(file_path, algorithm):\n",
    "    hash_func = hashlib.new(algorithm)\n",
    "    with open(file_path, 'rb') as f:\n",
    "        while chunk := f.read(8192):\n",
    "            hash_func.update(chunk)\n",
    "    return hash_func.hexdigest()\n",
    "\n",
    "# Extract the checksum algorithm and value\n",
    "checksum_parts = checksum.split(':', 1)\n",
    "checksum_algorithm = checksum_parts[0]\n",
    "expected_checksum = checksum_parts[1]\n",
    "\n",
    "# Calculate the checksum of the downloaded file\n",
    "calculated_checksum = calculate_checksum(download_path, algorithm=checksum_algorithm)\n",
    "\n",
    "# Compare the calculated checksum with the provided checksum\n",
    "if calculated_checksum == expected_checksum:\n",
    "    print(\"Checksum verification passed.\")\n",
    "else:\n",
    "    print(\"Checksum verification failed.\")\n",
    "    print(f\"Expected: {expected_checksum}\")\n",
    "    print(f\"Calculated: {calculated_checksum}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract the tar file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/2025-02-19/01_input/argo-france.tar to ./data/2025-02-19/02_extracted/argo-france...\n",
      "Extraction complete.\n",
      "Files extracted to: ./data/2025-02-19/02_extracted/argo-france\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if the extraction directory already exists and contains files\n",
    "if os.path.exists(extraction_path) and os.listdir(extraction_path):\n",
    "    print(\"The tar file has already been extracted.\")\n",
    "else:\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(extraction_path, exist_ok=True)\n",
    "\n",
    "    # Extract the tar file in the parent directory of the extraction_path - because the tar file contains a folder structure repeating the name of the tar file\n",
    "    print(f\"Extracting {download_path} to {extraction_path}...\")\n",
    "    parent_extraction_path = os.path.dirname(extraction_path)\n",
    "    with tarfile.open(download_path, 'r') as tar:\n",
    "        tar.extractall(path=parent_extraction_path)\n",
    "\n",
    "    print(\"Extraction complete.\")\n",
    "    print(f\"Files extracted to: {extraction_path}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 285\n",
      "First 5 files:\n",
      "part-00000-2c0de614-bb18-4931-bd6a-64f101a27baf-c000.json.gz\n",
      "part-00001-2c0de614-bb18-4931-bd6a-64f101a27baf-c000.json.gz\n",
      "part-00002-2c0de614-bb18-4931-bd6a-64f101a27baf-c000.json.gz\n",
      "part-00003-2c0de614-bb18-4931-bd6a-64f101a27baf-c000.json.gz\n",
      "part-00004-2c0de614-bb18-4931-bd6a-64f101a27baf-c000.json.gz\n",
      "                                              filename\n",
      "0    part-00000-2c0de614-bb18-4931-bd6a-64f101a27ba...\n",
      "1    part-00001-2c0de614-bb18-4931-bd6a-64f101a27ba...\n",
      "2    part-00002-2c0de614-bb18-4931-bd6a-64f101a27ba...\n",
      "3    part-00003-2c0de614-bb18-4931-bd6a-64f101a27ba...\n",
      "4    part-00004-2c0de614-bb18-4931-bd6a-64f101a27ba...\n",
      "..                                                 ...\n",
      "280  part-00580-2c0de614-bb18-4931-bd6a-64f101a27ba...\n",
      "281  part-00583-2c0de614-bb18-4931-bd6a-64f101a27ba...\n",
      "282  part-00592-2c0de614-bb18-4931-bd6a-64f101a27ba...\n",
      "283  part-00618-2c0de614-bb18-4931-bd6a-64f101a27ba...\n",
      "284  part-00736-2c0de614-bb18-4931-bd6a-64f101a27ba...\n",
      "\n",
      "[285 rows x 1 columns]\n",
      "DataFrame dimensions: (285, 1)\n",
      "Randomly selected files with full paths for testing:\n",
      "./data/2025-02-19/02_extracted/argo-france/part-00542-2c0de614-bb18-4931-bd6a-64f101a27baf-c000.json.gz\n",
      "./data/2025-02-19/02_extracted/argo-france/part-00272-2c0de614-bb18-4931-bd6a-64f101a27baf-c000.json.gz\n",
      "./data/2025-02-19/02_extracted/argo-france/part-00006-2c0de614-bb18-4931-bd6a-64f101a27baf-c000.json.gz\n",
      "./data/2025-02-19/02_extracted/argo-france/part-00055-2c0de614-bb18-4931-bd6a-64f101a27baf-c000.json.gz\n",
      "./data/2025-02-19/02_extracted/argo-france/part-00184-2c0de614-bb18-4931-bd6a-64f101a27baf-c000.json.gz\n",
      "Random file selected for later use: part-00309-2c0de614-bb18-4931-bd6a-64f101a27baf-c000.json.gz\n",
      "Path to the random file: ./data/2025-02-19/02_extracted/argo-france/part-00309-2c0de614-bb18-4931-bd6a-64f101a27baf-c000.json.gz\n",
      "The random file exists: ./data/2025-02-19/02_extracted/argo-france/part-00309-2c0de614-bb18-4931-bd6a-64f101a27baf-c000.json.gz\n"
     ]
    }
   ],
   "source": [
    "# List the extracted files\n",
    "extracted_files = os.listdir(extraction_path)\n",
    "\n",
    "# add the path to the extracted files\n",
    "extracted_files_with_path = [os.path.join(extraction_path, file) for file in extracted_files]\n",
    "\n",
    "# count the number of files in the extracted folder\n",
    "num_files = len(extracted_files)\n",
    "print(f\"Number of files: {num_files}\")\n",
    "\n",
    "# print the first 5 files\n",
    "print(\"First 5 files:\")\n",
    "for file in extracted_files[:5]:\n",
    "    print(file) \n",
    "\n",
    "# make a DataFrame for the extracted files\n",
    "df_extracted_files = pd.DataFrame(extracted_files, columns=['filename'])\n",
    "# Sort the DataFrame by filename alphabetically\n",
    "df_extracted_files = df_extracted_files.sort_values(by='filename')\n",
    "# Print the DataFrame\n",
    "print(df_extracted_files)\n",
    "\n",
    "# print the dimensions of the DataFrame\n",
    "print(f\"DataFrame dimensions: {df_extracted_files.shape}\")\n",
    "\n",
    "# print a random 5 files, to be used for testing, and use in a variable for later use\n",
    "import random\n",
    "random_files = random.sample(extracted_files, 5)\n",
    "random_files_with_path = [os.path.join(extraction_path, file) for file in random_files]\n",
    "print(\"Randomly selected files with full paths for testing:\")\n",
    "for file in random_files_with_path:\n",
    "    print(file)\n",
    "\n",
    "# one random file for later use\n",
    "random_file = random.choice(extracted_files)\n",
    "print(f\"Random file selected for later use: {random_file}\")\n",
    "# Define the path to the random file\n",
    "random_file_path = os.path.join(extraction_path, random_file)\n",
    "print(f\"Path to the random file: {random_file_path}\")\n",
    "# Check if the random file exists\n",
    "if os.path.exists(random_file_path):\n",
    "    print(f\"The random file exists: {random_file_path}\")\n",
    "else:\n",
    "    print(f\"The random file does not exist: {random_file_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Get a data sample to generate parquetfile and the SQL schema\n",
    "We do this before we process the bulk of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output file path: ./data/2025-02-19/03_transformed/argo-france/argo-france-sample.parquet\n",
      "Multiple sample file path: ./data/2025-02-19/03_transformed/argo-france/argo-france-multiple-sample.parquet\n",
      "Master file path: ./data/2025-02-19/03_transformed/argo-france/argo-france-master.parquet\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "transformation_folder_path = f\"./data/{publication_date}/03_transformed/{folder_name}\"\n",
    "\n",
    "# Ensure the target directory exists\n",
    "os.makedirs(transformation_folder_path, exist_ok=True)\n",
    "\n",
    "# for testing: Define and print the target output sample file path\n",
    "sample_file = f\"{transformation_folder_path}/{folder_name}-sample.parquet\"\n",
    "print(f\"Output file path: {sample_file}\")\n",
    "\n",
    "# for testing: define and print the target output sample file for the multiple selected random sample files\n",
    "multiple_sample_file = f\"{transformation_folder_path}/{folder_name}-multiple-sample.parquet\"\n",
    "print(f\"Multiple sample file path: {multiple_sample_file}\")\n",
    "\n",
    "# for production: define and print the target output master file for all extracted files\n",
    "master_file = f\"{transformation_folder_path}/{folder_name}-master.parquet\"\n",
    "print(f\"Master file path: {master_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for testing: this part is for running on a single sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed data saved to: ./data/2025-02-19/03_transformed/argo-france/argo-france-sample.parquet\n",
      "File size: 0.03 MB\n"
     ]
    }
   ],
   "source": [
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Use DuckDB to process the extracted JSON files and save a sample of all rows as a Parquet file\n",
    "con.sql(f'''\n",
    "    COPY (\n",
    "        \n",
    "        FROM (\n",
    "            SELECT *\n",
    "            FROM read_json('{random_file_path}', sample_size=-1, union_by_name=true)\n",
    "        )\n",
    "    )\n",
    "    TO '{sample_file}' (FORMAT parquet, COMPRESSION gzip)\n",
    "''')\n",
    "\n",
    "print(f\"Transformed data saved to: {sample_file}\")\n",
    "print(f\"File size: {os.path.getsize(sample_file) / (1024**2):.2f} MB\")\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema file path: ./data/2025-02-19/03_transformed/argo-france/argo-france-schema.sql\n",
      "Schema file exists: ./data/2025-02-19/03_transformed/argo-france/argo-france-schema.sql\n",
      "Schema file content:\n",
      "column_name,column_type,null,key,default,extra\n",
      "authors,\"STRUCT(fullName VARCHAR, \"\"name\"\" VARCHAR, rank BIGINT, surname VARCHAR, pid STRUCT(id STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR), provenance STRUCT(provenance VARCHAR, trust VARCHAR)))[]\",YES,,,\n",
      "bestAccessRight,\"STRUCT(code VARCHAR, \"\"label\"\" VARCHAR, scheme VARCHAR)\",YES,,,\n",
      "collectedFrom,\"STRUCT(\"\"key\"\" VARCHAR, \"\"value\"\" VARCHAR)[]\",YES,,,\n",
      "communities,\"STRUCT(code VARCHAR, \"\"label\"\" VARCHAR, provenance STRUCT(provenance VARCHAR, trust VARCHAR)[])[]\",YES,,,\n",
      "contributors,VARCHAR[],YES,,,\n",
      "countries,\"STRUCT(code VARCHAR, \"\"label\"\" VARCHAR, provenance STRUCT(provenance VARCHAR, trust VARCHAR))[]\",YES,,,\n",
      "coverages,JSON[],YES,,,\n",
      "dateOfCollection,VARCHAR,YES,,,\n",
      "descriptions,VARCHAR[],YES,,,\n",
      "formats,VARCHAR[],YES,,,\n",
      "id,VARCHAR,YES,,,\n",
      "indicators,\"STRUCT(citationImpact STRUCT(citationClass VARCHAR, citationCount DOUBLE, impulse DOUBLE, impulseClass VARCHAR, influence DOUBLE, influenceClass VARCHAR, popularity DOUBLE, popularityClass VARCHAR))\",YES,,,\n",
      "instances,\"STRUCT(alternateIdentifiers STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR)[], collectedFrom STRUCT(\"\"key\"\" VARCHAR, \"\"value\"\" VARCHAR), hostedBy STRUCT(\"\"key\"\" VARCHAR, \"\"value\"\" VARCHAR), pids STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR)[], publicationDate DATE, refereed VARCHAR, \"\"type\"\" VARCHAR, urls VARCHAR[], accessRight STRUCT(code VARCHAR, \"\"label\"\" VARCHAR, scheme VARCHAR, openAccessRoute VARCHAR), license VARCHAR, articleProcessingCharge STRUCT(amount VARCHAR, currency VARCHAR))[]\",YES,,,\n",
      "isGreen,BOOLEAN,YES,,,\n",
      "isInDiamondJournal,BOOLEAN,YES,,,\n",
      "language,\"STRUCT(code VARCHAR, \"\"label\"\" VARCHAR)\",YES,,,\n",
      "lastUpdateTimeStamp,BIGINT,YES,,,\n",
      "mainTitle,VARCHAR,YES,,,\n",
      "originalIds,VARCHAR[],YES,,,\n",
      "pids,\"STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR)[]\",YES,,,\n",
      "publicationDate,DATE,YES,,,\n",
      "publiclyFunded,BOOLEAN,YES,,,\n",
      "publisher,VARCHAR,YES,,,\n",
      "sources,VARCHAR[],YES,,,\n",
      "subjects,\"STRUCT(provenance STRUCT(provenance VARCHAR, trust VARCHAR), subject STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR))[]\",YES,,,\n",
      "type,VARCHAR,YES,,,\n",
      "container,\"STRUCT(issnOnline VARCHAR, \"\"name\"\" VARCHAR, vol VARCHAR)\",YES,,,\n",
      "openAccessColor,VARCHAR,YES,,,\n",
      "organizations,\"STRUCT(id VARCHAR, legalName VARCHAR, pids STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR)[])[]\",YES,,,\n",
      "projects,\"STRUCT(acronym VARCHAR, code VARCHAR, funder STRUCT(fundingStream VARCHAR, jurisdiction VARCHAR, \"\"name\"\" VARCHAR, shortName VARCHAR), id VARCHAR, provenance STRUCT(provenance VARCHAR, trust VARCHAR), title VARCHAR, validated STRUCT(validatedByFunder BOOLEAN, validationDate DATE))[]\",YES,,,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# schema path\n",
    "schema_file_path = f\"{transformation_folder_path}/{folder_name}-schema.sql\"\n",
    "\n",
    "#print the schema file path\n",
    "print(f\"Schema file path: {schema_file_path}\")\n",
    "\n",
    "duckdb.sql(f'''\n",
    "    COPY (\n",
    "        SELECT *\n",
    "        FROM (DESCRIBE '{sample_file}')\n",
    "    )\n",
    "    TO '{schema_file_path}'\n",
    "''')\n",
    "# check if the schema file exists\n",
    "if os.path.exists(schema_file_path):\n",
    "    print(f\"Schema file exists: {schema_file_path}\")\n",
    "else:\n",
    "    print(f\"Schema file does not exist: {schema_file_path}\")\n",
    "# Print the schema file content\n",
    "with open(schema_file_path, 'r') as schema_file:\n",
    "    schema_content = schema_file.read()\n",
    "    print(\"Schema file content:\")\n",
    "    print(schema_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in the Parquet file: 3\n"
     ]
    }
   ],
   "source": [
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Count the number of records in the Parquet file\n",
    "record_count = con.sql(f'''\n",
    "    SELECT COUNT(*)\n",
    "    FROM read_parquet('{sample_file}')\n",
    "''').fetchone()[0]\n",
    "\n",
    "# Print the record count\n",
    "print(f\"Number of records in the Parquet file: {record_count}\")\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titles in the Parquet file:\n",
      "World Ocean Database 2013. \n",
      "Arctic mid-winter phytoplankton growth revealed by autonomous profilers\n",
      "A shift in the ocean circulation has warmed the subpolar North Atlantic Ocean since 2016\n"
     ]
    }
   ],
   "source": [
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Query the titles from the Parquet file\n",
    "titles = con.sql(f'''\n",
    "    SELECT mainTitle\n",
    "    FROM read_parquet('{sample_file}')\n",
    "''').fetchall()\n",
    "\n",
    "# Print the titles\n",
    "print(\"Titles in the Parquet file:\")\n",
    "for title in titles:\n",
    "    print(title[0])\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for testing: this part is for running on a random sample of multiple files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed data saved to: ./data/2025-02-19/03_transformed/argo-france/argo-france-multiple-sample.parquet\n",
      "File size: 0.07 MB\n"
     ]
    }
   ],
   "source": [
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Use DuckDB to process the extracted JSON files and save a sample of all rows as a Parquet file\n",
    "# Join the list of file paths into a comma-separated string\n",
    "file_paths = ','.join(f\"'{file}'\" for file in random_files_with_path)\n",
    "\n",
    "# Use DuckDB to process the extracted JSON files and save a sample of all rows as a Parquet file\n",
    "con.sql(f'''\n",
    "    COPY (\n",
    "        SELECT *\n",
    "        FROM read_json([{file_paths}], sample_size=-1, union_by_name=true)\n",
    "    )\n",
    "    TO '{multiple_sample_file}' (FORMAT parquet, COMPRESSION gzip)\n",
    "''')\n",
    "\n",
    "print(f\"Transformed data saved to: {multiple_sample_file}\")\n",
    "print(f\"File size: {os.path.getsize(multiple_sample_file) / (1024**2):.2f} MB\")\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema file path: ./data/2025-02-19/03_transformed/argo-france/argo-france-schema.sql\n",
      "Schema file exists: ./data/2025-02-19/03_transformed/argo-france/argo-france-schema.sql\n",
      "Schema file content:\n",
      "column_name,column_type,null,key,default,extra\n",
      "authors,\"STRUCT(fullName VARCHAR, \"\"name\"\" VARCHAR, pid STRUCT(id STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR), provenance STRUCT(provenance VARCHAR, trust VARCHAR)), rank BIGINT, surname VARCHAR)[]\",YES,,,\n",
      "bestAccessRight,\"STRUCT(code VARCHAR, \"\"label\"\" VARCHAR, scheme VARCHAR)\",YES,,,\n",
      "collectedFrom,\"STRUCT(\"\"key\"\" VARCHAR, \"\"value\"\" VARCHAR)[]\",YES,,,\n",
      "communities,\"STRUCT(code VARCHAR, \"\"label\"\" VARCHAR, provenance STRUCT(provenance VARCHAR, trust VARCHAR)[])[]\",YES,,,\n",
      "container,\"STRUCT(issnOnline VARCHAR, \"\"name\"\" VARCHAR, sp VARCHAR, vol VARCHAR, issnPrinted VARCHAR, ep VARCHAR)\",YES,,,\n",
      "contributors,VARCHAR[],YES,,,\n",
      "countries,\"STRUCT(code VARCHAR, \"\"label\"\" VARCHAR, provenance STRUCT(provenance VARCHAR, trust VARCHAR))[]\",YES,,,\n",
      "coverages,JSON[],YES,,,\n",
      "dateOfCollection,VARCHAR,YES,,,\n",
      "descriptions,VARCHAR[],YES,,,\n",
      "formats,VARCHAR[],YES,,,\n",
      "id,VARCHAR,YES,,,\n",
      "indicators,\"STRUCT(citationImpact STRUCT(citationClass VARCHAR, citationCount DOUBLE, impulse DOUBLE, impulseClass VARCHAR, influence DOUBLE, influenceClass VARCHAR, popularity DOUBLE, popularityClass VARCHAR), usageCounts STRUCT(downloads BIGINT, \"\"views\"\" BIGINT))\",YES,,,\n",
      "instances,\"STRUCT(accessRight STRUCT(code VARCHAR, \"\"label\"\" VARCHAR, openAccessRoute VARCHAR, scheme VARCHAR), alternateIdentifiers STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR)[], collectedFrom STRUCT(\"\"key\"\" VARCHAR, \"\"value\"\" VARCHAR), hostedBy STRUCT(\"\"key\"\" VARCHAR, \"\"value\"\" VARCHAR), license VARCHAR, pids STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR)[], publicationDate DATE, refereed VARCHAR, \"\"type\"\" VARCHAR, urls VARCHAR[])[]\",YES,,,\n",
      "isGreen,BOOLEAN,YES,,,\n",
      "isInDiamondJournal,BOOLEAN,YES,,,\n",
      "language,\"STRUCT(code VARCHAR, \"\"label\"\" VARCHAR)\",YES,,,\n",
      "lastUpdateTimeStamp,BIGINT,YES,,,\n",
      "mainTitle,VARCHAR,YES,,,\n",
      "openAccessColor,VARCHAR,YES,,,\n",
      "organizations,\"STRUCT(id VARCHAR, legalName VARCHAR, pids STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR)[])[]\",YES,,,\n",
      "originalIds,VARCHAR[],YES,,,\n",
      "pids,\"STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR)[]\",YES,,,\n",
      "projects,\"STRUCT(acronym VARCHAR, code VARCHAR, funder STRUCT(fundingStream VARCHAR, jurisdiction VARCHAR, \"\"name\"\" VARCHAR, shortName VARCHAR), id VARCHAR, provenance STRUCT(provenance VARCHAR, trust VARCHAR), title VARCHAR, validated STRUCT(validatedByFunder BOOLEAN, validationDate DATE))[]\",YES,,,\n",
      "publicationDate,DATE,YES,,,\n",
      "publiclyFunded,BOOLEAN,YES,,,\n",
      "publisher,VARCHAR,YES,,,\n",
      "sources,VARCHAR[],YES,,,\n",
      "subjects,\"STRUCT(provenance STRUCT(provenance VARCHAR, trust VARCHAR), subject STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR))[]\",YES,,,\n",
      "type,VARCHAR,YES,,,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# schema path\n",
    "schema_file_path = f\"{transformation_folder_path}/{folder_name}-schema.sql\"\n",
    "\n",
    "#print the schema file path\n",
    "print(f\"Schema file path: {schema_file_path}\")\n",
    "\n",
    "duckdb.sql(f'''\n",
    "    COPY (\n",
    "        SELECT *\n",
    "        FROM (DESCRIBE '{multiple_sample_file}')\n",
    "    )\n",
    "    TO '{schema_file_path}'\n",
    "''')\n",
    "# check if the schema file exists\n",
    "if os.path.exists(schema_file_path):\n",
    "    print(f\"Schema file exists: {schema_file_path}\")\n",
    "else:\n",
    "    print(f\"Schema file does not exist: {schema_file_path}\")\n",
    "# Print the schema file content\n",
    "with open(schema_file_path, 'r') as schema_file:\n",
    "    schema_content = schema_file.read()\n",
    "    print(\"Schema file content:\")\n",
    "    print(schema_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in the Parquet file: 13\n"
     ]
    }
   ],
   "source": [
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Count the number of records in the Parquet file\n",
    "record_count = con.sql(f'''\n",
    "    SELECT COUNT(*)\n",
    "    FROM read_parquet('{multiple_sample_file}')\n",
    "''').fetchone()[0]\n",
    "\n",
    "# Print the record count\n",
    "print(f\"Number of records in the Parquet file: {record_count}\")\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titles in the Parquet file:\n",
      "Climatic, Decadal, and Interannual Variability in the Upper Layer of the Mediterranean Sea Using Remotely Sensed and In-Situ Data\n",
      "Achievements and Prospects of Global Broadband Seismographic Networks After 30Years of Continuous Geophysical Observations\n",
      "Vortexwall interaction on the <i></i>-plane and the generation of deep submesoscale cyclones by internal Kelvin Wavescurrent interactions\n",
      "A global probabilistic study of the ocean heat content lowfrequency variability: Atmospheric forcing versus oceanic chaos\n",
      "Particulate concentration and seasonal dynamics in the mesopelagic ocean based on the backscattering coefficient measured with BiogeochemicalArgo floats\n",
      "Observing the full ocean volume using Deep Argo floats\n",
      "Using climatological salinities for estimating the oxygen content in ARGO floats\n",
      "Aliasing of the Indian Ocean externally-forced warming spatial pattern by internal climate variability\n",
      "Multifrequency seismic detectability of seasonal thermoclines assessed from ARGO data\n",
      "Formation and Transport of the South Atlantic Subtropical Mode Water in EddyPermitting Observations\n",
      "Report on Euro-Argo ERIC links with industry\n",
      "Low-frequency thermohaline variability in the Subtropical South Atlantic pycnocline during 2002-2013\n",
      "<scp>The Underwater Vision Profiler 6: an imaging sensor of particle size spectra and plankton, for autonomous and cabled platforms</scp>\n"
     ]
    }
   ],
   "source": [
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Query the titles from the Parquet file\n",
    "titles = con.sql(f'''\n",
    "    SELECT mainTitle\n",
    "    FROM read_parquet('{multiple_sample_file}')\n",
    "''').fetchall()\n",
    "\n",
    "# Print the titles\n",
    "print(\"Titles in the Parquet file:\")\n",
    "for title in titles:\n",
    "    print(title[0])\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for production: parsing all extracted files into one master parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'master_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 14\u001b[0m\n\u001b[1;32m      6\u001b[0m file_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m extracted_files_with_path)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Use DuckDB to process the extracted JSON files and save a sample of all rows as a Parquet file\u001b[39;00m\n\u001b[1;32m      9\u001b[0m con\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m    COPY (\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124m        SELECT *\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124m        FROM read_json([\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_paths\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], sample_size=-1, union_by_name=true)\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124m    )\u001b[39m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;124m    TO \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmaster_file\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (FORMAT parquet, COMPRESSION gzip)\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124m'''\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformed data saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmaster_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mgetsize(master_file)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m(\u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m MB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'master_file' is not defined"
     ]
    }
   ],
   "source": [
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Use DuckDB to process the extracted JSON files and save a sample of all rows as a Parquet file\n",
    "# Join the list of file paths into a comma-separated string\n",
    "file_paths = ','.join(f\"'{file}'\" for file in extracted_files_with_path)\n",
    "\n",
    "# Use DuckDB to process the extracted JSON files and save a sample of all rows as a Parquet file\n",
    "con.sql(f'''\n",
    "    COPY (\n",
    "        SELECT *\n",
    "        FROM read_json([{file_paths}], sample_size=-1, union_by_name=true)\n",
    "    )\n",
    "    TO '{master_file}' (FORMAT parquet, COMPRESSION gzip)\n",
    "''')\n",
    "\n",
    "print(f\"Transformed data saved to: {master_file}\")\n",
    "print(f\"File size: {os.path.getsize(master_file) / (1024**2):.2f} MB\")\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
