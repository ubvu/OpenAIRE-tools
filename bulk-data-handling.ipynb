{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAIRE Community Data Dump Handling: Extraction, Tranformation and Enrichment\n",
    "\n",
    "In this notebook we will start with one of the [OpenAIRE Community Subgraphs](https://graph.openaire.eu/docs/downloads/subgraphs) to enrich that informatino for further analysis.\n",
    "\n",
    "This data process will extract an [OpenAIRE community data dump from Zenodo](https://doi.org/10.5281/zenodo.3974604), transforms it in to a portable file format .parquet (and updatable with changes for time seires analysis), that can be used to query with DuckDB, to enrich this with additional data  (also in .parquet, for join queries).\n",
    "\n",
    "This additional data can be societal impact data from [Altmetric.com](https://details-page-api-docs.altmetric.com/) or [Overton.io](https://app.overton.io/swagger.php), Gender data using [genderize.io](https://genderize.io/documentation), sdg classification using [aurora-sdg](https://aurora-universities.eu/sdg-research/sdg-api/)\n",
    "\n",
    "This script needs to be written in a way so that it can run every month using  the latest data.\n",
    "\n",
    "## Processing steps\n",
    "\n",
    "* the folder ./data/ is put in .gitignore to prevent that bulk datais sent to a code repository. So make sure that folder exists, and mkdir if not exists. \n",
    "* The script downloads the lastest Data Dump Tar file from one selected community. See https://doi.org/10.5281/zenodo.3974604 for the latest list. In our case the Aurora tar file. https://zenodo.org/records/14887484/files/aurora.tar?download=1\n",
    "  * Use the json record of zenodo to get to the latest record, and fetch the download link of the aurora.tar file. for example : https://zenodo.org/records/14887484/export/json or https://zenodo.org/api/records/14887484/versions/latest \n",
    "  Make the tar filename a variable, so it can be used for multiple community dumps.\n",
    "  Download the tar file in a target folder ./data/{filename+timestamp}/ where a subfolder is created using the filename and the timestamp. Make this also as a  variable to use later on.\n",
    "* Extract the tar file, to the compressed .json.gz files and put these in target folder ./data/{filename+timestamp}/01-extracted/\n",
    "* Transform the compressed .json.gz files into a single .parquet file in target folder ./data/{filename+timestamp}/02-transformed/\n",
    "Use instructions in sections \"Processing JSON files with DuckDB\" and \"Full dataset, bit by bit\" and \"Splitting and Processing JSON Files in Batches\" https://github.com/mosart/OpenAIRE-tools/blob/main/duckdb-querying.ipynb to start with. (be aware of error messages, and fix the issues to get all the data in)\n",
    "* Extract the SQL schema (schema-datadump.sql) from the .parquet file and put it in target folder ./data/{filename+timestamp}/02-transformed/ This is needed for further processing of the records with DuckDB later on.\n",
    "Use instructions in section \"Extracting Schema from Parquet File\" https://github.com/mosart/OpenAIRE-tools/blob/main/duckdb-querying.ipynb to start with.\n",
    "* Query to get all identifiers: openaire id, doi, isbn, hdl, etc.\n",
    "* **Get Altmetric data:**\n",
    "* Extract the Altmetric data using the Identifiers. put that in target folder ./data/{filename+timestamp}/03-altmetric-extracted/\n",
    "* Transform the Altmetric data to a single .parquet file, with the identifiers. put that in target folder ./data/{filename+timestamp}/04-altmetric-transformed/ This way duckDB can make a join when querying over multiple parquet files.\n",
    "* Extract the SQL schema (schema-altmetric.sql) from the .parquet file and put it in target folder ./data/{filename+timestamp}/04-altmetric-transformed/\n",
    "* **Get Overton data:** Repeat the altmetric steps, bun than for Overton.\n",
    "* **Get Gender data** query for the Author names and country codes, and run them over the gerderize api\n",
    "* **Get SDG data** query for the abstracts, and run abstracs larger than 100 tokens over the aurora-SDG api.\n",
    "\n",
    "ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Get the latest Community Dump File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37 .tar files in the dataset.\n",
      "Details of .tar files:\n",
      "[{'filename': 'energy-planning_1.tar', 'size': '6.99 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/energy-planning_1.tar/content', 'checksum': 'md5:0a2f551db46a9e629bb1d0a0098ae5cd'}, {'filename': 'edih-adria_1.tar', 'size': '5.86 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/edih-adria_1.tar/content', 'checksum': 'md5:23559bed5a9023398b431777bdc8a126'}, {'filename': 'uarctic_1.tar', 'size': '9.75 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/uarctic_1.tar/content', 'checksum': 'md5:302e3844ebd041c5f4ed94505eb9a285'}, {'filename': 'netherlands_1.tar', 'size': '3.91 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/netherlands_1.tar/content', 'checksum': 'md5:d1416c058b3961483aac340750ea8726'}, {'filename': 'knowmad_1.tar', 'size': '10.08 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/knowmad_1.tar/content', 'checksum': 'md5:a79573a02f2c9a9d65c33b3f3a2eaab9'}, {'filename': 'argo-france.tar', 'size': '0.00 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/argo-france.tar/content', 'checksum': 'md5:2ce6b0fcc6f876b600207759a0dc9758'}, {'filename': 'civica.tar', 'size': '0.23 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/civica.tar/content', 'checksum': 'md5:d2f24bbef06809a91d124f0b07cb1034'}, {'filename': 'covid-19.tar', 'size': '2.03 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/covid-19.tar/content', 'checksum': 'md5:3b741e8138f39932ca6c13ca106fe5d3'}, {'filename': 'aurora.tar', 'size': '1.73 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/aurora.tar/content', 'checksum': 'md5:9b6a8f38cd6f0ce16a85dfc020c220bf'}, {'filename': 'dh-ch.tar', 'size': '1.16 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/dh-ch.tar/content', 'checksum': 'md5:dbebdcc8ad7fd1dc7894fe03ebe2a978'}, {'filename': 'heritage-science.tar', 'size': '0.03 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/heritage-science.tar/content', 'checksum': 'md5:ffd2537b08c58d78eea4bc23a99b3c07'}, {'filename': 'dth.tar', 'size': '0.01 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/dth.tar/content', 'checksum': 'md5:643894810ac8bfce0f8273cf40d05a7a'}, {'filename': 'egrise.tar', 'size': '0.02 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/egrise.tar/content', 'checksum': 'md5:2f52b49fa8bd983bcf6884d6c4f5e952'}, {'filename': 'lifewatch-eric.tar', 'size': '0.05 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/lifewatch-eric.tar/content', 'checksum': 'md5:213c3f1ac83454ec01560d683a26362d'}, {'filename': 'iperionhs.tar', 'size': '0.00 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/iperionhs.tar/content', 'checksum': 'md5:6ffaf325257d9af5e43daa68505b1797'}, {'filename': 'eutopia.tar', 'size': '1.60 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/eutopia.tar/content', 'checksum': 'md5:f9eb5a1bb86caf6a4f2a7563453fc6df'}, {'filename': 'sdsn-gr.tar', 'size': '0.05 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/sdsn-gr.tar/content', 'checksum': 'md5:696f8b509a12c0bc898e1b9040a53790'}, {'filename': 'north-american-studies.tar', 'size': '0.36 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/north-american-studies.tar/content', 'checksum': 'md5:b677758820184053d9c1e6714394dd70'}, {'filename': 'knowmad_3.tar', 'size': '10.06 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/knowmad_3.tar/content', 'checksum': 'md5:68c5839a6355ea26f979ba781bc26a56'}, {'filename': 'knowmad_2.tar', 'size': '10.07 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/knowmad_2.tar/content', 'checksum': 'md5:95442e14703ba5db573cbf12296d76f4'}, {'filename': 'knowmad_5.tar', 'size': '5.37 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/knowmad_5.tar/content', 'checksum': 'md5:35482782eb621df9ec7365d34ccf3d07'}, {'filename': 'knowmad_4.tar', 'size': '10.04 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/knowmad_4.tar/content', 'checksum': 'md5:d770fcd862d5d7d4d918c59f028e24da'}, {'filename': 'beopen.tar', 'size': '0.20 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/beopen.tar/content', 'checksum': 'md5:447dbe25eaedc20a75568fd340f8e25d'}, {'filename': 'dariah.tar', 'size': '0.02 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/dariah.tar/content', 'checksum': 'md5:814e3a79b29da6b605018009f8575a8c'}, {'filename': 'eu-conexus.tar', 'size': '0.18 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/eu-conexus.tar/content', 'checksum': 'md5:19c86e2b79bde505112fb6b3d6e38eef'}, {'filename': 'elixir-gr.tar', 'size': '0.01 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/elixir-gr.tar/content', 'checksum': 'md5:e1dfe593d40c498e31a6ff5132da5fa4'}, {'filename': 'eut.tar', 'size': '0.21 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/eut.tar/content', 'checksum': 'md5:3da085b997006205c694b023aefafe7c'}, {'filename': 'enermaps.tar', 'size': '1.59 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/enermaps.tar/content', 'checksum': 'md5:b157900f8cb97cf4aa4f82ef59e2ff6e'}, {'filename': 'forthem.tar', 'size': '0.91 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/forthem.tar/content', 'checksum': 'md5:7864a0ddb0b676bb053bbcdf12a12525'}, {'filename': 'inria.tar', 'size': '0.27 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/inria.tar/content', 'checksum': 'md5:66a7e88ddda08626cbf26e182f7eafea'}, {'filename': 'mes.tar', 'size': '0.38 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/mes.tar/content', 'checksum': 'md5:10362f805616e391d350b395d9ae30a3'}, {'filename': 'neanias-underwater.tar', 'size': '0.02 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/neanias-underwater.tar/content', 'checksum': 'md5:4422b9a3be4a1e6cad61da46044a0ca2'}, {'filename': 'neanias-atmospheric.tar', 'size': '0.57 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/neanias-atmospheric.tar/content', 'checksum': 'md5:a3b9edbd956aee813cdfe97655c3fc9e'}, {'filename': 'neanias-space.tar', 'size': '0.73 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/neanias-space.tar/content', 'checksum': 'md5:618fbecca154af288dc1c92246b3d73b'}, {'filename': 'rural-digital-europe.tar', 'size': '0.83 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/rural-digital-europe.tar/content', 'checksum': 'md5:6543f5ead539a8bbad04593b641af0a1'}, {'filename': 'tunet.tar', 'size': '0.05 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/tunet.tar/content', 'checksum': 'md5:178b5a944133ded65d741ea5dc2c5990'}, {'filename': 'ni.tar', 'size': '3.01 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/ni.tar/content', 'checksum': 'md5:e82f580135522acd2da7277ea9389718'}]\n",
      "Publication date: 2025-02-19\n",
      "DOI: 10.5281/zenodo.14887484\n",
      "Title: OpenAIRE Graph: Dataset for research communities and initiatives\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Fetch the JSON data from the URL\n",
    "url = \"https://zenodo.org/api/records/14887484/versions/latest\"\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "# Extract the files information\n",
    "files = data.get(\"files\", [])\n",
    "\n",
    "# Create a list of dictionaries for the .tar files\n",
    "tar_files = []\n",
    "for file in files:\n",
    "    if file[\"key\"].endswith(\".tar\"):\n",
    "        tar_files.append({\n",
    "            \"filename\": file[\"key\"],\n",
    "            \"size\": f\"{file['size'] / (1024**3):.2f} GB\",  # Convert bytes to GB\n",
    "            \"downloadlink\": file[\"links\"][\"self\"],\n",
    "            \"checksum\": file[\"checksum\"]\n",
    "        })\n",
    "\n",
    "# print the tar files\n",
    "# If no tar files found, print a message\n",
    "if not tar_files:\n",
    "    print(\"No .tar files found in the dataset.\")\n",
    "else:\n",
    "    print(f\"Found {len(tar_files)} .tar files in the dataset.\")\n",
    "    print(\"Details of .tar files:\")\n",
    "    print(tar_files)\n",
    "\n",
    "# get and print the publication date\n",
    "publication_date = data.get(\"metadata\", {}).get(\"publication_date\", \"Unknown\")\n",
    "print(f\"Publication date: {publication_date}\")\n",
    "# get and print the DOI\n",
    "doi = data.get(\"doi\", \"Unknown\")\n",
    "print(f\"DOI: {doi}\")\n",
    "# get and print the title\n",
    "title = data.get(\"title\", \"Unknown\")\n",
    "print(f\"Title: {title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      filename      size  \\\n",
      "5              argo-france.tar   0.00 GB   \n",
      "8                   aurora.tar   1.73 GB   \n",
      "22                  beopen.tar   0.20 GB   \n",
      "6                   civica.tar   0.23 GB   \n",
      "7                 covid-19.tar   2.03 GB   \n",
      "23                  dariah.tar   0.02 GB   \n",
      "9                    dh-ch.tar   1.16 GB   \n",
      "11                     dth.tar   0.01 GB   \n",
      "1             edih-adria_1.tar   5.86 GB   \n",
      "12                  egrise.tar   0.02 GB   \n",
      "25               elixir-gr.tar   0.01 GB   \n",
      "0        energy-planning_1.tar   6.99 GB   \n",
      "27                enermaps.tar   1.59 GB   \n",
      "24              eu-conexus.tar   0.18 GB   \n",
      "26                     eut.tar   0.21 GB   \n",
      "15                 eutopia.tar   1.60 GB   \n",
      "28                 forthem.tar   0.91 GB   \n",
      "10        heritage-science.tar   0.03 GB   \n",
      "29                   inria.tar   0.27 GB   \n",
      "14               iperionhs.tar   0.00 GB   \n",
      "4                knowmad_1.tar  10.08 GB   \n",
      "19               knowmad_2.tar  10.07 GB   \n",
      "18               knowmad_3.tar  10.06 GB   \n",
      "21               knowmad_4.tar  10.04 GB   \n",
      "20               knowmad_5.tar   5.37 GB   \n",
      "13          lifewatch-eric.tar   0.05 GB   \n",
      "30                     mes.tar   0.38 GB   \n",
      "32     neanias-atmospheric.tar   0.57 GB   \n",
      "33           neanias-space.tar   0.73 GB   \n",
      "31      neanias-underwater.tar   0.02 GB   \n",
      "3            netherlands_1.tar   3.91 GB   \n",
      "36                      ni.tar   3.01 GB   \n",
      "17  north-american-studies.tar   0.36 GB   \n",
      "34    rural-digital-europe.tar   0.83 GB   \n",
      "16                 sdsn-gr.tar   0.05 GB   \n",
      "35                   tunet.tar   0.05 GB   \n",
      "2                uarctic_1.tar   9.75 GB   \n",
      "\n",
      "                                         downloadlink  \\\n",
      "5   https://zenodo.org/api/records/14887484/files/...   \n",
      "8   https://zenodo.org/api/records/14887484/files/...   \n",
      "22  https://zenodo.org/api/records/14887484/files/...   \n",
      "6   https://zenodo.org/api/records/14887484/files/...   \n",
      "7   https://zenodo.org/api/records/14887484/files/...   \n",
      "23  https://zenodo.org/api/records/14887484/files/...   \n",
      "9   https://zenodo.org/api/records/14887484/files/...   \n",
      "11  https://zenodo.org/api/records/14887484/files/...   \n",
      "1   https://zenodo.org/api/records/14887484/files/...   \n",
      "12  https://zenodo.org/api/records/14887484/files/...   \n",
      "25  https://zenodo.org/api/records/14887484/files/...   \n",
      "0   https://zenodo.org/api/records/14887484/files/...   \n",
      "27  https://zenodo.org/api/records/14887484/files/...   \n",
      "24  https://zenodo.org/api/records/14887484/files/...   \n",
      "26  https://zenodo.org/api/records/14887484/files/...   \n",
      "15  https://zenodo.org/api/records/14887484/files/...   \n",
      "28  https://zenodo.org/api/records/14887484/files/...   \n",
      "10  https://zenodo.org/api/records/14887484/files/...   \n",
      "29  https://zenodo.org/api/records/14887484/files/...   \n",
      "14  https://zenodo.org/api/records/14887484/files/...   \n",
      "4   https://zenodo.org/api/records/14887484/files/...   \n",
      "19  https://zenodo.org/api/records/14887484/files/...   \n",
      "18  https://zenodo.org/api/records/14887484/files/...   \n",
      "21  https://zenodo.org/api/records/14887484/files/...   \n",
      "20  https://zenodo.org/api/records/14887484/files/...   \n",
      "13  https://zenodo.org/api/records/14887484/files/...   \n",
      "30  https://zenodo.org/api/records/14887484/files/...   \n",
      "32  https://zenodo.org/api/records/14887484/files/...   \n",
      "33  https://zenodo.org/api/records/14887484/files/...   \n",
      "31  https://zenodo.org/api/records/14887484/files/...   \n",
      "3   https://zenodo.org/api/records/14887484/files/...   \n",
      "36  https://zenodo.org/api/records/14887484/files/...   \n",
      "17  https://zenodo.org/api/records/14887484/files/...   \n",
      "34  https://zenodo.org/api/records/14887484/files/...   \n",
      "16  https://zenodo.org/api/records/14887484/files/...   \n",
      "35  https://zenodo.org/api/records/14887484/files/...   \n",
      "2   https://zenodo.org/api/records/14887484/files/...   \n",
      "\n",
      "                                checksum  \n",
      "5   md5:2ce6b0fcc6f876b600207759a0dc9758  \n",
      "8   md5:9b6a8f38cd6f0ce16a85dfc020c220bf  \n",
      "22  md5:447dbe25eaedc20a75568fd340f8e25d  \n",
      "6   md5:d2f24bbef06809a91d124f0b07cb1034  \n",
      "7   md5:3b741e8138f39932ca6c13ca106fe5d3  \n",
      "23  md5:814e3a79b29da6b605018009f8575a8c  \n",
      "9   md5:dbebdcc8ad7fd1dc7894fe03ebe2a978  \n",
      "11  md5:643894810ac8bfce0f8273cf40d05a7a  \n",
      "1   md5:23559bed5a9023398b431777bdc8a126  \n",
      "12  md5:2f52b49fa8bd983bcf6884d6c4f5e952  \n",
      "25  md5:e1dfe593d40c498e31a6ff5132da5fa4  \n",
      "0   md5:0a2f551db46a9e629bb1d0a0098ae5cd  \n",
      "27  md5:b157900f8cb97cf4aa4f82ef59e2ff6e  \n",
      "24  md5:19c86e2b79bde505112fb6b3d6e38eef  \n",
      "26  md5:3da085b997006205c694b023aefafe7c  \n",
      "15  md5:f9eb5a1bb86caf6a4f2a7563453fc6df  \n",
      "28  md5:7864a0ddb0b676bb053bbcdf12a12525  \n",
      "10  md5:ffd2537b08c58d78eea4bc23a99b3c07  \n",
      "29  md5:66a7e88ddda08626cbf26e182f7eafea  \n",
      "14  md5:6ffaf325257d9af5e43daa68505b1797  \n",
      "4   md5:a79573a02f2c9a9d65c33b3f3a2eaab9  \n",
      "19  md5:95442e14703ba5db573cbf12296d76f4  \n",
      "18  md5:68c5839a6355ea26f979ba781bc26a56  \n",
      "21  md5:d770fcd862d5d7d4d918c59f028e24da  \n",
      "20  md5:35482782eb621df9ec7365d34ccf3d07  \n",
      "13  md5:213c3f1ac83454ec01560d683a26362d  \n",
      "30  md5:10362f805616e391d350b395d9ae30a3  \n",
      "32  md5:a3b9edbd956aee813cdfe97655c3fc9e  \n",
      "33  md5:618fbecca154af288dc1c92246b3d73b  \n",
      "31  md5:4422b9a3be4a1e6cad61da46044a0ca2  \n",
      "3   md5:d1416c058b3961483aac340750ea8726  \n",
      "36  md5:e82f580135522acd2da7277ea9389718  \n",
      "17  md5:b677758820184053d9c1e6714394dd70  \n",
      "34  md5:6543f5ead539a8bbad04593b641af0a1  \n",
      "16  md5:696f8b509a12c0bc898e1b9040a53790  \n",
      "35  md5:178b5a944133ded65d741ea5dc2c5990  \n",
      "2   md5:302e3844ebd041c5f4ed94505eb9a285  \n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame to hold the tar files information for later use.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df_tar_files = pd.DataFrame(tar_files)\n",
    "\n",
    "# Sort the DataFrame by filename alphabetically\n",
    "df_tar_files = df_tar_files.sort_values(by='filename')\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df_tar_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tar files:\n",
      "    index                    filename      size\n",
      "0       5             argo-france.tar   0.00 GB\n",
      "1       8                  aurora.tar   1.73 GB\n",
      "2      22                  beopen.tar   0.20 GB\n",
      "3       6                  civica.tar   0.23 GB\n",
      "4       7                covid-19.tar   2.03 GB\n",
      "5      23                  dariah.tar   0.02 GB\n",
      "6       9                   dh-ch.tar   1.16 GB\n",
      "7      11                     dth.tar   0.01 GB\n",
      "8       1            edih-adria_1.tar   5.86 GB\n",
      "9      12                  egrise.tar   0.02 GB\n",
      "10     25               elixir-gr.tar   0.01 GB\n",
      "11      0       energy-planning_1.tar   6.99 GB\n",
      "12     27                enermaps.tar   1.59 GB\n",
      "13     24              eu-conexus.tar   0.18 GB\n",
      "14     26                     eut.tar   0.21 GB\n",
      "15     15                 eutopia.tar   1.60 GB\n",
      "16     28                 forthem.tar   0.91 GB\n",
      "17     10        heritage-science.tar   0.03 GB\n",
      "18     29                   inria.tar   0.27 GB\n",
      "19     14               iperionhs.tar   0.00 GB\n",
      "20      4               knowmad_1.tar  10.08 GB\n",
      "21     19               knowmad_2.tar  10.07 GB\n",
      "22     18               knowmad_3.tar  10.06 GB\n",
      "23     21               knowmad_4.tar  10.04 GB\n",
      "24     20               knowmad_5.tar   5.37 GB\n",
      "25     13          lifewatch-eric.tar   0.05 GB\n",
      "26     30                     mes.tar   0.38 GB\n",
      "27     32     neanias-atmospheric.tar   0.57 GB\n",
      "28     33           neanias-space.tar   0.73 GB\n",
      "29     31      neanias-underwater.tar   0.02 GB\n",
      "30      3           netherlands_1.tar   3.91 GB\n",
      "31     36                      ni.tar   3.01 GB\n",
      "32     17  north-american-studies.tar   0.36 GB\n",
      "33     34    rural-digital-europe.tar   0.83 GB\n",
      "34     16                 sdsn-gr.tar   0.05 GB\n",
      "35     35                   tunet.tar   0.05 GB\n",
      "36      2               uarctic_1.tar   9.75 GB\n",
      "Selected file: argo-france.tar\n",
      "Download link: https://zenodo.org/api/records/14887484/files/argo-france.tar/content\n",
      "Checksum: md5:2ce6b0fcc6f876b600207759a0dc9758\n"
     ]
    }
   ],
   "source": [
    "# Print a reindexed list of available tar files\n",
    "print(\"Available tar files:\")\n",
    "print(df_tar_files[['filename', 'size']].reset_index())\n",
    "\n",
    "import signal\n",
    "\n",
    "# Function to handle timeout\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutError\n",
    "\n",
    "# Set the timeout handler for the input\n",
    "signal.signal(signal.SIGALRM, timeout_handler)\n",
    "signal.alarm(10)  # Set the timeout to 10 seconds\n",
    "\n",
    "try:\n",
    "    # Ask the user to select a tar file by its index\n",
    "    selected_index = int(input(\"Enter the index of the tar file you want to download: \"))\n",
    "except TimeoutError:\n",
    "    print(\"No response received. Defaulting to index 1.\")\n",
    "    selected_index = 1\n",
    "finally:\n",
    "    signal.alarm(0)  # Disable the alarm\n",
    "\n",
    "# Get the selected tar file's download link and checksum\n",
    "selected_file = df_tar_files.iloc[selected_index]\n",
    "downloadlink = selected_file['downloadlink']\n",
    "checksum = selected_file['checksum']\n",
    "\n",
    "print(f\"Selected file: {selected_file['filename']}\")\n",
    "print(f\"Download link: {downloadlink}\")\n",
    "print(f\"Checksum: {checksum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name: argo-france.tar\n",
      "Download Path File: ./data/2025-02-19/01_input/argo-france.tar\n",
      "Folder Name: argo-france\n",
      "Extraction Path Folder: ./data/2025-02-19/02_extracted/argo-france\n"
     ]
    }
   ],
   "source": [
    "# Path Variables\n",
    "\n",
    "# Extract the file name from the selected file\n",
    "file_name = selected_file['filename']    \n",
    "\n",
    "# Path to save the downloaded tar file using file_name variable\n",
    "download_path = f\"./data/{publication_date}/01_input/{file_name}\"\n",
    "\n",
    "# Create the folder name by removing the .tar extension\n",
    "folder_name = selected_file['filename'].replace('.tar', '')\n",
    "\n",
    "# Path to save the extracted files using the file_name variable without the .tar extension\n",
    "extraction_path = f\"./data/{publication_date}/02_extracted/{folder_name}\"\n",
    "\n",
    "\n",
    "print(f\"File Name: {file_name}\")\n",
    "print(f\"Download Path File: {download_path}\")\n",
    "print(f\"Folder Name: {folder_name}\")\n",
    "print(f\"Extraction Path Folder: {extraction_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the tar file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists: ./data/2025-02-19/01_input/argo-france.tar\n",
      "Download URL: https://zenodo.org/api/records/14887484/files/argo-france.tar/content\n",
      "Download complete: ./data/2025-02-19/01_input/argo-france.tar\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Ensure the directory for the download path exists\n",
    "os.makedirs(os.path.dirname(download_path), exist_ok=True)\n",
    "\n",
    "# Check if the file already exists\n",
    "if not os.path.exists(download_path):\n",
    "    # Get the file size in bytes\n",
    "    file_size_bytes = float(selected_file['size'].split()[0]) * (1024**3)  # Convert GB to bytes\n",
    "    print(f\"Downloading file: {selected_file['filename']} ({selected_file['size']})\")\n",
    "    print(f\"Download URL: {downloadlink}\")\n",
    "    \n",
    "    # Estimate download duration assuming an average speed of 10 MB/s\n",
    "    avg_speed = 10 * (1024**2)  # 10 MB/s in bytes\n",
    "    estimated_duration = file_size_bytes / avg_speed\n",
    "    print(f\"Estimated download time: {estimated_duration:.2f} seconds\")\n",
    "    \n",
    "    # Download the selected tar file\n",
    "    response = requests.get(downloadlink, stream=True)\n",
    "    with open(download_path, 'wb') as file:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            file.write(chunk)\n",
    "else:\n",
    "    print(f\"File already exists: {download_path}\")\n",
    "    print(f\"Download URL: {downloadlink}\")\n",
    "\n",
    "print(f\"Download complete: {download_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checksum verification passed.\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "# Function to calculate the checksum of a file\n",
    "def calculate_checksum(file_path, algorithm):\n",
    "    hash_func = hashlib.new(algorithm)\n",
    "    with open(file_path, 'rb') as f:\n",
    "        while chunk := f.read(8192):\n",
    "            hash_func.update(chunk)\n",
    "    return hash_func.hexdigest()\n",
    "\n",
    "# Extract the checksum algorithm and value\n",
    "checksum_parts = checksum.split(':', 1)\n",
    "checksum_algorithm = checksum_parts[0]\n",
    "expected_checksum = checksum_parts[1]\n",
    "\n",
    "# Calculate the checksum of the downloaded file\n",
    "calculated_checksum = calculate_checksum(download_path, algorithm=checksum_algorithm)\n",
    "\n",
    "# Compare the calculated checksum with the provided checksum\n",
    "if calculated_checksum == expected_checksum:\n",
    "    print(\"Checksum verification passed.\")\n",
    "else:\n",
    "    print(\"Checksum verification failed.\")\n",
    "    print(f\"Expected: {expected_checksum}\")\n",
    "    print(f\"Calculated: {calculated_checksum}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract the tar file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tar file has already been extracted.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if the extraction directory already exists and contains files\n",
    "if os.path.exists(extraction_path) and os.listdir(extraction_path):\n",
    "    print(\"The tar file has already been extracted.\")\n",
    "else:\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(extraction_path, exist_ok=True)\n",
    "\n",
    "    # Extract the tar file in the parent directory of the extraction_path - because the tar file contains a folder structure repeating the name of the tar file\n",
    "    print(f\"Extracting {download_path} to {extraction_path}...\")\n",
    "    parent_extraction_path = os.path.dirname(extraction_path)\n",
    "    with tarfile.open(download_path, 'r') as tar:\n",
    "        tar.extractall(path=parent_extraction_path)\n",
    "\n",
    "    print(\"Extraction complete.\")\n",
    "    print(f\"Files extracted to: {extraction_path}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 285\n",
      "First 5 files:\n",
      "part-00000-2c0de614-bb18-4931-bd6a-64f101a27baf-c000.json.gz\n",
      "part-00001-2c0de614-bb18-4931-bd6a-64f101a27baf-c000.json.gz\n",
      "part-00002-2c0de614-bb18-4931-bd6a-64f101a27baf-c000.json.gz\n",
      "part-00003-2c0de614-bb18-4931-bd6a-64f101a27baf-c000.json.gz\n",
      "part-00004-2c0de614-bb18-4931-bd6a-64f101a27baf-c000.json.gz\n",
      "                                              filename\n",
      "0    part-00000-2c0de614-bb18-4931-bd6a-64f101a27ba...\n",
      "1    part-00001-2c0de614-bb18-4931-bd6a-64f101a27ba...\n",
      "2    part-00002-2c0de614-bb18-4931-bd6a-64f101a27ba...\n",
      "3    part-00003-2c0de614-bb18-4931-bd6a-64f101a27ba...\n",
      "4    part-00004-2c0de614-bb18-4931-bd6a-64f101a27ba...\n",
      "..                                                 ...\n",
      "280  part-00580-2c0de614-bb18-4931-bd6a-64f101a27ba...\n",
      "281  part-00583-2c0de614-bb18-4931-bd6a-64f101a27ba...\n",
      "282  part-00592-2c0de614-bb18-4931-bd6a-64f101a27ba...\n",
      "283  part-00618-2c0de614-bb18-4931-bd6a-64f101a27ba...\n",
      "284  part-00736-2c0de614-bb18-4931-bd6a-64f101a27ba...\n",
      "\n",
      "[285 rows x 1 columns]\n",
      "DataFrame dimensions: (285, 1)\n",
      "Randomly selected files with full paths for testing:\n",
      "./data/2025-02-19/02_extracted/argo-france/part-00245-2c0de614-bb18-4931-bd6a-64f101a27baf-c000.json.gz\n",
      "./data/2025-02-19/02_extracted/argo-france/part-00388-2c0de614-bb18-4931-bd6a-64f101a27baf-c000.json.gz\n",
      "./data/2025-02-19/02_extracted/argo-france/part-00736-2c0de614-bb18-4931-bd6a-64f101a27baf-c000.json.gz\n",
      "./data/2025-02-19/02_extracted/argo-france/part-00218-2c0de614-bb18-4931-bd6a-64f101a27baf-c000.json.gz\n",
      "./data/2025-02-19/02_extracted/argo-france/part-00200-2c0de614-bb18-4931-bd6a-64f101a27baf-c000.json.gz\n",
      "Random file selected for later use: part-00335-2c0de614-bb18-4931-bd6a-64f101a27baf-c000.json.gz\n",
      "Path to the random file: ./data/2025-02-19/02_extracted/argo-france/part-00335-2c0de614-bb18-4931-bd6a-64f101a27baf-c000.json.gz\n",
      "The random file exists: ./data/2025-02-19/02_extracted/argo-france/part-00335-2c0de614-bb18-4931-bd6a-64f101a27baf-c000.json.gz\n"
     ]
    }
   ],
   "source": [
    "# List the extracted files\n",
    "extracted_files = os.listdir(extraction_path)\n",
    "\n",
    "# add the path to the extracted files\n",
    "extracted_files_with_path = [os.path.join(extraction_path, file) for file in extracted_files]\n",
    "\n",
    "# count the number of files in the extracted folder\n",
    "num_files = len(extracted_files)\n",
    "print(f\"Number of files: {num_files}\")\n",
    "\n",
    "# print the first 5 files\n",
    "print(\"First 5 files:\")\n",
    "for file in extracted_files[:5]:\n",
    "    print(file) \n",
    "\n",
    "# make a DataFrame for the extracted files\n",
    "df_extracted_files = pd.DataFrame(extracted_files, columns=['filename'])\n",
    "# Sort the DataFrame by filename alphabetically\n",
    "df_extracted_files = df_extracted_files.sort_values(by='filename')\n",
    "# Print the DataFrame\n",
    "print(df_extracted_files)\n",
    "\n",
    "# print the dimensions of the DataFrame\n",
    "print(f\"DataFrame dimensions: {df_extracted_files.shape}\")\n",
    "\n",
    "# print a random 5 files, to be used for testing, and use in a variable for later use\n",
    "import random\n",
    "random_files = random.sample(extracted_files, 5)\n",
    "random_files_with_path = [os.path.join(extraction_path, file) for file in random_files]\n",
    "print(\"Randomly selected files with full paths for testing:\")\n",
    "for file in random_files_with_path:\n",
    "    print(file)\n",
    "\n",
    "# one random file for later use\n",
    "random_file = random.choice(extracted_files)\n",
    "print(f\"Random file selected for later use: {random_file}\")\n",
    "# Define the path to the random file\n",
    "random_file_path = os.path.join(extraction_path, random_file)\n",
    "print(f\"Path to the random file: {random_file_path}\")\n",
    "# Check if the random file exists\n",
    "if os.path.exists(random_file_path):\n",
    "    print(f\"The random file exists: {random_file_path}\")\n",
    "else:\n",
    "    print(f\"The random file does not exist: {random_file_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Get a data sample to generate parquetfile and the SQL schema\n",
    "We do this before we process the bulk of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output file path: ./data/2025-02-19/03_transformed/argo-france/argo-france-sample.parquet\n",
      "Multiple sample file path: ./data/2025-02-19/03_transformed/argo-france/argo-france-multiple-sample.parquet\n",
      "Master file path: ./data/2025-02-19/03_transformed/argo-france/argo-france-master.parquet\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "transformation_folder_path = f\"./data/{publication_date}/03_transformed/{folder_name}\"\n",
    "\n",
    "# Ensure the target directory exists\n",
    "os.makedirs(transformation_folder_path, exist_ok=True)\n",
    "\n",
    "# for testing: Define and print the target output sample file path\n",
    "sample_file = f\"{transformation_folder_path}/{folder_name}-sample.parquet\"\n",
    "print(f\"Output file path: {sample_file}\")\n",
    "\n",
    "# for testing: define and print the target output sample file for the multiple selected random sample files\n",
    "multiple_sample_file = f\"{transformation_folder_path}/{folder_name}-multiple-sample.parquet\"\n",
    "print(f\"Multiple sample file path: {multiple_sample_file}\")\n",
    "\n",
    "# for production: define and print the target output master file for all extracted files\n",
    "master_file = f\"{transformation_folder_path}/{folder_name}-master.parquet\"\n",
    "print(f\"Master file path: {master_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for testing: this part is for running on a single sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed data saved to: ./data/2025-02-19/03_transformed/argo-france/argo-france-sample.parquet\n",
      "File size: 0.06 MB\n"
     ]
    }
   ],
   "source": [
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Use DuckDB to process the extracted JSON files and save a sample of all rows as a Parquet file\n",
    "con.sql(f'''\n",
    "    COPY (\n",
    "        \n",
    "        FROM (\n",
    "            SELECT *\n",
    "            FROM read_json('{random_file_path}', sample_size=-1, union_by_name=true)\n",
    "        )\n",
    "    )\n",
    "    TO '{sample_file}' (FORMAT parquet, COMPRESSION gzip)\n",
    "''')\n",
    "\n",
    "print(f\"Transformed data saved to: {sample_file}\")\n",
    "print(f\"File size: {os.path.getsize(sample_file) / (1024**2):.2f} MB\")\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema file path: ./data/2025-02-19/03_transformed/argo-france/argo-france-schema.sql\n",
      "Schema file exists: ./data/2025-02-19/03_transformed/argo-france/argo-france-schema.sql\n",
      "Schema file content:\n",
      "column_name,column_type,null,key,default,extra\n",
      "authors,\"STRUCT(fullName VARCHAR, \"\"name\"\" VARCHAR, rank BIGINT, surname VARCHAR, pid STRUCT(id STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR), provenance STRUCT(provenance VARCHAR, trust VARCHAR)))[]\",YES,,,\n",
      "bestAccessRight,\"STRUCT(code VARCHAR, \"\"label\"\" VARCHAR, scheme VARCHAR)\",YES,,,\n",
      "collectedFrom,\"STRUCT(\"\"key\"\" VARCHAR, \"\"value\"\" VARCHAR)[]\",YES,,,\n",
      "communities,\"STRUCT(code VARCHAR, \"\"label\"\" VARCHAR, provenance STRUCT(provenance VARCHAR, trust VARCHAR)[])[]\",YES,,,\n",
      "container,\"STRUCT(ep VARCHAR, issnOnline VARCHAR, issnPrinted VARCHAR, \"\"name\"\" VARCHAR, sp VARCHAR, vol VARCHAR)\",YES,,,\n",
      "contributors,VARCHAR[],YES,,,\n",
      "countries,\"STRUCT(code VARCHAR, \"\"label\"\" VARCHAR, provenance STRUCT(provenance VARCHAR, trust VARCHAR))[]\",YES,,,\n",
      "coverages,JSON[],YES,,,\n",
      "dateOfCollection,VARCHAR,YES,,,\n",
      "descriptions,VARCHAR[],YES,,,\n",
      "formats,VARCHAR[],YES,,,\n",
      "id,VARCHAR,YES,,,\n",
      "indicators,\"STRUCT(citationImpact STRUCT(citationClass VARCHAR, citationCount DOUBLE, impulse DOUBLE, impulseClass VARCHAR, influence DOUBLE, influenceClass VARCHAR, popularity DOUBLE, popularityClass VARCHAR))\",YES,,,\n",
      "instances,\"STRUCT(accessRight STRUCT(code VARCHAR, \"\"label\"\" VARCHAR, scheme VARCHAR, openAccessRoute VARCHAR), alternateIdentifiers STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR)[], collectedFrom STRUCT(\"\"key\"\" VARCHAR, \"\"value\"\" VARCHAR), hostedBy STRUCT(\"\"key\"\" VARCHAR, \"\"value\"\" VARCHAR), license VARCHAR, pids STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR)[], publicationDate DATE, refereed VARCHAR, \"\"type\"\" VARCHAR, urls VARCHAR[])[]\",YES,,,\n",
      "isGreen,BOOLEAN,YES,,,\n",
      "isInDiamondJournal,BOOLEAN,YES,,,\n",
      "language,\"STRUCT(code VARCHAR, \"\"label\"\" VARCHAR)\",YES,,,\n",
      "lastUpdateTimeStamp,BIGINT,YES,,,\n",
      "mainTitle,VARCHAR,YES,,,\n",
      "openAccessColor,VARCHAR,YES,,,\n",
      "organizations,\"STRUCT(id VARCHAR, legalName VARCHAR, pids STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR)[])[]\",YES,,,\n",
      "originalIds,VARCHAR[],YES,,,\n",
      "pids,\"STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR)[]\",YES,,,\n",
      "projects,\"STRUCT(acronym VARCHAR, code VARCHAR, funder STRUCT(fundingStream VARCHAR, jurisdiction VARCHAR, \"\"name\"\" VARCHAR, shortName VARCHAR), id VARCHAR, provenance STRUCT(provenance VARCHAR, trust VARCHAR), title VARCHAR, validated STRUCT(validatedByFunder BOOLEAN, validationDate DATE))[]\",YES,,,\n",
      "publicationDate,DATE,YES,,,\n",
      "publiclyFunded,BOOLEAN,YES,,,\n",
      "publisher,VARCHAR,YES,,,\n",
      "sources,VARCHAR[],YES,,,\n",
      "subjects,\"STRUCT(provenance STRUCT(provenance VARCHAR, trust VARCHAR), subject STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR))[]\",YES,,,\n",
      "type,VARCHAR,YES,,,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# schema path\n",
    "schema_file_path = f\"{transformation_folder_path}/{folder_name}-schema.sql\"\n",
    "\n",
    "#print the schema file path\n",
    "print(f\"Schema file path: {schema_file_path}\")\n",
    "\n",
    "duckdb.sql(f'''\n",
    "    COPY (\n",
    "        SELECT *\n",
    "        FROM (DESCRIBE '{sample_file}')\n",
    "    )\n",
    "    TO '{schema_file_path}'\n",
    "''')\n",
    "# check if the schema file exists\n",
    "if os.path.exists(schema_file_path):\n",
    "    print(f\"Schema file exists: {schema_file_path}\")\n",
    "else:\n",
    "    print(f\"Schema file does not exist: {schema_file_path}\")\n",
    "# Print the schema file content\n",
    "with open(schema_file_path, 'r') as schema_file:\n",
    "    schema_content = schema_file.read()\n",
    "    print(\"Schema file content:\")\n",
    "    print(schema_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in the Parquet file: 10\n"
     ]
    }
   ],
   "source": [
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Count the number of records in the Parquet file\n",
    "record_count = con.sql(f'''\n",
    "    SELECT COUNT(*)\n",
    "    FROM read_parquet('{sample_file}')\n",
    "''').fetchone()[0]\n",
    "\n",
    "# Print the record count\n",
    "print(f\"Number of records in the Parquet file: {record_count}\")\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titles in the Parquet file:\n",
      "Correction of profiles of in‐situ chlorophyll fluorometry for the contribution of fluorescence originating from non‐algal matter\n",
      "Plankton Assemblage Estimated with BGC‐Argo Floats in the Southern Ocean: Implications for Seasonal Successions and Particle Export\n",
      "Main processes of the Atlantic cold tongue interannual variability\n",
      "A Simplified Model for the Baroclinic and Barotropic Ocean Response to Moving Tropical Cyclones: 2. Model and Simulations\n",
      "Atmospherically Forced and Chaotic Interannual Variability of Regional Sea Level and Its Components Over 1993–2015\n",
      "Advances in operational oceanography : expanding Europe's ocean observing and forecasting capacity\n",
      "CDOM Spatiotemporal Variability in the Mediterranean Sea: A Modelling Study\n",
      "Report on new products\n",
      "A European strategy plan with regard to the Argo extension in WBC and other boundary regions\n",
      "Recommendations to operate shallow coastal float in European Marginal Seas\n"
     ]
    }
   ],
   "source": [
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Query the titles from the Parquet file\n",
    "titles = con.sql(f'''\n",
    "    SELECT mainTitle\n",
    "    FROM read_parquet('{sample_file}')\n",
    "''').fetchall()\n",
    "\n",
    "# Print the titles\n",
    "print(\"Titles in the Parquet file:\")\n",
    "for title in titles:\n",
    "    print(title[0])\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for testing: this part is for running on a random sample of multiple files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed data saved to: ./data/2025-02-19/03_transformed/argo-france/argo-france-multiple-sample.parquet\n",
      "File size: 0.05 MB\n"
     ]
    }
   ],
   "source": [
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Use DuckDB to process the extracted JSON files and save a sample of all rows as a Parquet file\n",
    "# Join the list of file paths into a comma-separated string\n",
    "file_paths = ','.join(f\"'{file}'\" for file in random_files_with_path)\n",
    "\n",
    "# Use DuckDB to process the extracted JSON files and save a sample of all rows as a Parquet file\n",
    "con.sql(f'''\n",
    "    COPY (\n",
    "        SELECT *\n",
    "        FROM read_json([{file_paths}], sample_size=-1, union_by_name=true)\n",
    "    )\n",
    "    TO '{multiple_sample_file}' (FORMAT parquet, COMPRESSION gzip)\n",
    "''')\n",
    "\n",
    "print(f\"Transformed data saved to: {multiple_sample_file}\")\n",
    "print(f\"File size: {os.path.getsize(multiple_sample_file) / (1024**2):.2f} MB\")\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema file path: ./data/2025-02-19/03_transformed/argo-france/argo-france-schema.sql\n",
      "Schema file exists: ./data/2025-02-19/03_transformed/argo-france/argo-france-schema.sql\n",
      "Schema file content:\n",
      "column_name,column_type,null,key,default,extra\n",
      "authors,\"STRUCT(fullName VARCHAR, \"\"name\"\" VARCHAR, pid STRUCT(id STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR), provenance STRUCT(provenance VARCHAR, trust VARCHAR)), rank BIGINT, surname VARCHAR)[]\",YES,,,\n",
      "bestAccessRight,\"STRUCT(code VARCHAR, \"\"label\"\" VARCHAR, scheme VARCHAR)\",YES,,,\n",
      "collectedFrom,\"STRUCT(\"\"key\"\" VARCHAR, \"\"value\"\" VARCHAR)[]\",YES,,,\n",
      "communities,\"STRUCT(code VARCHAR, \"\"label\"\" VARCHAR, provenance STRUCT(provenance VARCHAR, trust VARCHAR)[])[]\",YES,,,\n",
      "container,\"STRUCT(issnOnline VARCHAR, \"\"name\"\" VARCHAR, vol VARCHAR, ep VARCHAR, issnPrinted VARCHAR, sp VARCHAR)\",YES,,,\n",
      "contributors,VARCHAR[],YES,,,\n",
      "countries,\"STRUCT(code VARCHAR, \"\"label\"\" VARCHAR, provenance STRUCT(provenance VARCHAR, trust VARCHAR))[]\",YES,,,\n",
      "coverages,JSON[],YES,,,\n",
      "dateOfCollection,VARCHAR,YES,,,\n",
      "descriptions,VARCHAR[],YES,,,\n",
      "formats,VARCHAR[],YES,,,\n",
      "id,VARCHAR,YES,,,\n",
      "indicators,\"STRUCT(citationImpact STRUCT(citationClass VARCHAR, citationCount DOUBLE, impulse DOUBLE, impulseClass VARCHAR, influence DOUBLE, influenceClass VARCHAR, popularity DOUBLE, popularityClass VARCHAR), usageCounts STRUCT(downloads BIGINT, \"\"views\"\" BIGINT))\",YES,,,\n",
      "instances,\"STRUCT(accessRight STRUCT(code VARCHAR, \"\"label\"\" VARCHAR, openAccessRoute VARCHAR, scheme VARCHAR), alternateIdentifiers STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR)[], collectedFrom STRUCT(\"\"key\"\" VARCHAR, \"\"value\"\" VARCHAR), hostedBy STRUCT(\"\"key\"\" VARCHAR, \"\"value\"\" VARCHAR), license VARCHAR, pids STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR)[], publicationDate DATE, refereed VARCHAR, \"\"type\"\" VARCHAR, urls VARCHAR[], articleProcessingCharge STRUCT(amount VARCHAR, currency VARCHAR))[]\",YES,,,\n",
      "isGreen,BOOLEAN,YES,,,\n",
      "isInDiamondJournal,BOOLEAN,YES,,,\n",
      "language,\"STRUCT(code VARCHAR, \"\"label\"\" VARCHAR)\",YES,,,\n",
      "lastUpdateTimeStamp,BIGINT,YES,,,\n",
      "mainTitle,VARCHAR,YES,,,\n",
      "openAccessColor,VARCHAR,YES,,,\n",
      "organizations,\"STRUCT(id VARCHAR, legalName VARCHAR, pids STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR)[])[]\",YES,,,\n",
      "originalIds,VARCHAR[],YES,,,\n",
      "pids,\"STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR)[]\",YES,,,\n",
      "projects,\"STRUCT(acronym VARCHAR, code VARCHAR, funder STRUCT(fundingStream VARCHAR, jurisdiction VARCHAR, \"\"name\"\" VARCHAR, shortName VARCHAR), id VARCHAR, provenance STRUCT(provenance VARCHAR, trust VARCHAR), title VARCHAR, validated STRUCT(validatedByFunder BOOLEAN, validationDate DATE))[]\",YES,,,\n",
      "publicationDate,DATE,YES,,,\n",
      "publiclyFunded,BOOLEAN,YES,,,\n",
      "publisher,VARCHAR,YES,,,\n",
      "sources,VARCHAR[],YES,,,\n",
      "subjects,\"STRUCT(provenance STRUCT(provenance VARCHAR, trust VARCHAR), subject STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR))[]\",YES,,,\n",
      "type,VARCHAR,YES,,,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# schema path\n",
    "schema_file_path = f\"{transformation_folder_path}/{folder_name}-schema.sql\"\n",
    "\n",
    "#print the schema file path\n",
    "print(f\"Schema file path: {schema_file_path}\")\n",
    "\n",
    "duckdb.sql(f'''\n",
    "    COPY (\n",
    "        SELECT *\n",
    "        FROM (DESCRIBE '{multiple_sample_file}')\n",
    "    )\n",
    "    TO '{schema_file_path}'\n",
    "''')\n",
    "# check if the schema file exists\n",
    "if os.path.exists(schema_file_path):\n",
    "    print(f\"Schema file exists: {schema_file_path}\")\n",
    "else:\n",
    "    print(f\"Schema file does not exist: {schema_file_path}\")\n",
    "# Print the schema file content\n",
    "with open(schema_file_path, 'r') as schema_file:\n",
    "    schema_content = schema_file.read()\n",
    "    print(\"Schema file content:\")\n",
    "    print(schema_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in the Parquet file: 8\n"
     ]
    }
   ],
   "source": [
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Count the number of records in the Parquet file\n",
    "record_count = con.sql(f'''\n",
    "    SELECT COUNT(*)\n",
    "    FROM read_parquet('{multiple_sample_file}')\n",
    "''').fetchone()[0]\n",
    "\n",
    "# Print the record count\n",
    "print(f\"Number of records in the Parquet file: {record_count}\")\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titles in the Parquet file:\n",
      "Biogeochemical Argo: The Test Case of the NAOS Mediterranean Array\n",
      "Recommendations to increase the overall life expectancy of Argo floats, based on at-sea monitoring fleet behavior monitoring, assessment and report (including a review of metadata that impact life expectancy: specific float configurations, batteries)\n",
      "Monitoring the Oceans and Climate Change with Argo. MOCCA project 5 – year achievements\n",
      "Dissolved Organic Nitrogen Production and Export by Meridional Overturning in the Eastern Subpolar North Atlantic\n",
      "A new record of Atlantic sea surface salinity from 1896 to 2013 reveals the signatures of climate variability and long‐term trends\n",
      "How Deep Argo Will Improve the Deep Ocean in an Ocean Reanalysis\n",
      "CORA-IBI, Coriolis Ocean Dataset for Reanalysis for the Ireland-Biscay-Iberia region\n",
      "Spreading and Vertical Structure of the Persian Gulf and Red Sea Outflows in the Northwestern Indian Ocean\n"
     ]
    }
   ],
   "source": [
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Query the titles from the Parquet file\n",
    "titles = con.sql(f'''\n",
    "    SELECT mainTitle\n",
    "    FROM read_parquet('{multiple_sample_file}')\n",
    "''').fetchall()\n",
    "\n",
    "# Print the titles\n",
    "print(\"Titles in the Parquet file:\")\n",
    "for title in titles:\n",
    "    print(title[0])\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for production: parsing all extracted files into one master parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master file already exists: ./data/2025-02-19/03_transformed/argo-france/argo-france-master.parquet\n",
      "Do you want to overwrite it? (y/n) [Default: n, timeout 10s]:\n",
      "Using the existing master file.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import signal\n",
    "\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutError\n",
    "\n",
    "# Check if the master file already exists\n",
    "if os.path.exists(master_file):\n",
    "    print(f\"Master file already exists: {master_file}\")\n",
    "    print(\"Do you want to overwrite it? (y/n) [Default: n, timeout 10s]:\")\n",
    "    signal.signal(signal.SIGALRM, timeout_handler)\n",
    "    signal.alarm(10)\n",
    "    try:\n",
    "        user_input = input()\n",
    "        overwrite = user_input.strip().lower() == 'y'\n",
    "    except TimeoutError:\n",
    "        print(\"No response received. Continuing with the existing master file.\")\n",
    "        overwrite = False\n",
    "    finally:\n",
    "        signal.alarm(0)\n",
    "    if not overwrite:\n",
    "        print(\"Using the existing master file.\")\n",
    "    else:\n",
    "        # Overwrite: regenerate the master file\n",
    "        con = duckdb.connect()\n",
    "        file_paths = ','.join(f\"'{file}'\" for file in extracted_files_with_path)\n",
    "        con.sql(f'''\n",
    "            COPY (\n",
    "                SELECT *\n",
    "                FROM read_json([{file_paths}], sample_size=-1, union_by_name=true)\n",
    "            )\n",
    "            TO '{master_file}' (FORMAT parquet, COMPRESSION gzip)\n",
    "        ''')\n",
    "        print(f\"Transformed data saved to: {master_file}\")\n",
    "        print(f\"File size: {os.path.getsize(master_file) / (1024**2):.2f} MB\")\n",
    "        con.close()\n",
    "else:\n",
    "    # Master file does not exist, create it\n",
    "    con = duckdb.connect()\n",
    "    file_paths = ','.join(f\"'{file}'\" for file in extracted_files_with_path)\n",
    "    con.sql(f'''\n",
    "        COPY (\n",
    "            SELECT *\n",
    "            FROM read_json([{file_paths}], sample_size=-1, union_by_name=true)\n",
    "        )\n",
    "        TO '{master_file}' (FORMAT parquet, COMPRESSION gzip)\n",
    "    ''')\n",
    "    print(f\"Transformed data saved to: {master_file}\")\n",
    "    print(f\"File size: {os.path.getsize(master_file) / (1024**2):.2f} MB\")\n",
    "    con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema file path: ./data/2025-02-19/03_transformed/argo-france/argo-france-schema.sql\n",
      "Schema file exists: ./data/2025-02-19/03_transformed/argo-france/argo-france-schema.sql\n",
      "Schema file content:\n",
      "column_name,column_type,null,key,default,extra\n",
      "authors,\"STRUCT(fullName VARCHAR, \"\"name\"\" VARCHAR, pid STRUCT(id STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR), provenance STRUCT(provenance VARCHAR, trust VARCHAR)), rank BIGINT, surname VARCHAR)[]\",YES,,,\n",
      "bestAccessRight,\"STRUCT(code VARCHAR, \"\"label\"\" VARCHAR, scheme VARCHAR)\",YES,,,\n",
      "collectedFrom,\"STRUCT(\"\"key\"\" VARCHAR, \"\"value\"\" VARCHAR)[]\",YES,,,\n",
      "communities,\"STRUCT(code VARCHAR, \"\"label\"\" VARCHAR, provenance STRUCT(provenance VARCHAR, trust VARCHAR)[])[]\",YES,,,\n",
      "container,\"STRUCT(ep VARCHAR, issnOnline VARCHAR, issnPrinted VARCHAR, \"\"name\"\" VARCHAR, sp VARCHAR, vol VARCHAR, edition VARCHAR, iss VARCHAR, issnLinking VARCHAR)\",YES,,,\n",
      "contributors,VARCHAR[],YES,,,\n",
      "countries,\"STRUCT(code VARCHAR, \"\"label\"\" VARCHAR, provenance STRUCT(provenance VARCHAR, trust VARCHAR))[]\",YES,,,\n",
      "coverages,JSON[],YES,,,\n",
      "dateOfCollection,VARCHAR,YES,,,\n",
      "descriptions,VARCHAR[],YES,,,\n",
      "formats,VARCHAR[],YES,,,\n",
      "id,VARCHAR,YES,,,\n",
      "indicators,\"STRUCT(citationImpact STRUCT(citationClass VARCHAR, citationCount DOUBLE, impulse DOUBLE, impulseClass VARCHAR, influence DOUBLE, influenceClass VARCHAR, popularity DOUBLE, popularityClass VARCHAR), usageCounts STRUCT(downloads BIGINT, \"\"views\"\" BIGINT))\",YES,,,\n",
      "instances,\"STRUCT(accessRight STRUCT(code VARCHAR, \"\"label\"\" VARCHAR, openAccessRoute VARCHAR, scheme VARCHAR), alternateIdentifiers STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR)[], collectedFrom STRUCT(\"\"key\"\" VARCHAR, \"\"value\"\" VARCHAR), hostedBy STRUCT(\"\"key\"\" VARCHAR, \"\"value\"\" VARCHAR), license VARCHAR, pids STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR)[], publicationDate DATE, refereed VARCHAR, \"\"type\"\" VARCHAR, urls VARCHAR[], articleProcessingCharge STRUCT(amount VARCHAR, currency VARCHAR))[]\",YES,,,\n",
      "isGreen,BOOLEAN,YES,,,\n",
      "isInDiamondJournal,BOOLEAN,YES,,,\n",
      "language,\"STRUCT(code VARCHAR, \"\"label\"\" VARCHAR)\",YES,,,\n",
      "lastUpdateTimeStamp,BIGINT,YES,,,\n",
      "mainTitle,VARCHAR,YES,,,\n",
      "openAccessColor,VARCHAR,YES,,,\n",
      "organizations,\"STRUCT(id VARCHAR, legalName VARCHAR, pids STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR)[])[]\",YES,,,\n",
      "originalIds,VARCHAR[],YES,,,\n",
      "pids,\"STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR)[]\",YES,,,\n",
      "projects,\"STRUCT(acronym VARCHAR, code VARCHAR, funder STRUCT(fundingStream VARCHAR, jurisdiction VARCHAR, \"\"name\"\" VARCHAR, shortName VARCHAR), id VARCHAR, provenance STRUCT(provenance VARCHAR, trust VARCHAR), title VARCHAR, validated STRUCT(validatedByFunder BOOLEAN, validationDate DATE))[]\",YES,,,\n",
      "publicationDate,DATE,YES,,,\n",
      "publiclyFunded,BOOLEAN,YES,,,\n",
      "publisher,VARCHAR,YES,,,\n",
      "sources,VARCHAR[],YES,,,\n",
      "subjects,\"STRUCT(provenance STRUCT(provenance VARCHAR, trust VARCHAR), subject STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR))[]\",YES,,,\n",
      "type,VARCHAR,YES,,,\n",
      "subTitle,VARCHAR,YES,,,\n",
      "embargoEndDate,DATE,YES,,,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# schema path\n",
    "schema_file_path = f\"{transformation_folder_path}/{folder_name}-schema.sql\"\n",
    "\n",
    "#print the schema file path\n",
    "print(f\"Schema file path: {schema_file_path}\")\n",
    "\n",
    "duckdb.sql(f'''\n",
    "    COPY (\n",
    "        SELECT *\n",
    "        FROM (DESCRIBE '{master_file}')\n",
    "    )\n",
    "    TO '{schema_file_path}'\n",
    "''')\n",
    "# check if the schema file exists\n",
    "if os.path.exists(schema_file_path):\n",
    "    print(f\"Schema file exists: {schema_file_path}\")\n",
    "else:\n",
    "    print(f\"Schema file does not exist: {schema_file_path}\")\n",
    "# Print the schema file content\n",
    "with open(schema_file_path, 'r') as schema_file:\n",
    "    schema_content = schema_file.read()\n",
    "    print(\"Schema file content:\")\n",
    "    print(schema_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in the Parquet file: 674\n"
     ]
    }
   ],
   "source": [
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Count the number of records in the Parquet file\n",
    "record_count = con.sql(f'''\n",
    "    SELECT COUNT(*)\n",
    "    FROM read_parquet('{master_file}')\n",
    "''').fetchone()[0]\n",
    "\n",
    "# Print the record count\n",
    "print(f\"Number of records in the Parquet file: {record_count}\")\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Random Titles in the Parquet file:\n",
      "Intercomparison and validation of the mixed layer depth fields of global ocean syntheses\n",
      "Baltic Sea workshop report\n",
      "3D Structure of the Ras Al Hadd Oceanic Dipole\n",
      "QC Report. AtlantOS project\n",
      "Deep mixed ocean volume in the Labrador Sea in HighResMIP models\n",
      "CTD DATA - EUREC4A_OA Atalante Cruise\n",
      "Budget of organic carbon in the <scp>N</scp>orth‐<scp>W</scp>estern <scp>M</scp>editerranean open sea over the period 2004–2008 using 3‐D coupled physical‐biogeochemical modeling\n",
      "Applications and Challenges of GRACE and GRACE Follow-On Satellite Gravimetry\n",
      "Recommendations to increase the overall life expectancy of Argo floats, based on at-sea monitoring fleet behavior monitoring, assessment and report (including a review of metadata that impact life expectancy: specific float configurations, batteries)\n",
      "CERA‐20C: A Coupled Reanalysis of the Twentieth Century\n",
      "Number of unique titles in the Parquet file: 666\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Query all titles from the Parquet file\n",
    "titles = con.sql(f'''\n",
    "    SELECT mainTitle\n",
    "    FROM read_parquet('{master_file}')\n",
    "''').fetchall()\n",
    "\n",
    "# Select 10 random titles\n",
    "random_titles = random.sample([title[0] for title in titles if title[0]], min(10, len(titles)))\n",
    "\n",
    "print(\"10 Random Titles in the Parquet file:\")\n",
    "for title in random_titles:\n",
    "    print(title)\n",
    "\n",
    "# print the number of unique titles\n",
    "unique_titles = set(title[0] for title in titles if title[0])\n",
    "print(f\"Number of unique titles in the Parquet file: {len(unique_titles)}\")\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Random DOIs:\n",
      "10.1038/s41467-020-14474-y\n",
      "10.1016/j.ocemod.2018.11.005\n",
      "10.5194/osd-10-1127-2013\n",
      "10.1038/s41598-018-27407-z\n",
      "10.3389/fmars.2023.1287867\n",
      "10.5281/zenodo.7369190\n",
      "10.5067/ghgoy-4fe01\n",
      "10.3389/fmars.2019.00519\n",
      "10.5194/os-18-129-2022\n",
      "10.1029/2021jc017999\n",
      "Total number of DOIs: 709\n",
      "Number of unique DOIs: 709\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "# Query to extract DOIs from the pids column\n",
    "dois = con.sql(f'''\n",
    "    SELECT unnest.value AS doi\n",
    "    FROM read_parquet('{master_file}')\n",
    "    CROSS JOIN UNNEST(pids) AS unnest\n",
    "    WHERE unnest.scheme = 'doi'\n",
    "''').fetchall()\n",
    "\n",
    "# Select 10 random DOIs\n",
    "random_dois = random.sample([doi[0] for doi in dois if doi[0]], min(10, len(dois)))\n",
    "\n",
    "# Print the 10 random DOIs\n",
    "print(\"10 Random DOIs:\")\n",
    "for doi in random_dois:\n",
    "    print(doi)\n",
    "\n",
    "# print total number of DOIs\n",
    "print(f\"Total number of DOIs: {len(dois)}\")\n",
    "\n",
    "# print the number of unique DOIs\n",
    "unique_dois = set(doi[0] for doi in dois if doi[0])\n",
    "print(f\"Number of unique DOIs: {len(unique_dois)}\")\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct PID schemes in the master table:\n",
      "doi\n",
      "mag_id\n",
      "arXiv\n",
      "handle\n",
      "pmid\n",
      "pmc\n"
     ]
    }
   ],
   "source": [
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Query to extract distinct PID schemes from the master file\n",
    "pid_schemes = con.sql(f'''\n",
    "    SELECT DISTINCT unnest.scheme AS scheme\n",
    "    FROM read_parquet('{master_file}')\n",
    "    CROSS JOIN UNNEST(pids) AS unnest\n",
    "''').fetchall()\n",
    "\n",
    "# Print the distinct PID schemes\n",
    "print(\"Distinct PID schemes in the master table:\")\n",
    "for scheme in pid_schemes:\n",
    "    print(scheme[0])\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIDs grouped by schemes:\n",
      "   scheme                                               pids\n",
      "0  handle  [20.500.14243/381862, 1871/48380, 1912/27589, ...\n",
      "1    pmid  [31996687, 32978152, 35865129, 31875863, 30659...\n",
      "2     pmc  [PMC6989661, PMC7518875, PMC9287098, PMC691659...\n",
      "3     doi  [10.1175/jpo-d-16-0107.1, 10.1002/2016jc012629...\n",
      "4  mag_id  [2497128190, 2601592524, 2332474861, 205755597...\n",
      "5   arXiv                  [http://arxiv.org/abs/1607.08469]\n"
     ]
    }
   ],
   "source": [
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Query to extract PIDs grouped by their schemes\n",
    "pids_by_scheme = con.sql(f'''\n",
    "    SELECT unnest.scheme AS scheme, LIST(unnest.value) AS pids\n",
    "    FROM read_parquet('{master_file}')\n",
    "    CROSS JOIN UNNEST(pids) AS unnest\n",
    "    GROUP BY unnest.scheme\n",
    "''').fetchall()\n",
    "\n",
    "# dataframe to hold the PIDs grouped by their schemes\n",
    "df_pids_by_scheme = pd.DataFrame(pids_by_scheme, columns=['scheme', 'pids'])\n",
    "# Print the DataFrame of PIDs grouped by their schemes\n",
    "print(\"PIDs grouped by schemes:\")  \n",
    "print(df_pids_by_scheme)\n",
    " \n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geting ready for further processing the master data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create a folder for processed data\n",
    "processing_folder_path = f\"./data/{publication_date}/04_processed/{folder_name}\"\n",
    "\n",
    "# Ensure the target directory exists\n",
    "os.makedirs(processing_folder_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Get the DOI's and other identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined id and pid:\n",
      "                                           record_id pid_scheme  \\\n",
      "0     doi_dedup___::fe2c8a61b8ccaa6515d3c2996b2144c9        doi   \n",
      "1     doi_dedup___::fe2c8a61b8ccaa6515d3c2996b2144c9     mag_id   \n",
      "2     doi_dedup___::7bd738ce5851f7e450ebf6388ad51522        doi   \n",
      "3     doi_dedup___::7bd738ce5851f7e450ebf6388ad51522     mag_id   \n",
      "4     doi_dedup___::7bd738ce5851f7e450ebf6388ad51522     handle   \n",
      "...                                              ...        ...   \n",
      "1392  doi_dedup___::da9603d028d4bd4ad2d9c980917fd5ac        doi   \n",
      "1393  doi_dedup___::3da738372eaf79174655e4ef24d74cd2        doi   \n",
      "1394  doi_dedup___::3da738372eaf79174655e4ef24d74cd2        doi   \n",
      "1395  doi_dedup___::3da738372eaf79174655e4ef24d74cd2        doi   \n",
      "1396  doi_dedup___::3da738372eaf79174655e4ef24d74cd2        doi   \n",
      "\n",
      "                    pid_value  \\\n",
      "0     10.1175/jpo-d-16-0107.1   \n",
      "1                  2497128190   \n",
      "2        10.1002/2016jc012629   \n",
      "3                  2601592524   \n",
      "4         20.500.14243/381862   \n",
      "...                       ...   \n",
      "1392   10.5281/zenodo.1137795   \n",
      "1393   10.5281/zenodo.4009263   \n",
      "1394   10.5281/zenodo.4010160   \n",
      "1395   10.5281/zenodo.6343858   \n",
      "1396   10.5281/zenodo.4009264   \n",
      "\n",
      "                                        combined_id_pid  \n",
      "0     doi_dedup___::fe2c8a61b8ccaa6515d3c2996b2144c9...  \n",
      "1     doi_dedup___::fe2c8a61b8ccaa6515d3c2996b2144c9...  \n",
      "2     doi_dedup___::7bd738ce5851f7e450ebf6388ad51522...  \n",
      "3     doi_dedup___::7bd738ce5851f7e450ebf6388ad51522...  \n",
      "4     doi_dedup___::7bd738ce5851f7e450ebf6388ad51522...  \n",
      "...                                                 ...  \n",
      "1392  doi_dedup___::da9603d028d4bd4ad2d9c980917fd5ac...  \n",
      "1393  doi_dedup___::3da738372eaf79174655e4ef24d74cd2...  \n",
      "1394  doi_dedup___::3da738372eaf79174655e4ef24d74cd2...  \n",
      "1395  doi_dedup___::3da738372eaf79174655e4ef24d74cd2...  \n",
      "1396  doi_dedup___::3da738372eaf79174655e4ef24d74cd2...  \n",
      "\n",
      "[1397 rows x 4 columns]\n",
      "Combined data saved to: ./data/2025-02-19/04_processed/argo-france/argo-france-combined-id-pid.parquet\n",
      "File size: 0.08 MB\n"
     ]
    }
   ],
   "source": [
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Query to combine the id with each pid\n",
    "combined_data = con.sql(f'''\n",
    "    SELECT \n",
    "        id AS record_id,\n",
    "        unnest.scheme AS pid_scheme,\n",
    "        unnest.value AS pid_value,\n",
    "        CONCAT(id, '_', unnest.value) AS combined_id_pid\n",
    "    FROM read_parquet('{master_file}')\n",
    "    CROSS JOIN UNNEST(pids) AS unnest\n",
    "''').fetchdf()\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(\"Combined id and pid:\")\n",
    "print(combined_data)\n",
    "\n",
    "\n",
    "# Save the combined data to a new Parquet file for later use\n",
    "combined_file_path = f\"{processing_folder_path}/{folder_name}-combined-id-pid.parquet\"\n",
    "combined_data.to_parquet(combined_file_path, index=False)\n",
    "print(f\"Combined data saved to: {combined_file_path}\")\n",
    "print(f\"File size: {os.path.getsize(combined_file_path) / (1024**2):.2f} MB\")\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Get Altmetric data\n",
    "\n",
    "a. use the PIDS (df_pids_by_scheme) along with the record id (to be used as primary keys, connecting the tables later on), \n",
    "\n",
    "b. get mention data by parsing the pids over the altmetric API,\n",
    "\n",
    "c. save the outcomes in a separate parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Get Overton data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Get SDG classification labels\n",
    "\n",
    "a. Query the abstracts first along with the id (to be used as primary keys, connecting the tables later on), \n",
    "\n",
    "b. get sdg data by parsing the abstracts with more than 100 tokens over an LLM API with system prompt, \n",
    "\n",
    "c. save the outcomes in a separate parquet file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### step 7a: Get the abstracts, including the record id and the number of tokens i nthe abstract\n",
    "\n",
    "Number of tokens are important later on, less then 100 tokens in the abstract deliver low quality SDG classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptions with token counts:\n",
      "                                          record_id  \\\n",
      "0    doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef   \n",
      "1    doi_dedup___::e246801fc9ed25782358bac694517f8f   \n",
      "2    doi_dedup___::fe2c8a61b8ccaa6515d3c2996b2144c9   \n",
      "3    doi_dedup___::7bd738ce5851f7e450ebf6388ad51522   \n",
      "4    doi_dedup___::bf1098713a38f89cb9c67a7f59401107   \n",
      "..                                              ...   \n",
      "650  doi_dedup___::3894f0d63b65411c0d289bf831716e48   \n",
      "651  doi_dedup___::a96d857fd60b818f7bde9aa0c99bfc3f   \n",
      "652  doi_dedup___::551f2ca097a75326cc2e7561f831d38b   \n",
      "653  doi_dedup___::da9603d028d4bd4ad2d9c980917fd5ac   \n",
      "654  doi_dedup___::3da738372eaf79174655e4ef24d74cd2   \n",
      "\n",
      "                                           description  token_count  \n",
      "0    Abstract</jats:title><jats:p>The Black Sea, th...          161  \n",
      "1     The early twenty-first century’s warming tren...          247  \n",
      "2    Abstract</jats:title><jats:p>The semienclosed ...          226  \n",
      "3    Abstract</jats:title><jats:p>Identification of...          249  \n",
      "4    This study focuses on the interaction between ...          165  \n",
      "..                                                 ...          ...  \n",
      "650  Development of innovative 3D web based Argo Da...            9  \n",
      "651                      Data Management Plan document            4  \n",
      "652         Report of the 2nd Ocean Observers workshop            7  \n",
      "653  Provides Matlab functions as well as example w...           31  \n",
      "654  Argo is a real-time global ocean in situ obser...          120  \n",
      "\n",
      "[655 rows x 3 columns]\n",
      "Description data saved to: ./data/2025-02-19/04_processed/argo-france/argo-france-descriptions-with-tokens.parquet\n",
      "File size: 0.47 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "# Query to extract the ID, description, remove XML tags, and calculate the number of tokens in the description\n",
    "description_data = con.sql(f'''\n",
    "    SELECT \n",
    "        id AS record_id,\n",
    "        regexp_replace(descriptions[1], '<[^>]+>', '') AS description,  -- Remove XML tags\n",
    "        array_length(split(regexp_replace(descriptions[1], '<[^>]+>', ''), ' ')) AS token_count\n",
    "    FROM read_parquet('{master_file}')\n",
    "    WHERE descriptions IS NOT NULL AND array_length(descriptions) > 0\n",
    "''').fetchdf()\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(\"Descriptions with token counts:\")\n",
    "print(description_data)\n",
    "\n",
    "# Save the data to a new Parquet file for later use\n",
    "description_file_path = f\"{processing_folder_path}/{folder_name}-descriptions-with-tokens.parquet\"\n",
    "description_data.to_parquet(description_file_path, index=False)\n",
    "print(f\"Description data saved to: {description_file_path}\")\n",
    "print(f\"File size: {os.path.getsize(description_file_path) / (1024**2):.2f} MB\")\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### step 7b Get the official definitions of the SDG's from https://metadata.un.org/sdg/ using the Accept header application/rdf+xml\n",
    "\n",
    "First we get the links to the top level goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDG definitions saved to: ./data/2025-02-19/04_processed/argo-france/sdg_definitions.rdf\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL for the SDG metadata\n",
    "sdg_metadata_url = \"https://metadata.un.org/sdg/\"\n",
    "\n",
    "# Set the headers to request RDF/XML format\n",
    "headers = {\n",
    "    \"Accept\": \"application/rdf+xml\"\n",
    "}\n",
    "\n",
    "# Send the GET request\n",
    "response = requests.get(sdg_metadata_url, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Save the RDF/XML content to a file\n",
    "    rdf_file_path = f\"{processing_folder_path}/sdg_definitions.rdf\"\n",
    "    with open(rdf_file_path, \"wb\") as rdf_file:\n",
    "        rdf_file.write(response.content)\n",
    "    print(f\"SDG definitions saved to: {rdf_file_path}\")\n",
    "else:\n",
    "    print(f\"Failed to fetch SDG definitions. Status code: {response.status_code}\")\n",
    "    print(f\"Response: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top concept URLs found in the RDF/XML:\n",
      "http://metadata.un.org/sdg/1\n",
      "http://metadata.un.org/sdg/2\n",
      "http://metadata.un.org/sdg/3\n",
      "http://metadata.un.org/sdg/4\n",
      "http://metadata.un.org/sdg/5\n",
      "http://metadata.un.org/sdg/6\n",
      "http://metadata.un.org/sdg/7\n",
      "http://metadata.un.org/sdg/8\n",
      "http://metadata.un.org/sdg/9\n",
      "http://metadata.un.org/sdg/10\n",
      "http://metadata.un.org/sdg/11\n",
      "http://metadata.un.org/sdg/12\n",
      "http://metadata.un.org/sdg/13\n",
      "http://metadata.un.org/sdg/14\n",
      "http://metadata.un.org/sdg/15\n",
      "http://metadata.un.org/sdg/16\n",
      "http://metadata.un.org/sdg/17\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Parse the RDF/XML file\n",
    "tree = ET.parse(rdf_file_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "# Find all skos:hasTopConcept elements and extract their rdf:resource attribute\n",
    "top_concept_urls = []\n",
    "for elem in root.findall('.//{http://www.w3.org/2004/02/skos/core#}hasTopConcept'):\n",
    "    url = elem.attrib.get('{http://www.w3.org/1999/02/22-rdf-syntax-ns#}resource')\n",
    "    if url:\n",
    "        top_concept_urls.append(url)\n",
    "\n",
    "# sort the URLs based on the integer in the last part of the URL\n",
    "top_concept_urls.sort(key=lambda x: int(x.split('/')[-1]))\n",
    "\n",
    "print(\"Top concept URLs found in the RDF/XML:\")\n",
    "for url in top_concept_urls:\n",
    "    print(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we get the goal number, goal name and goal description for each top level goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   goal_number                               goal_title  \\\n",
      "0            1                               No poverty   \n",
      "1            2                              Zero hunger   \n",
      "2            3               Good health and well-being   \n",
      "3            4                        Quality education   \n",
      "4            5                          Gender equality   \n",
      "5            6               Clean water and sanitation   \n",
      "6            7              Affordable and clean energy   \n",
      "7            8          Decent work and economic growth   \n",
      "8            9  Industry, innovation and infrastructure   \n",
      "9           10                     Reduced inequalities   \n",
      "10          11       Sustainable cities and communities   \n",
      "11          12   Responsible consumption and production   \n",
      "12          13                           Climate action   \n",
      "13          14                         Life below water   \n",
      "14          15                             Life on land   \n",
      "15          16   Peace, justice and strong institutions   \n",
      "16          17               Partnerships for the goals   \n",
      "\n",
      "                                     goal_description  \\\n",
      "0             End poverty in all its forms everywhere   \n",
      "1   End hunger, achieve food security and improved...   \n",
      "2   Ensure healthy lives and promote well-being fo...   \n",
      "3   Ensure inclusive and equitable quality educati...   \n",
      "4   Achieve gender equality and empower all women ...   \n",
      "5   Ensure availability and sustainable management...   \n",
      "6   Ensure access to affordable, reliable, sustain...   \n",
      "7   Promote sustained, inclusive and sustainable e...   \n",
      "8   Build resilient infrastructure, promote inclus...   \n",
      "9        Reduce inequality within and among countries   \n",
      "10  Make cities and human settlements inclusive, s...   \n",
      "11  Ensure sustainable consumption and production ...   \n",
      "12  Take urgent action to combat climate change an...   \n",
      "13  Conserve and sustainably use the oceans, seas ...   \n",
      "14  Protect, restore and promote sustainable use o...   \n",
      "15  Promote peaceful and inclusive societies for s...   \n",
      "16  Strengthen the means of implementation and rev...   \n",
      "\n",
      "                         goal_url  \n",
      "0    http://metadata.un.org/sdg/1  \n",
      "1    http://metadata.un.org/sdg/2  \n",
      "2    http://metadata.un.org/sdg/3  \n",
      "3    http://metadata.un.org/sdg/4  \n",
      "4    http://metadata.un.org/sdg/5  \n",
      "5    http://metadata.un.org/sdg/6  \n",
      "6    http://metadata.un.org/sdg/7  \n",
      "7    http://metadata.un.org/sdg/8  \n",
      "8    http://metadata.un.org/sdg/9  \n",
      "9   http://metadata.un.org/sdg/10  \n",
      "10  http://metadata.un.org/sdg/11  \n",
      "11  http://metadata.un.org/sdg/12  \n",
      "12  http://metadata.un.org/sdg/13  \n",
      "13  http://metadata.un.org/sdg/14  \n",
      "14  http://metadata.un.org/sdg/15  \n",
      "15  http://metadata.un.org/sdg/16  \n",
      "16  http://metadata.un.org/sdg/17  \n",
      "SDG goals saved to: ./data/2025-02-19/04_processed/argo-france/sdg_goals.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Prepare lists to store the results\n",
    "goal_numbers = []\n",
    "goal_titles = []\n",
    "goal_descriptions = []\n",
    "goal_urls = []\n",
    "\n",
    "# Loop through each top concept URL\n",
    "for url in top_concept_urls:\n",
    "    try:\n",
    "        # Fetch the RDF/XML content\n",
    "        resp = requests.get(url, headers={\"Accept\": \"application/rdf+xml\"})\n",
    "        resp.raise_for_status()\n",
    "        root = ET.fromstring(resp.content)\n",
    "        # Find the main Description element\n",
    "        desc = root.find('.//{http://www.w3.org/1999/02/22-rdf-syntax-ns#}Description')\n",
    "        if desc is None:\n",
    "            continue\n",
    "        # Extract <skos:note xml:lang=\"en\">Goal N</skos:note>\n",
    "        goal_number = None\n",
    "        for note in desc.findall('{http://www.w3.org/2004/02/skos/core#}note'):\n",
    "            if note.attrib.get('{http://www.w3.org/XML/1998/namespace}lang') == 'en' and note.text and note.text.startswith('Goal'):\n",
    "                goal_number = note.text.replace('Goal ', '').strip()\n",
    "                break\n",
    "        # Extract <skos:altLabel xml:lang=\"en\">...</skos:altLabel>\n",
    "        goal_title = None\n",
    "        for alt in desc.findall('{http://www.w3.org/2004/02/skos/core#}altLabel'):\n",
    "            if alt.attrib.get('{http://www.w3.org/XML/1998/namespace}lang') == 'en':\n",
    "                goal_title = alt.text.strip()\n",
    "                break\n",
    "        # Extract <skos:prefLabel xml:lang=\"en\">...</skos:prefLabel>\n",
    "        goal_description = None\n",
    "        for pref in desc.findall('{http://www.w3.org/2004/02/skos/core#}prefLabel'):\n",
    "            if pref.attrib.get('{http://www.w3.org/XML/1998/namespace}lang') == 'en':\n",
    "                goal_description = pref.text.strip()\n",
    "                break\n",
    "        # Store results\n",
    "        goal_numbers.append(goal_number)\n",
    "        goal_titles.append(goal_title)\n",
    "        goal_descriptions.append(goal_description)\n",
    "        goal_urls.append(url)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {e}\")\n",
    "\n",
    "# Create DataFrame\n",
    "df_sdg_goals = pd.DataFrame({\n",
    "    \"goal_number\": goal_numbers,\n",
    "    \"goal_title\": goal_titles,\n",
    "    \"goal_description\": goal_descriptions,\n",
    "    \"goal_url\": goal_urls\n",
    "})\n",
    "\n",
    "print(df_sdg_goals)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "sdg_goals_csv_path = f\"{processing_folder_path}/sdg_goals.csv\"\n",
    "df_sdg_goals.to_csv(sdg_goals_csv_path, index=False)\n",
    "print(f\"SDG goals saved to: {sdg_goals_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 7c Here we prepare the System and User prompts to be used by an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text to classify:\n",
      "\n",
      "The United Nations Sustainable Development Goals (SDGs) are a universal call to action to end poverty, protect the planet, and ensure prosperity for all by 2030. They address global challenges such as inequality, climate change, environmental degradation, peace, and justice. The SDGs consist of 17 goals and 169 targets that aim to achieve a better and more sustainable future for all.\n",
      "\n",
      "Example Output Format:\n",
      "\n",
      "{\n",
      "    \"sdgs\": [2, 6, 17],\n",
      "    \"explanation\": \"This text is related to SDG 2 (Zero hunger) because it discusses food security, SDG 6 (Clean water and sanitation) due to references to environmental protection, and SDG 17 (Partnerships for the goals) as it mentions global cooperation.\"\n",
      "}\n",
      "\n",
      "System Prompt:\n",
      "\n",
      "You are an intelligent multi-label classification system designed to map texts to their relevant Sustainable Development Goals.\n",
      "Take the text delimited by triple quotation marks and return a JSON list of relevant SDGs. \n",
      "Example output format: \n",
      "{\n",
      "    \"sdgs\": [2, 6, 17],\n",
      "    \"explanation\": \"This text is related to SDG 2 (Zero hunger) because it discusses food security, SDG 6 (Clean water and sanitation) due to references to environmental protection, and SDG 17 (Partnerships for the goals) as it mentions global cooperation.\"\n",
      "}\n",
      "\n",
      "\n",
      "Here are the SDG goals and their descriptions:\n",
      "1: No poverty - End poverty in all its forms everywhere\n",
      "2: Zero hunger - End hunger, achieve food security and improved nutrition and promote sustainable agriculture\n",
      "3: Good health and well-being - Ensure healthy lives and promote well-being for all at all ages\n",
      "4: Quality education - Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all\n",
      "5: Gender equality - Achieve gender equality and empower all women and girls\n",
      "6: Clean water and sanitation - Ensure availability and sustainable management of water and sanitation for all\n",
      "7: Affordable and clean energy - Ensure access to affordable, reliable, sustainable and modern energy for all\n",
      "8: Decent work and economic growth - Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all\n",
      "9: Industry, innovation and infrastructure - Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation\n",
      "10: Reduced inequalities - Reduce inequality within and among countries\n",
      "11: Sustainable cities and communities - Make cities and human settlements inclusive, safe, resilient and sustainable\n",
      "12: Responsible consumption and production - Ensure sustainable consumption and production patterns\n",
      "13: Climate action - Take urgent action to combat climate change and its impacts\n",
      "14: Life below water - Conserve and sustainably use the oceans, seas and marine resources for sustainable development\n",
      "15: Life on land - Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss\n",
      "16: Peace, justice and strong institutions - Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels\n",
      "17: Partnerships for the goals - Strengthen the means of implementation and revitalize the Global Partnership for Sustainable Development\n",
      "\n",
      "\n",
      "User Prompt:\n",
      "\n",
      "\"Classify the following text in terms of its relevance to the Sustainable Development Goals:\",\n",
      "Text: '''\n",
      "The United Nations Sustainable Development Goals (SDGs) are a universal call to action to end poverty, protect the planet, and ensure prosperity for all by 2030. They address global challenges such as inequality, climate change, environmental degradation, peace, and justice. The SDGs consist of 17 goals and 169 targets that aim to achieve a better and more sustainable future for all.\n",
      "'''\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the text to classify\n",
    "text = \"\"\"\n",
    "The United Nations Sustainable Development Goals (SDGs) are a universal call to action to end poverty, protect the planet, and ensure prosperity for all by 2030. They address global challenges such as inequality, climate change, environmental degradation, peace, and justice. The SDGs consist of 17 goals and 169 targets that aim to achieve a better and more sustainable future for all.\n",
    "\"\"\"\n",
    "# Print the text to classify\n",
    "print(\"Text to classify:\")\n",
    "print(text)\n",
    "\n",
    "# Define the expected output format, now including an explanation field\n",
    "example_output_format = \"\"\"\n",
    "{\n",
    "    \"sdgs\": [2, 6, 17],\n",
    "    \"explanation\": \"This text is related to SDG 2 (Zero hunger) because it discusses food security, SDG 6 (Clean water and sanitation) due to references to environmental protection, and SDG 17 (Partnerships for the goals) as it mentions global cooperation.\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Print the example output format\n",
    "print(\"Example Output Format:\")\n",
    "print(example_output_format)\n",
    "\n",
    "# system_prompt\n",
    "# Build SDG goal info string from df_sdg_goals\n",
    "sdg_goal_info = \"\\n\".join(\n",
    "    f\"{row.goal_number}: {row.goal_title} - {row.goal_description}\"\n",
    "    for _, row in df_sdg_goals.iterrows()\n",
    ")\n",
    "\n",
    "sdg_system_prompt = f\"\"\"\n",
    "You are an intelligent multi-label classification system designed to map texts to their relevant Sustainable Development Goals.\n",
    "Take the text delimited by triple quotation marks and return a JSON list of relevant SDGs. \n",
    "Example output format: {example_output_format}\n",
    "\n",
    "Here are the SDG goals and their descriptions:\n",
    "{sdg_goal_info}\n",
    "\n",
    "\"\"\"\n",
    "# Print the system prompt\n",
    "print(\"System Prompt:\")\n",
    "print(sdg_system_prompt)\n",
    "# user_prompt\n",
    "sdg_user_prompt = f\"\"\"\n",
    "\"Classify the following text in terms of its relevance to the Sustainable Development Goals:\",\n",
    "Text: '''{text}'''\n",
    "\"\"\"\n",
    "# Print the user prompt\n",
    "print(\"User Prompt:\")\n",
    "print(sdg_user_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 7d: Get the LLM API prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenWebUI API configuration\n",
    "openwebui_base_url = \"https://nebula.cs.vu.nl\"  # Replace with your actual OpenWebUI API base URL\n",
    "openwebui_api_key = \"sk-5b5a024888c14a019c0e9b4857df9329\"  # Replace with your actual API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first get the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "coffeescript"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "- id: deepseek-r1:1.5b, name: deepseek-r1:1.5b, parameter_size: 1.8B\n",
      "- id: deepseek-r1:8b, name: deepseek-r1:8b, parameter_size: 8.0B\n",
      "- id: llama3.1:8b, name: llama3.1:8b, parameter_size: 8.0B\n",
      "- id: qwen2.5:1.5b, name: qwen2.5:1.5b, parameter_size: 1.5B\n",
      "- id: qwen2.5:7b, name: qwen2.5:7b, parameter_size: 7.6B\n",
      "curl -X GET 'https://nebula.cs.vu.nl/api/models' -H 'Authorization: Bearer sk-5b5a024888c14a019c0e9b4857df9329'\n"
     ]
    }
   ],
   "source": [
    "# This script fetches the list of available models from the OpenWebUI API\n",
    "# and prints their IDs, names, and parameter sizes.\n",
    "\n",
    "import requests\n",
    "\n",
    "# Use the existing variables openwebui_base_url and openwebui_api_key\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {openwebui_api_key}\"\n",
    "}\n",
    "\n",
    "# Ensure the base URL does not end with a slash\n",
    "api_url = openwebui_base_url.rstrip('/') + \"/api/models\"\n",
    "\n",
    "# print the request in curl\n",
    "print(f\"curl -X GET '{api_url}' -H 'Authorization: Bearer {openwebui_api_key}'\")\n",
    "\n",
    "response = requests.get(api_url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    models_json = response.json()\n",
    "    models = models_json.get(\"data\", [])\n",
    "    print(\"Available models:\")\n",
    "    for model in models:\n",
    "        print(f\"- id: {model.get('id')}, name: {model.get('name')}, parameter_size: {model.get('ollama', {}).get('details', {}).get('parameter_size')}\")\n",
    "else:\n",
    "    print(f\"Failed to fetch models. Status code: {response.status_code}\")\n",
    "    print(f\"Response: {response.text}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the model to use, when no model is chosen, deepseek-r1:1.5b will be the default (faser & cheaper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "0: deepseek-r1:1.5b\n",
      "1: deepseek-r1:8b\n",
      "2: llama3.1:8b\n",
      "3: qwen2.5:1.5b\n",
      "4: qwen2.5:7b\n",
      "Select the model index to use (default: 0, deepseek-r1:1.5b) [timeout 10s]:\n",
      "Model selected: llama3.1:8b\n"
     ]
    }
   ],
   "source": [
    "import signal\n",
    "\n",
    "# Select the model to use, when no model is chosen, deepseek-r1:1.5b will be the default\n",
    "model = \"deepseek-r1:1.5b\"  # Replace with your actual model name\n",
    "\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutError\n",
    "\n",
    "print(\"Available models:\")\n",
    "for i, m in enumerate(models):\n",
    "    print(f\"{i}: {m['id']}\")\n",
    "\n",
    "print(\"Select the model index to use (default: 0, deepseek-r1:1.5b) [timeout 10s]:\")\n",
    "signal.signal(signal.SIGALRM, timeout_handler)\n",
    "signal.alarm(10)\n",
    "try:\n",
    "    user_input = input()\n",
    "    if user_input.strip().isdigit():\n",
    "        selected_model_index = int(user_input.strip())\n",
    "        if 0 <= selected_model_index < len(models):\n",
    "            model = models[selected_model_index]['id']\n",
    "        else:\n",
    "            print(\"Invalid index, using default model.\")\n",
    "            model = \"deepseek-r1:1.5b\"\n",
    "    else:\n",
    "        print(\"No valid input, using default model.\")\n",
    "        model = \"deepseek-r1:1.5b\"\n",
    "except TimeoutError:\n",
    "    print(\"No response received. Using default model.\")\n",
    "    model = \"deepseek-r1:1.5b\"\n",
    "finally:\n",
    "    signal.alarm(0)\n",
    "\n",
    "print(f\"Model selected: {model}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for each abstract, run the system and user prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for record_id doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef: {'model': 'llama3.1:8b', 'messages': [{'role': 'system', 'content': '\\nYou are an intelligent multi-label classification system designed to map texts to their relevant Sustainable Development Goals.\\nTake the text delimited by triple quotation marks and return a JSON list of relevant SDGs. \\nExample output format: \\n{\\n    \"sdgs\": [2, 6, 17],\\n    \"explanation\": \"This text is related to SDG 2 (Zero hunger) because it discusses food security, SDG 6 (Clean water and sanitation) due to references to environmental protection, and SDG 17 (Partnerships for the goals) as it mentions global cooperation.\"\\n}\\n\\n\\nHere are the SDG goals and their descriptions:\\n1: No poverty - End poverty in all its forms everywhere\\n2: Zero hunger - End hunger, achieve food security and improved nutrition and promote sustainable agriculture\\n3: Good health and well-being - Ensure healthy lives and promote well-being for all at all ages\\n4: Quality education - Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all\\n5: Gender equality - Achieve gender equality and empower all women and girls\\n6: Clean water and sanitation - Ensure availability and sustainable management of water and sanitation for all\\n7: Affordable and clean energy - Ensure access to affordable, reliable, sustainable and modern energy for all\\n8: Decent work and economic growth - Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all\\n9: Industry, innovation and infrastructure - Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation\\n10: Reduced inequalities - Reduce inequality within and among countries\\n11: Sustainable cities and communities - Make cities and human settlements inclusive, safe, resilient and sustainable\\n12: Responsible consumption and production - Ensure sustainable consumption and production patterns\\n13: Climate action - Take urgent action to combat climate change and its impacts\\n14: Life below water - Conserve and sustainably use the oceans, seas and marine resources for sustainable development\\n15: Life on land - Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss\\n16: Peace, justice and strong institutions - Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels\\n17: Partnerships for the goals - Strengthen the means of implementation and revitalize the Global Partnership for Sustainable Development\\n\\n'}, {'role': 'user', 'content': \"Classify the following text in terms of its relevance to the Sustainable Development Goals:\\nText: '''Abstract</jats:title><jats:p>The Black Sea, the largest semienclosed anoxic basin on Earth, can be considered as an excellent natural laboratory for oxic and anoxic biogeochemical processes. The suboxic zone, a thin interface between oxic and anoxic waters, still remains poorly understood because it has been undersampled. This has led to alternative concepts regarding the underlying processes that create it. Existing hypotheses suggest that the interface originates either by isopycnal intrusions that introduce oxygen or the dynamics of manganese redox cycling that are associated with the sinking of particles or chemosynthetic bacteria. Here we reexamine these concepts using high‐resolution oxygen, sulfide, nitrate, and particle concentration profiles obtained with sensors deployed on profiling floats. Our results show an extremely stable structure in density space over the entire basin with the exception of areas near the Bosporus plume and in the southern areas dominated by coastal anticyclones. The absence of large‐scale horizontal intrusive signatures in the open‐sea supports a hypothesis prioritizing the role of biogeochemical processes.</jats:p>'''\"}]}\n",
      "Response for record_id doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef: {'id': 'llama3.1:8b-f8f2584b-d6df-4223-9bb0-5c882c6d4b11', 'created': 1750335821, 'model': 'llama3.1:8b', 'choices': [{'index': 0, 'logprobs': None, 'finish_reason': 'stop', 'message': {'role': 'assistant', 'content': '{\\n  \"sdgs\": [6, 13],\\n  \"explanation\": \"This text is related to SDG 6 (Clean water and sanitation) because it discusses the Black Sea\\'s water quality and anoxic conditions. Additionally, it touches on environmental issues that align with SDG 13 (Climate action), particularly in its discussion of biogeochemical processes and their impact on the ocean ecosystem.\"\\n}'}}], 'object': 'chat.completion', 'usage': {'response_token/s': 7.28, 'prompt_token/s': 42.53, 'total_duration': 30401756197, 'load_duration': 968501093, 'prompt_eval_count': 755, 'prompt_tokens': 755, 'prompt_eval_duration': 17750099291, 'eval_count': 85, 'completion_tokens': 85, 'eval_duration': 11678037054, 'approximate_total': '0h0m30s', 'total_tokens': 840, 'completion_tokens_details': {'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}}}\n",
      "Processed record_id: doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef, SDGs: [6, 13]\n",
      "Data for record_id doi_dedup___::e246801fc9ed25782358bac694517f8f: {'model': 'llama3.1:8b', 'messages': [{'role': 'system', 'content': '\\nYou are an intelligent multi-label classification system designed to map texts to their relevant Sustainable Development Goals.\\nTake the text delimited by triple quotation marks and return a JSON list of relevant SDGs. \\nExample output format: \\n{\\n    \"sdgs\": [2, 6, 17],\\n    \"explanation\": \"This text is related to SDG 2 (Zero hunger) because it discusses food security, SDG 6 (Clean water and sanitation) due to references to environmental protection, and SDG 17 (Partnerships for the goals) as it mentions global cooperation.\"\\n}\\n\\n\\nHere are the SDG goals and their descriptions:\\n1: No poverty - End poverty in all its forms everywhere\\n2: Zero hunger - End hunger, achieve food security and improved nutrition and promote sustainable agriculture\\n3: Good health and well-being - Ensure healthy lives and promote well-being for all at all ages\\n4: Quality education - Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all\\n5: Gender equality - Achieve gender equality and empower all women and girls\\n6: Clean water and sanitation - Ensure availability and sustainable management of water and sanitation for all\\n7: Affordable and clean energy - Ensure access to affordable, reliable, sustainable and modern energy for all\\n8: Decent work and economic growth - Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all\\n9: Industry, innovation and infrastructure - Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation\\n10: Reduced inequalities - Reduce inequality within and among countries\\n11: Sustainable cities and communities - Make cities and human settlements inclusive, safe, resilient and sustainable\\n12: Responsible consumption and production - Ensure sustainable consumption and production patterns\\n13: Climate action - Take urgent action to combat climate change and its impacts\\n14: Life below water - Conserve and sustainably use the oceans, seas and marine resources for sustainable development\\n15: Life on land - Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss\\n16: Peace, justice and strong institutions - Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels\\n17: Partnerships for the goals - Strengthen the means of implementation and revitalize the Global Partnership for Sustainable Development\\n\\n'}, {'role': 'user', 'content': \"Classify the following text in terms of its relevance to the Sustainable Development Goals:\\nText: ''' The early twenty-first century’s warming trend of the full-depth global ocean is calculated by combining the analysis of Argo (top 2000 m) and repeat hydrography into a blended full-depth observing system. The surface-to-bottom temperature change over the last decade of sustained observation is equivalent to a heat uptake of 0.71 ± 0.09 W m<jats:sup>−2</jats:sup> applied over the surface of Earth, 90% of it being found above 2000-m depth. The authors decompose the temperature trend pointwise into changes in isopycnal depth (heave) and temperature changes along an isopycnal (spiciness) to describe the mechanisms controlling the variability. The heave component dominates the global heat content increase, with the largest trends found in the Southern Hemisphere’s extratropics (0–2000 m) highlighting a volumetric increase of subtropical mode waters. Significant heave-related warming is also found in the deep North Atlantic and Southern Oceans (2000–4000 m), reflecting a potential decrease in deep water mass renewal rates. The spiciness component shows its strongest contribution at intermediate levels (700–2000 m), with striking localized warming signals in regions of intense vertical mixing (North Atlantic and Southern Oceans). Finally, the agreement between the independent Argo and repeat hydrography temperature changes at 2000 m provides an overall good confidence in the blended heat content evaluation on global and ocean scales but also highlights basin-scale discrepancies between the two independent estimates. Those mismatches are largest in those basins with the largest heave signature (Southern Ocean) and reflect both the temporal and spatial sparseness of the hydrography sampling. </jats:p>'''\"}]}\n",
      "Response for record_id doi_dedup___::e246801fc9ed25782358bac694517f8f: {'id': 'llama3.1:8b-a2f29898-b5f0-45aa-a5f5-a3a533b54220', 'created': 1750335840, 'model': 'llama3.1:8b', 'choices': [{'index': 0, 'logprobs': None, 'finish_reason': 'stop', 'message': {'role': 'assistant', 'content': '{\\n    \"sdgs\": [13],\\n    \"explanation\": \"This text is primarily related to SDG 13 (Climate action) as it discusses the warming trend of the global ocean, heat uptake, and changes in ocean temperature and circulation patterns, which are all relevant to understanding and addressing climate change.\"\\n}'}}], 'object': 'chat.completion', 'usage': {'response_token/s': 7.28, 'prompt_token/s': 101.73, 'total_duration': 17666305795, 'load_duration': 34765806, 'prompt_eval_count': 885, 'prompt_tokens': 885, 'prompt_eval_duration': 8699732107, 'eval_count': 65, 'completion_tokens': 65, 'eval_duration': 8927851583, 'approximate_total': '0h0m17s', 'total_tokens': 950, 'completion_tokens_details': {'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}}}\n",
      "Processed record_id: doi_dedup___::e246801fc9ed25782358bac694517f8f, SDGs: [13]\n",
      "Number of SDG results collected: 0\n",
      "SDG LLM results saved to: ./data/2025-02-19/04_processed/argo-france/argo-france-sdg-results-llama3.1-8b.parquet\n",
      "File size: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Add a testing parameter to limit the number of abstracts\n",
    "testing_mode = True  # Set to True to limit to 2 abstracts for testing\n",
    "\n",
    "if testing_mode:\n",
    "    # Load only the first 2 abstracts for testing\n",
    "    description_df = pd.read_parquet(f\"{processing_folder_path}/{folder_name}-descriptions-with-tokens.parquet\").head(2)\n",
    "else:\n",
    "    # Load all abstracts for production\n",
    "    description_df = pd.read_parquet(f\"{processing_folder_path}/{folder_name}-descriptions-with-tokens.parquet\")                                                             \n",
    "\n",
    "# Filter abstracts with at least 100 tokens\n",
    "description_df = description_df[description_df['token_count'] >= 100]\n",
    "\n",
    "# Prepare results list\n",
    "sdg_results = []\n",
    "\n",
    "# Loop through each abstract\n",
    "for idx, row in description_df.iterrows():\n",
    "    record_id = row['record_id']\n",
    "    abstract = row['description']\n",
    "\n",
    "    # Prepare the messages for the API\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": sdg_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Classify the following text in terms of its relevance to the Sustainable Development Goals:\\nText: '''{abstract}'''\"}\n",
    "    ]\n",
    "\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages\n",
    "    }\n",
    "\n",
    "    # Print the data variable for debugging\n",
    "    print(f\"Data for record_id {record_id}: {data}\")\n",
    "\n",
    "    # Make the API call\n",
    "    response = requests.post(\n",
    "        openwebui_base_url.rstrip('/') + \"/api/chat/completions\",\n",
    "        headers={\"Authorization\": f\"Bearer {openwebui_api_key}\", \"Content-Type\": \"application/json\"},\n",
    "        json=data\n",
    "    )\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error processing record_id {record_id}: {response.status_code} - {response.text}\")\n",
    "        continue\n",
    "\n",
    "    # Print the response for debugging\n",
    "    print(f\"Response for record_id {record_id}: {response.json()}\")\n",
    "    \n",
    "    # Parse the response\n",
    "    try:\n",
    "        result = response.json()\n",
    "        # Try to extract the SDG list from the response\n",
    "        content = result['choices'][0]['message']['content']\n",
    "        # Try to parse the JSON from the model output\n",
    "        try:\n",
    "            sdg_json = eval(content) if isinstance(content, str) else content\n",
    "            sdgs = sdg_json.get(\"sdgs\", [])\n",
    "            explanation = sdg_json.get(\"explanation\", \"\")\n",
    "        except Exception:\n",
    "            sdgs = []\n",
    "            explanation = \"\"\n",
    "    except Exception:\n",
    "        sdgs = []\n",
    "        explanation = \"\"\n",
    "\n",
    "    # Append to results, including the explanation if available\n",
    "    sdg_results.append({\n",
    "        \"record_id\": record_id,\n",
    "        \"abstract\": abstract,\n",
    "        \"sdgs\": sdgs,\n",
    "        \"explanation\": explanation\n",
    "    })\n",
    "\n",
    "    # Optional: print progress\n",
    "    print(f\"Processed record_id: {record_id}, SDGs: {sdgs}\")\n",
    "\n",
    "    # Optional: delay to avoid rate limits\n",
    "    time.sleep(1)\n",
    "\n",
    "# Print the number of results\n",
    "print(f\"Number of SDG results collected: {len(sdg_results)}\")\n",
    "\n",
    "# Make the value of the model variable suitable for using in the file names\n",
    "model_filename = model.replace(\":\", \"-\").replace(\" \", \"_\")\n",
    "\n",
    "# Save results to parquet\n",
    "sdg_results_df = pd.DataFrame(sdg_results)\n",
    "sdg_results_path = f\"{processing_folder_path}/{folder_name}-sdg-results-{model_filename}.parquet\"\n",
    "sdg_results_df.to_parquet(sdg_results_path, index=False)\n",
    "print(f\"SDG LLM results saved to: {sdg_results_path}\")\n",
    "print(f\"File size: {os.path.getsize(sdg_results_path) / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Get Genderize data\n",
    "a. First Query the authors with country of the affiliation along with the record id (to be used as primary keys, connecting the tables later on), \n",
    "\n",
    "b. get gender data by parsing the author names with country label over an API, \n",
    "\n",
    "c. save the outcomes in a separate parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authors with full names and ORCID IDs:\n",
      "                                           record_id             full_name  \\\n",
      "0     doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef        Emil V. Stanev   \n",
      "1     doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef  Pierre‐Marie Poulain   \n",
      "2     doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef      Sebastian Grayek   \n",
      "3     doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef    Kenneth S. Johnson   \n",
      "4     doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef        Hervé Claustre   \n",
      "...                                              ...                   ...   \n",
      "5846  doi_dedup___::53e879864d1a55d055a8f2385005f5e0         Xiaogang Xing   \n",
      "5847  doi_dedup___::53e879864d1a55d055a8f2385005f5e0        Antoine Poteau   \n",
      "5848  doi_dedup___::53e879864d1a55d055a8f2385005f5e0     Giorgio Dall'Olmo   \n",
      "5849  doi_dedup___::53e879864d1a55d055a8f2385005f5e0        Annick Bricaud   \n",
      "5850  doi_dedup___::f872f5c02d3bce9bc6ae9962dbec5083                  Argo   \n",
      "\n",
      "        first_name last_name                orcid country_name country_code  \n",
      "0          Emil V.    Stanev  0000-0002-1110-8645       France           FR  \n",
      "1     Pierre‐Marie   Poulain  0000-0003-1342-8463       France           FR  \n",
      "2        Sebastian    Grayek  0000-0002-2461-757x       France           FR  \n",
      "3       Kenneth S.   Johnson  0000-0001-5513-5584       France           FR  \n",
      "4            Hervé  Claustre  0000-0001-6243-0258       France           FR  \n",
      "...            ...       ...                  ...          ...          ...  \n",
      "5846          None      None                 None        Italy           IT  \n",
      "5847          None      None  0000-0002-0519-5180        Italy           IT  \n",
      "5848          None      None  0000-0003-3931-4675        Italy           IT  \n",
      "5849          None      None                 None        Italy           IT  \n",
      "5850                                         None       Canada           CA  \n",
      "\n",
      "[5851 rows x 7 columns]\n",
      "Authors data saved to: ./data/2025-02-19/04_processed/argo-france/argo-france-authors.parquet\n",
      "File size: 0.14 MB\n"
     ]
    }
   ],
   "source": [
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Query to extract authors along with their full names and record IDs\n",
    "authors = con.sql(f'''\n",
    "    SELECT \n",
    "        id AS record_id,\n",
    "        unnest.fullName AS full_name,\n",
    "        unnest.name AS first_name,\n",
    "        unnest.surname AS last_name,\n",
    "        unnest.pid.id.value AS orcid,\n",
    "        countries[1].label AS country_name,\n",
    "        countries[1].code AS country_code\n",
    "    FROM read_parquet('{master_file}')\n",
    "    CROSS JOIN UNNEST(authors) AS unnest\n",
    "    WHERE countries IS NOT NULL AND array_length(countries) > 0\n",
    "''').fetchall()\n",
    "\n",
    "# convert the result to a DataFrame\n",
    "import pandas as pd\n",
    "authors_df = pd.DataFrame(authors, columns=['record_id', 'full_name', 'first_name', 'last_name', 'orcid', 'country_name', 'country_code'])\n",
    "# Print the authors DataFrame\n",
    "print(\"Authors with full names and ORCID IDs:\")\n",
    "print(authors_df)  \n",
    "\n",
    "# Save the authors data to a new Parquet file for later use\n",
    "authors_file_path = f\"{processing_folder_path}/{folder_name}-authors.parquet\"\n",
    "authors_df.to_parquet(authors_file_path, index=False)\n",
    "print(f\"Authors data saved to: {authors_file_path}\")\n",
    "print(f\"File size: {os.path.getsize(authors_file_path) / (1024**2):.2f} MB\")\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique authors linked to record IDs:\n",
      "                                           record_id    first_name  \\\n",
      "0     doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef          Emil   \n",
      "1     doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef  Pierre‐Marie   \n",
      "2     doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef     Sebastian   \n",
      "3     doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef       Kenneth   \n",
      "4     doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef         Hervé   \n",
      "...                                              ...           ...   \n",
      "5834  doi_dedup___::46a4623c5543eb6a03c0389effc47b8d       Nicolas   \n",
      "5835  doi_dedup___::46a4623c5543eb6a03c0389effc47b8d         Sally   \n",
      "5836  doi_dedup___::46a4623c5543eb6a03c0389effc47b8d       Thierry   \n",
      "5837  doi_dedup___::46a4623c5543eb6a03c0389effc47b8d     Jean-Marc   \n",
      "5838  doi_dedup___::46a4623c5543eb6a03c0389effc47b8d       Laurent   \n",
      "\n",
      "     country_code  \n",
      "0              FR  \n",
      "1              FR  \n",
      "2              FR  \n",
      "3              FR  \n",
      "4              FR  \n",
      "...           ...  \n",
      "5834           FR  \n",
      "5835           FR  \n",
      "5836           FR  \n",
      "5837           FR  \n",
      "5838           FR  \n",
      "\n",
      "[2747 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Filter the authors DataFrame to get unique names, countries, and record IDs\n",
    "# Only use the first occurrence of each first name\n",
    "unique_authors = authors_df[['record_id', 'first_name', 'country_code']].copy()\n",
    "unique_authors['first_name'] = unique_authors['first_name'].str.split().str[0]  # Keep only the first word\n",
    "# Remove one-letter names (e.g., \"L.\", \"S.\") that often end with a dot\n",
    "unique_authors = unique_authors[~unique_authors['first_name'].str.match(r'^[A-Z]\\.$', na=False)]\n",
    "# Drop rows where 'first_name' is None or NaN\n",
    "unique_authors = unique_authors.dropna(subset=['first_name'])\n",
    "unique_authors = unique_authors[unique_authors['first_name'] != 'None']\n",
    "unique_authors = unique_authors.drop_duplicates(subset=['first_name', 'record_id'], keep='first')\n",
    "\n",
    "# Print unique authors with record IDs\n",
    "print(\"Unique authors linked to record IDs:\")\n",
    "print(unique_authors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paid Subscription: False\n",
      "Testing Mode: True\n",
      "Rate Limit: 10 requests per second\n",
      "Delay between requests: 0.50 seconds\n"
     ]
    }
   ],
   "source": [
    "# Adding variables to handle rate limiting and API key for Genderize API\n",
    "\n",
    "# Check if the user has a paid subscription\n",
    "paid_subscription = False  # Set this to True if you have a paid subscription\n",
    "\n",
    "testing_mode = True  # Set to True for testing, False for production\n",
    "\n",
    "# Set the rate limit based on the testing mode\n",
    "if testing_mode:\n",
    "    rate_limit = 10  # Reduced rate limit for testing\n",
    "else:\n",
    "    rate_limit = 1000 if paid_subscription else 100 # Adjust rate limit based on subscription, setting a default for free users\n",
    "\n",
    "# delay between requests in seconds\n",
    "delay_between_requests = 0.5  # Calculate delay based on rate limit\n",
    "\n",
    "# Genderize API key\n",
    "genderize_api_key= \"da1a264b9bab63b46f27ac635dd7d2df\"  # Replace with your actual API key\n",
    "\n",
    "# Initialize request count\n",
    "request_count = 0  # Initialize request count\n",
    "\n",
    "# Base URL for Genderize API\n",
    "base_url = \"https://api.genderize.io\"\n",
    "\n",
    "# print all the above variables\n",
    "print(f\"Paid Subscription: {paid_subscription}\")\n",
    "print(f\"Testing Mode: {testing_mode}\")\n",
    "print(f\"Rate Limit: {rate_limit} requests per second\")\n",
    "print(f\"Delay between requests: {delay_between_requests:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Stopping for the day.\n",
      "Gender data saved to: ./data/2025-02-19/04_processed/argo-france/argo-france-gender-data.parquet\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "# Initialize the list to store gender results\n",
    "gender_results = []\n",
    "\n",
    "# Iterate over the unique authors\n",
    "for _, row in unique_authors.iterrows():\n",
    "    if request_count >= rate_limit: # type: ignore\n",
    "        print(\"Rate limit reached. Stopping for the day.\")\n",
    "        break\n",
    "\n",
    "    first_name = row['first_name']\n",
    "    country_code = row['country_code']\n",
    "    record_id = row['record_id']  # Add the record ID\n",
    "\n",
    "    # Skip if the first name is missing\n",
    "    if pd.isna(first_name):\n",
    "        continue\n",
    "\n",
    "    # Prepare the API request\n",
    "    params = {\n",
    "        \"name\": first_name,\n",
    "        \"country_id\": country_code\n",
    "    }\n",
    "    if paid_subscription:\n",
    "        params[\"apikey\"] = genderize_api_key\n",
    "\n",
    "    try:\n",
    "        # Send the request to the Genderize API\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        # Append the result to the list\n",
    "        gender_results.append({\n",
    "            \"first_name\": first_name,\n",
    "            \"country_code\": country_code,\n",
    "            \"gender\": data.get(\"gender\"),\n",
    "            \"probability\": data.get(\"probability\"),\n",
    "            \"count\": data.get(\"count\")\n",
    "        })\n",
    "\n",
    "        # Increment the request count\n",
    "        request_count += 1\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Processed: {first_name} ({country_code}) - Gender: {data.get('gender')}\")\n",
    "\n",
    "        # Add a delay between requests to avoid overwhelming the API\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error processing {first_name} ({country_code}): {e}\")\n",
    "\n",
    "        # Increment the request count\n",
    "        request_count += 1\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Processed: {first_name} ({country_code}) - Gender: {data.get('gender')}\")\n",
    "\n",
    "        # Add a small delay to avoid overwhelming the API\n",
    "        time.sleep(1)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error processing {first_name} ({country_code}): {e}\")\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "gender_df = pd.DataFrame(gender_results)\n",
    "\n",
    "# Save the results to a Parquet file\n",
    "gender_file_path = f\"{processing_folder_path}/{folder_name}-gender-data.parquet\"\n",
    "gender_df.to_parquet(gender_file_path, index=False)\n",
    "print(f\"Gender data saved to: {gender_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Get Citizen Science classification labels\n",
    "\n",
    "a. Query the abstracts first along with the id (to be used as primary keys, connecting the tables later on), \n",
    "\n",
    "b. get citizen science labels by parsing the abstract over an LLM API with system prompt, \n",
    "\n",
    "c. save the outcomes in a separate parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
