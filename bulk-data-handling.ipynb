{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAIRE Community Data Dump Handling: Extraction, Tranformation and Enrichment\n",
    "\n",
    "In this notebook we will start with one of the [OpenAIRE Community Subgraphs](https://graph.openaire.eu/docs/downloads/subgraphs) to enrich that informatino for further analysis.\n",
    "\n",
    "This data process will extract an [OpenAIRE community data dump from Zenodo](https://doi.org/10.5281/zenodo.3974604), transforms it in to a portable file format .parquet (and updatable with changes for time seires analysis), that can be used to query with DuckDB, to enrich this with additional data  (also in .parquet, for join queries).\n",
    "\n",
    "This additional data can be societal impact data from [Altmetric.com](https://details-page-api-docs.altmetric.com/) or [Overton.io](https://app.overton.io/swagger.php), Gender data using [genderize.io](https://genderize.io/documentation), sdg classification using [aurora-sdg](https://aurora-universities.eu/sdg-research/sdg-api/)\n",
    "\n",
    "This script needs to be written in a way so that it can run every month using  the latest data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing steps\n",
    "\n",
    "* the folder ./data/ is put in .gitignore to prevent that bulk datais sent to a code repository. So make sure that folder exists, and mkdir if not exists. \n",
    "* The script downloads the lastest Data Dump Tar file from one selected community. See https://doi.org/10.5281/zenodo.3974604 for the latest list. In our case the Aurora tar file. https://zenodo.org/records/14887484/files/aurora.tar?download=1\n",
    "  * Use the json record of zenodo to get to the latest record, and fetch the download link of the aurora.tar file. for example : https://zenodo.org/records/14887484/export/json or https://zenodo.org/api/records/14887484/versions/latest \n",
    "  Make the tar filename a variable, so it can be used for multiple community dumps.\n",
    "  Download the tar file in a target folder ./data/{publication_date}/01-downloaded/{filename} where a subfolder is created using the filename and the timestamp. Make this also as a  variable to use later on.\n",
    "* Extract the tar file, to the compressed .json.gz files and put these in target folder ./data/{publication_date}/02-extracted/{filename}\n",
    "* Transform the compressed .json.gz files into a single .parquet file in target folder ./data/{publication_date}/03-transformed/{filename}\n",
    "Use instructions in sections \"Processing JSON files with DuckDB\" and \"Full dataset, bit by bit\" and \"Splitting and Processing JSON Files in Batches\" https://github.com/mosart/OpenAIRE-tools/blob/main/duckdb-querying.ipynb to start with. (be aware of error messages, and fix the issues to get all the data in)\n",
    "* Extract the SQL schema (schema-datadump.sql) from the .parquet file and put it in target folder ./data/{filename+timestamp}/02-transformed/ This is needed for further processing of the records with DuckDB later on.\n",
    "Use instructions in section \"Extracting Schema from Parquet File\" https://github.com/mosart/OpenAIRE-tools/blob/main/duckdb-querying.ipynb to start with.\n",
    "* Query to get all identifiers: openaire id, doi, isbn, hdl, etc.\n",
    "* **Get Altmetric data:**\n",
    "* Extract the Altmetric data using the Identifiers. Transform keeping the record id in .parquet and put that in target folder ./data/{publication_date}/04-processed/{filename}/\n",
    "* Extract the SQL schema (schema-altmetric.sql) from the .parquet file and put it in target folder ./data/{publication_date}/04-processed/{filename}/\n",
    "* **Get Overton data:** Repeat the altmetric steps, bun than for Overton.\n",
    "* **Get Gender data** query for the Author names and country codes, and run them over the gerderize api\n",
    "* **Get SDG data** query for the abstracts, and run abstracs larger than 100 tokens over the aurora-SDG api."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Mode\n",
    "Testign mode will reduce the number of records to process. Set to False if you want to go for the long haul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mode = False # Set to False for production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing/importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /home/sfattori/.local/lib/python3.9/site-packages (25.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in /home/sfattori/.local/lib/python3.9/site-packages (2.32.5)\n",
      "Requirement already satisfied: pandas in /etc/miniconda/envs/custom-kernel/lib/python3.9/site-packages (2.3.0)\n",
      "Requirement already satisfied: duckdb in /home/sfattori/.local/lib/python3.9/site-packages (1.3.2)\n",
      "Requirement already satisfied: datetime in /home/sfattori/.local/lib/python3.9/site-packages (5.5)\n",
      "Requirement already satisfied: tqdm in /home/sfattori/.local/lib/python3.9/site-packages (4.67.1)\n",
      "Requirement already satisfied: pyarrow in /home/sfattori/.local/lib/python3.9/site-packages (21.0.0)\n",
      "Requirement already satisfied: fastparquet in /home/sfattori/.local/lib/python3.9/site-packages (2024.11.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/sfattori/.local/lib/python3.9/site-packages (from requests) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sfattori/.local/lib/python3.9/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sfattori/.local/lib/python3.9/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sfattori/.local/lib/python3.9/site-packages (from requests) (2025.8.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /etc/miniconda/envs/custom-kernel/lib/python3.9/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /etc/miniconda/envs/custom-kernel/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /etc/miniconda/envs/custom-kernel/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /etc/miniconda/envs/custom-kernel/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: zope.interface in /home/sfattori/.local/lib/python3.9/site-packages (from datetime) (7.2)\n",
      "Requirement already satisfied: cramjam>=2.3 in /home/sfattori/.local/lib/python3.9/site-packages (from fastparquet) (2.11.0)\n",
      "Requirement already satisfied: fsspec in /home/sfattori/.local/lib/python3.9/site-packages (from fastparquet) (2025.9.0)\n",
      "Requirement already satisfied: packaging in /etc/miniconda/envs/custom-kernel/lib/python3.9/site-packages (from fastparquet) (25.0)\n",
      "Requirement already satisfied: six>=1.5 in /etc/miniconda/envs/custom-kernel/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: setuptools in /etc/miniconda/envs/custom-kernel/lib/python3.9/site-packages (from zope.interface->datetime) (80.9.0)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for the notebook\n",
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install requests pandas duckdb datetime tqdm pyarrow fastparquet\n",
    "# After installing restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import signal\n",
    "import os\n",
    "import tarfile\n",
    "import random\n",
    "import time\n",
    "import hashlib\n",
    "import duckdb\n",
    "import ast\n",
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import tempfile\n",
    "import shutil\n",
    "from requests.adapters import HTTPAdapter, Retry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Get the latest Community Dump File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# Variables & paths\n",
    "# -------------------------------------\n",
    "url = \"https://zenodo.org/records/14887484/files/aurora.tar?download=1\"\n",
    "file_name = url.split(\"/\")[-1].split(\"?\")[0]\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "base_folder = \"./data\"\n",
    "input_folder = os.path.join(base_folder, \"01_input\")\n",
    "extracted_folder = os.path.join(base_folder, \"02_extracted\", f\"{file_name}_{timestamp}\")\n",
    "transformed_folder = os.path.join(base_folder, \"03_transformed\")\n",
    "enriched_folder = os.path.join(base_folder, \"04_enriched\")\n",
    "download_path = os.path.join(input_folder, file_name)\n",
    "\n",
    "# Create necessary folders\n",
    "os.makedirs(input_folder, exist_ok=True)\n",
    "os.makedirs(extracted_folder, exist_ok=True)\n",
    "os.makedirs(transformed_folder, exist_ok=True)\n",
    "os.makedirs(enriched_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already downloaded at ./data/01_input/aurora.tar\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------\n",
    "# Download .tar file\n",
    "# -------------------------------------\n",
    "def download_file(url, save_path):\n",
    "    print(f\"Downloading {url} ...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    print(f\"Saved to {save_path}\")\n",
    "\n",
    "if not os.path.exists(download_path):\n",
    "    download_file(url, download_path)\n",
    "else:\n",
    "    print(f\"File already downloaded at {download_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract the tar file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/01_input/aurora.tar to ./data/02_extracted/aurora.tar_20250910_141708 ...\n",
      "Extraction complete.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------\n",
    "# Extract the tar file\n",
    "# -------------------------------------\n",
    "def extract_tar(tar_path, extract_to):\n",
    "    print(f\"Extracting {tar_path} to {extract_to} ...\")\n",
    "    with tarfile.open(tar_path, \"r\") as tar:\n",
    "        tar.extractall(path=extract_to)\n",
    "    print(\"Extraction complete.\")\n",
    "\n",
    "extract_tar(download_path, extracted_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of top-level items in extracted folder: 1\n",
      "First 5 items:\n",
      " - aurora\n",
      "Subdirectories:\n",
      " - aurora\n",
      "Latest extraction path: ./data/02_extracted/aurora.tar_20250910_141708/aurora\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------\n",
    "# Introspect extracted directory\n",
    "# -------------------------------------\n",
    "extracted_files = os.listdir(extracted_folder)\n",
    "print(f\"Number of top-level items in extracted folder: {len(extracted_files)}\")\n",
    "\n",
    "print(\"First 5 items:\")\n",
    "for f in extracted_files[:5]:\n",
    "    print(f\" - {f}\")\n",
    "\n",
    "subdirectories = [f for f in extracted_files if os.path.isdir(os.path.join(extracted_folder, f))]\n",
    "print(\"Subdirectories:\")\n",
    "for sub in subdirectories:\n",
    "    print(f\" - {sub}\")\n",
    "\n",
    "# Get latest subdirectory by modification time\n",
    "latest_subdirectory = sorted(subdirectories, key=lambda x: os.path.getmtime(os.path.join(extracted_folder, x)))[-1]\n",
    "latest_extraction_path = os.path.join(extracted_folder, latest_subdirectory)\n",
    "print(f\"Latest extraction path: {latest_extraction_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Get a data sample to generate parquetfile and the SQL schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DuckDB schema from sample file:\n",
      "authors: STRUCT(fullName VARCHAR, \"name\" VARCHAR, rank BIGINT, surname VARCHAR, pid STRUCT(id STRUCT(scheme VARCHAR, \"value\" VARCHAR), provenance STRUCT(provenance VARCHAR, trust VARCHAR)))[]\n",
      "bestAccessRight: STRUCT(code VARCHAR, \"label\" VARCHAR, scheme VARCHAR)\n",
      "collectedFrom: STRUCT(\"key\" VARCHAR, \"value\" VARCHAR)[]\n",
      "communities: STRUCT(code VARCHAR, \"label\" VARCHAR, provenance STRUCT(provenance VARCHAR, trust VARCHAR)[])[]\n",
      "contributors: VARCHAR[]\n",
      "countries: STRUCT(code VARCHAR, \"label\" VARCHAR, provenance STRUCT(provenance VARCHAR, trust VARCHAR))[]\n",
      "coverages: JSON[]\n",
      "dateOfCollection: VARCHAR\n",
      "descriptions: VARCHAR[]\n",
      "formats: VARCHAR[]\n",
      "geoLocations: STRUCT(box VARCHAR, place VARCHAR, point VARCHAR)[]\n",
      "id: VARCHAR\n",
      "instances: STRUCT(accessRight STRUCT(code VARCHAR, \"label\" VARCHAR, scheme VARCHAR), alternateIdentifiers STRUCT(scheme VARCHAR, \"value\" VARCHAR)[], collectedFrom STRUCT(\"key\" VARCHAR, \"value\" VARCHAR), hostedBy STRUCT(\"key\" VARCHAR, \"value\" VARCHAR), pids STRUCT(scheme VARCHAR, \"value\" VARCHAR)[], refereed VARCHAR, \"type\" VARCHAR, urls VARCHAR[], publicationDate DATE, license VARCHAR)[]\n",
      "language: STRUCT(code VARCHAR, \"label\" VARCHAR)\n",
      "lastUpdateTimeStamp: BIGINT\n",
      "mainTitle: VARCHAR\n",
      "organizations: STRUCT(id VARCHAR, legalName VARCHAR, pids STRUCT(scheme VARCHAR, \"value\" VARCHAR)[])[]\n",
      "originalIds: VARCHAR[]\n",
      "pids: STRUCT(scheme VARCHAR, \"value\" VARCHAR)[]\n",
      "sources: VARCHAR[]\n",
      "subjects: STRUCT(subject STRUCT(scheme VARCHAR, \"value\" VARCHAR), provenance STRUCT(provenance VARCHAR, trust VARCHAR))[]\n",
      "type: VARCHAR\n",
      "indicators: STRUCT(citationImpact STRUCT(citationClass VARCHAR, citationCount DOUBLE, impulse DOUBLE, impulseClass VARCHAR, influence DOUBLE, influenceClass VARCHAR, popularity DOUBLE, popularityClass VARCHAR), usageCounts STRUCT(downloads BIGINT, \"views\" BIGINT))\n",
      "publicationDate: DATE\n",
      "publisher: VARCHAR\n",
      "projects: STRUCT(code VARCHAR, funder STRUCT(jurisdiction VARCHAR, \"name\" VARCHAR, shortName VARCHAR, fundingStream VARCHAR), id VARCHAR, provenance STRUCT(provenance VARCHAR, trust VARCHAR), title VARCHAR, acronym VARCHAR, validated STRUCT(validatedByFunder BOOLEAN, validationDate DATE))[]\n",
      "version: VARCHAR\n",
      "embargoEndDate: DATE\n",
      "size: VARCHAR\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------\n",
    "# Inspect schema from sample file\n",
    "# -------------------------------------\n",
    "sample_files = [f for f in os.listdir(latest_extraction_path) if f.endswith(\".json.gz\")][:3]\n",
    "if not sample_files:\n",
    "    print(\"No .json.gz files found for schema inspection!\")\n",
    "else:\n",
    "    con = duckdb.connect()\n",
    "    sample_file_path = os.path.join(latest_extraction_path, sample_files[0])\n",
    "    con.execute(\"\"\"\n",
    "        CREATE OR REPLACE TEMP TABLE temp_table AS \n",
    "        SELECT * FROM read_json_auto(?, compression='gzip')\n",
    "    \"\"\", [sample_file_path])\n",
    "    schema = con.execute(\"DESCRIBE temp_table\").fetchall()\n",
    "    print(\"\\nDuckDB schema from sample file:\")\n",
    "    for col in schema:\n",
    "        print(f\"{col[0]}: {col[1]}\")\n",
    "    con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transforming JSON.gz files in ./data/02_extracted/aurora.tar_20250910_141708/aurora to Parquet at ./data/03_transformed/aurora.tar_20250910_141708.parquet ...\n",
      "Parquet file saved: ./data/03_transformed/aurora.tar_20250910_141708.parquet\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------\n",
    "# Transform all .json.gz to Parquet\n",
    "# -------------------------------------\n",
    "def transform_json_to_parquet(source_folder, output_parquet_path):\n",
    "    print(f\"\\nTransforming JSON.gz files in {source_folder} to Parquet at {output_parquet_path} ...\")\n",
    "    gz_files = [os.path.join(source_folder, f) for f in os.listdir(source_folder) if f.endswith(\".json.gz\")]\n",
    "    if not gz_files:\n",
    "        print(\"No .json.gz files found for transformation!\")\n",
    "        return\n",
    "\n",
    "    con = duckdb.connect()\n",
    "    files_string = ','.join([f\"'{f}'\" for f in gz_files])\n",
    "    query = f\"\"\"\n",
    "        CREATE OR REPLACE TEMP TABLE temp_data AS \n",
    "        SELECT * FROM read_json_auto([{files_string}], compression='gzip', union_by_name=true, ignore_errors=true)\n",
    "    \"\"\"\n",
    "    con.execute(query)\n",
    "    os.makedirs(os.path.dirname(output_parquet_path), exist_ok=True)\n",
    "    con.execute(f\"COPY temp_data TO '{output_parquet_path}' (FORMAT 'parquet')\")\n",
    "    con.close()\n",
    "    print(f\"Parquet file saved: {output_parquet_path}\")\n",
    "\n",
    "# Run transformation\n",
    "parquet_path = os.path.join(transformed_folder, f\"{file_name}_{timestamp}.parquet\")\n",
    "transform_json_to_parquet(latest_extraction_path, parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SQL schema extracted and saved to ./data/05_schema/schema-datadump_20250910_141708.sql\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------\n",
    "# Extract SQL schema from Parquet and save it\n",
    "# -------------------------------------\n",
    "schema_folder = os.path.join(base_folder, \"05_schema\")\n",
    "os.makedirs(schema_folder, exist_ok=True)\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Read Parquet schema\n",
    "con.execute(f\"CREATE OR REPLACE TEMP TABLE parquet_table AS SELECT * FROM read_parquet('{parquet_path}')\")\n",
    "schema_info = con.execute(\"DESCRIBE parquet_table\").fetchall()\n",
    "\n",
    "# Format schema as SQL CREATE TABLE statement\n",
    "table_name = \"openaire_datadump\"\n",
    "columns_sql = \",\\n  \".join([f\"{col[0]} {col[1].upper()}\" for col in schema_info])\n",
    "create_table_sql = f\"CREATE TABLE {table_name} (\\n  {columns_sql}\\n);\"\n",
    "\n",
    "# Save schema SQL to file\n",
    "schema_file = os.path.join(schema_folder, f\"schema-datadump_{timestamp}.sql\")\n",
    "with open(schema_file, \"w\") as f:\n",
    "    f.write(create_table_sql)\n",
    "\n",
    "print(f\"\\nSQL schema extracted and saved to {schema_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Get the DOI's and other identifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted identifiers from Parquet (showing up to 10 rows):\n",
      "                                                pids\n",
      "0                                                 []\n",
      "1  [{'scheme': 'doi', 'value': '10.17182/hepdata....\n",
      "2  [{'scheme': 'doi', 'value': '10.17182/hepdata....\n",
      "3  [{'scheme': 'doi', 'value': '10.17182/hepdata....\n",
      "4  [{'scheme': 'doi', 'value': '10.17182/hepdata....\n",
      "5  [{'scheme': 'doi', 'value': '10.17182/hepdata....\n",
      "6  [{'scheme': 'doi', 'value': '10.17182/hepdata....\n",
      "7  [{'scheme': 'doi', 'value': '10.17182/hepdata....\n",
      "8  [{'scheme': 'doi', 'value': '10.17182/hepdata....\n",
      "9    [{'scheme': 'doi', 'value': '10.5517/ccxty9y'}]\n",
      "  scheme                            value\n",
      "0    doi    10.17182/hepdata.32752.v1/t11\n",
      "1    doi     10.17182/hepdata.89325.v1/t1\n",
      "2    doi   10.17182/hepdata.104860.v1/t88\n",
      "3    doi    10.17182/hepdata.84427.v1/t35\n",
      "4    doi     10.17182/hepdata.70063.v1/t8\n",
      "5    doi   10.17182/hepdata.115142.v1/t84\n",
      "6    doi  10.17182/hepdata.103063.v1/t597\n",
      "7    doi    10.17182/hepdata.74247.v1/t76\n",
      "8    doi                  10.5517/ccxty9y\n",
      "9    doi   10.17182/hepdata.13387.v1/t189\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------\n",
    "# Query all identifiers from Parquet\n",
    "# -------------------------------------\n",
    "# Common identifier columns to check (add more if needed)\n",
    "identifier_columns = [\"openaire_id\", \"doi\", \"isbn\", \"hdl\", \"pmid\", \"pmcid\", \"arxiv_id\",'pids']\n",
    "\n",
    "# Build query to select these columns if they exist\n",
    "existing_cols = [col[0] for col in schema_info]\n",
    "cols_to_select = [col for col in identifier_columns if col in existing_cols]\n",
    "\n",
    "if not cols_to_select:\n",
    "    print(\"No identifier columns found in the Parquet file.\")\n",
    "else:\n",
    "    query = f\"SELECT DISTINCT {', '.join(cols_to_select)} FROM parquet_table WHERE \" + \\\n",
    "            \" OR \".join([f\"{col} IS NOT NULL\" for col in cols_to_select])\n",
    "    result = con.execute(query).fetchdf()\n",
    "    print(f\"\\nExtracted identifiers from Parquet (showing up to 10 rows):\")\n",
    "    print(result.head(10))\n",
    "\n",
    "con.close()\n",
    "\n",
    "def explode_pids(row):\n",
    "    pids_list = row['pids']\n",
    "    # Check if pids_list is empty in a safe way\n",
    "    if pids_list is None or len(pids_list) == 0:\n",
    "        return pd.DataFrame()\n",
    "    return pd.DataFrame({\n",
    "        'scheme': [pid['scheme'] for pid in pids_list],\n",
    "        'value': [pid['value'] for pid in pids_list]\n",
    "    })\n",
    "\n",
    "\n",
    "dfs = []\n",
    "for _, row in result.iterrows():\n",
    "    dfs.append(explode_pids(row))\n",
    "\n",
    "flat_pids_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(flat_pids_df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Get Altmetric data\n",
    "\n",
    "a. use the PIDS (df_pids_by_scheme) along with the record id (to be used as primary keys, connecting the tables later on),\n",
    "\n",
    "b. get mention data by parsing the pids over the altmetric API,\n",
    "\n",
    "c. save the outcomes in a separate parquet file. Altmetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map known PID schemes to Altmetric API endpoints\n",
    "endpoint_map = {\n",
    "    'doi': 'doi',\n",
    "    'handle': 'handle',\n",
    "    'pmid': 'pmid',\n",
    "    'arxiv': 'arxiv',\n",
    "    'ads': 'ads',\n",
    "    'ssrn': 'ssrn',\n",
    "    'repec': 'repec',\n",
    "    'isbn': 'isbn',\n",
    "    'id': 'id',\n",
    "    'nct': 'nct_id',\n",
    "    'urn': 'urn',\n",
    "    'uri': 'uri'\n",
    "}\n",
    "\n",
    "def estimate_enrichment_time(n_items, rate_per_minute):\n",
    "    secs_per = 60 / rate_per_minute\n",
    "    total_secs = secs_per * n_items\n",
    "    print(f\"Estimated time for {n_items} items at {rate_per_minute}/min: {total_secs / 60:.1f} minutes\")\n",
    "\n",
    "def atomic_write_json(data, path):\n",
    "    \"\"\"Write JSON atomically to avoid corruption if interrupted.\"\"\"\n",
    "    dirpath = os.path.dirname(path)\n",
    "    with tempfile.NamedTemporaryFile('w', delete=False, dir=dirpath) as tf:\n",
    "        json.dump(data, tf)\n",
    "        tempname = tf.name\n",
    "    shutil.move(tempname, path)\n",
    "\n",
    "def fetch_altmetric_data(df,\n",
    "                         extracted_folder,              # e.g. ./data/{ts}/03-altmetric-extracted/\n",
    "                         transformed_folder,            # e.g. ./data/{ts}/04-altmetric-transformed/\n",
    "                         json_filename=\"altmetric_results.json\",\n",
    "                         parquet_filename=\"altmetric_results.parquet\",\n",
    "                         schema_filename=\"schema-altmetric.sql\",\n",
    "                         batch_size=100,\n",
    "                         sleep_sec=0.2):\n",
    "\n",
    "    # Create folders if missing\n",
    "    os.makedirs(extracted_folder, exist_ok=True)\n",
    "    os.makedirs(transformed_folder, exist_ok=True)\n",
    "\n",
    "    json_path = os.path.join(extracted_folder, json_filename)\n",
    "    parquet_path = os.path.join(transformed_folder, parquet_filename)\n",
    "    schema_path = os.path.join(transformed_folder, schema_filename)\n",
    "\n",
    "    # Load previously saved results\n",
    "    if os.path.exists(json_path):\n",
    "        with open(json_path, 'r') as f:\n",
    "            results = json.load(f)\n",
    "    else:\n",
    "        results = []\n",
    "\n",
    "    # Track processed items\n",
    "    processed_keys = {(r.get('scheme'), r.get('value')) for r in results}\n",
    "    df_to_process = df[~df.apply(lambda row: (row['scheme'], row['value']) in processed_keys, axis=1)].reset_index(drop=True)\n",
    "    total = len(df_to_process)\n",
    "\n",
    "    # Estimate runtime\n",
    "    if total > 0:\n",
    "        estimate_enrichment_time(total, rate_per_minute=(60 / sleep_sec))\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Loop with progress bar\n",
    "    for i, row in enumerate(tqdm(df_to_process.itertuples(index=False), total=total, desc=\"Fetching Altmetric data\")):\n",
    "        scheme = row.scheme.lower()\n",
    "        value = row.value\n",
    "\n",
    "        if scheme in endpoint_map:\n",
    "            endpoint = endpoint_map[scheme]\n",
    "            url = f\"https://api.altmetric.com/v1/{endpoint}/{value}\"\n",
    "\n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    # enrich with metadata\n",
    "                    data['openaire_id'] = getattr(row, 'openaire_id', None)\n",
    "                    data['scheme'] = scheme\n",
    "                    data['value'] = value\n",
    "                    results.append(data)\n",
    "                # silently ignore errors\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            time.sleep(sleep_sec)\n",
    "\n",
    "        # Save progress every batch or at the end\n",
    "        if (i + 1) % batch_size == 0 or (i + 1) == total:\n",
    "            atomic_write_json(results, json_path)\n",
    "            pd.json_normalize(results).astype(str).to_parquet(parquet_path, index=False)\n",
    "\n",
    "    # Extract SQL schema from Parquet\n",
    "    con = duckdb.connect()\n",
    "    con.execute(f\"DESCRIBE SELECT * FROM parquet_scan('{parquet_path}')\")\n",
    "    schema_df = con.fetchdf()\n",
    "    with open(schema_path, \"w\") as f:\n",
    "        for _, row in schema_df.iterrows():\n",
    "            f.write(f\"{row['column_name']} {row['column_type']},\\n\")\n",
    "    con.close()\n",
    "\n",
    "    return pd.json_normalize(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated time for 2635772 items at 300.0/min: 8785.9 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:   0%|                                                                | 154/2635772 [00:41<198:06:02,  3.70it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m     _ \u001b[38;5;241m=\u001b[39m fetch_altmetric_data(df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10000\u001b[39m), extracted_folder, transformed_folder)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:   \n\u001b[0;32m---> 12\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_altmetric_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextracted_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_folder\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 87\u001b[0m, in \u001b[0;36mfetch_altmetric_data\u001b[0;34m(df, extracted_folder, transformed_folder, json_filename, parquet_filename, schema_filename, batch_size, sleep_sec)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43msleep_sec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Save progress every batch or at the end\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m batch_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m total:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = flat_pids_df.copy()\n",
    "\n",
    "filename = \"data_dump_altmetric\"\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "base_path = f\"./data/{filename}_{timestamp}\"\n",
    "extracted_folder = os.path.join(base_path, \"03-altmetric-extracted\")\n",
    "transformed_folder = os.path.join(base_path, \"04-altmetric-transformed\")\n",
    "\n",
    "if testing_mode:\n",
    "    _ = fetch_altmetric_data(df.head(10000), extracted_folder, transformed_folder)\n",
    "else:   \n",
    "    _ = fetch_altmetric_data(df, extracted_folder, transformed_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Get Overton data\n",
    "a. (see 5a) use the PIDS (df_pids_by_scheme) along with the record id (to be used as primary keys, connecting the tables later on),\n",
    "\n",
    "b. get policy mention data by parsing the pids over the overton API,\n",
    "\n",
    "c. save the outcomes in a separate parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_enrichment_time(n_items, rate_per_minute, sleep_sec=2, big_pause_every=100, big_pause_sec=30):\n",
    "    secs_per = sleep_sec  # per DOI\n",
    "    total_secs = secs_per * n_items\n",
    "\n",
    "    # Add big pauses\n",
    "    n_pauses = n_items // big_pause_every\n",
    "    total_secs += n_pauses * big_pause_sec\n",
    "\n",
    "    print(f\"Estimated time for {n_items} items: {total_secs / 60:.1f} minutes \")\n",
    "\n",
    "\n",
    "def atomic_write_json(data, path):\n",
    "    \"\"\"Write JSON atomically to avoid corruption if interrupted.\"\"\"\n",
    "    dirpath = os.path.dirname(path)\n",
    "    with tempfile.NamedTemporaryFile('w', delete=False, dir=dirpath) as tf:\n",
    "        json.dump(data, tf)\n",
    "        tempname = tf.name\n",
    "    shutil.move(tempname, path)\n",
    "\n",
    "def fetch_overton_mentions(df, \n",
    "                           api_key, \n",
    "                           extracted_folder,              # e.g. ./data/{ts}/05-overton-extracted/\n",
    "                           transformed_folder,            # e.g. ./data/{ts}/06-overton-transformed/\n",
    "                           json_filename=\"overton_mentions.json\", \n",
    "                           parquet_filename=\"overton_mentions.parquet\",\n",
    "                           schema_filename=\"schema-overton.sql\",\n",
    "                           batch_size=50, \n",
    "                           sleep_sec=2):\n",
    "\n",
    "    # Validate input\n",
    "    if df.empty or 'scheme' not in df.columns or 'value' not in df.columns:\n",
    "        print(\"Input DataFrame empty or missing required columns ('scheme', 'value'). Skipping enrichment.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    os.makedirs(extracted_folder, exist_ok=True)\n",
    "    os.makedirs(transformed_folder, exist_ok=True)\n",
    "\n",
    "    json_path = os.path.join(extracted_folder, json_filename)\n",
    "    parquet_path = os.path.join(transformed_folder, parquet_filename)\n",
    "    schema_path = os.path.join(transformed_folder, schema_filename)\n",
    "\n",
    "    # Load previously saved results if available\n",
    "    if os.path.exists(json_path):\n",
    "        print(f\"Resuming from saved JSON: {json_path}\")\n",
    "        with open(json_path, 'r') as f:\n",
    "            results = json.load(f)\n",
    "    else:\n",
    "        results = []\n",
    "\n",
    "    # Track processed DOIs\n",
    "    processed_dois = {r.get('queried_doi') for r in results if 'queried_doi' in r}\n",
    "\n",
    "    # Filter to only DOIs and exclude already processed\n",
    "    df_dois = df[df['scheme'].str.lower() == 'doi'].copy()\n",
    "    df_dois = df_dois[~df_dois['value'].isin(processed_dois)].drop_duplicates('value')\n",
    "    total = len(df_dois)\n",
    "\n",
    "    print(f\"Total DOIs to process: {total}\")\n",
    "    if total > 0:\n",
    "        estimate_enrichment_time(total, rate_per_minute=(60 / sleep_sec))\n",
    "\n",
    "    base_url = \"https://app.overton.io/documents.php\"\n",
    "\n",
    "    # Session with retry logic\n",
    "    session = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=5,\n",
    "        backoff_factor=2,  # exponential backoff\n",
    "        status_forcelist=[429, 500, 502, 503, 504]\n",
    "    )\n",
    "    session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "\n",
    "    # Main loop\n",
    "    for i, row in enumerate(tqdm(df_dois.itertuples(index=False), total=total, desc=\"Fetching Overton mentions\")):\n",
    "        doi = row.value\n",
    "        params = {\n",
    "            \"plain_dois_cited\": doi,\n",
    "            \"format\": \"json\",\n",
    "            \"api_key\": api_key\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = session.get(base_url, params=params)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "\n",
    "                # Extract mentions (explode \"results\")\n",
    "                for r in data.get(\"results\", []):\n",
    "                    r[\"queried_doi\"] = doi\n",
    "                    r[\"openaire_id\"] = getattr(row, 'openaire_id', None)\n",
    "                    r[\"queried_at\"] = datetime.utcnow().isoformat()\n",
    "                    results.append(r)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {doi}: {e}\")\n",
    "\n",
    "        # Small pause\n",
    "        time.sleep(sleep_sec)\n",
    "\n",
    "        # Big pause every 100 requests to avoid API throttling\n",
    "        if (i + 1) % 100 == 0:\n",
    "            time.sleep(30)\n",
    "\n",
    "        # Save progress every batch_size or at the end\n",
    "        if (i + 1) % batch_size == 0 or (i + 1) == total:\n",
    "            atomic_write_json(results, json_path)\n",
    "            df_flat = pd.json_normalize(results)\n",
    "            df_flat = df_flat.astype(str)  # avoid ArrowInvalid issues\n",
    "            df_flat.to_parquet(parquet_path, index=False)\n",
    "\n",
    "    # Extract SQL schema from the Parquet file\n",
    "    con = duckdb.connect()\n",
    "    con.execute(f\"DESCRIBE SELECT * FROM parquet_scan('{parquet_path}')\")\n",
    "    schema_df = con.fetchdf()\n",
    "    with open(schema_path, \"w\") as f:\n",
    "        f.write(\"CREATE TABLE overton_mentions (\\n\")\n",
    "        for j, row in schema_df.iterrows():\n",
    "            comma = \",\" if j < len(schema_df)-1 else \"\"\n",
    "            f.write(f\"  {row['column_name']} {row['column_type']}{comma}\\n\")\n",
    "        f.write(\");\\n\")\n",
    "    con.close()\n",
    "\n",
    "    return pd.json_normalize(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total DOIs to process: 792203\n",
      "Estimated time for 792203 items: 30367.8 minutes \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Overton mentions:   0%|                                                                 | 2/792203 [00:05<582:56:53,  2.65s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m     overton_df \u001b[38;5;241m=\u001b[39m fetch_overton_mentions(df\u001b[38;5;241m.\u001b[39mtail(\u001b[38;5;241m100\u001b[39m), api_key, extracted_folder, transformed_folder)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 10\u001b[0m     overton_df \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_overton_mentions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextracted_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_folder\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 98\u001b[0m, in \u001b[0;36mfetch_overton_mentions\u001b[0;34m(df, api_key, extracted_folder, transformed_folder, json_filename, parquet_filename, schema_filename, batch_size, sleep_sec)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError fetching \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Small pause\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43msleep_sec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Big pause every 100 requests to avoid API throttling\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "api_key = '90e529-844128-9f360b'\n",
    "filename = \"data_dump_overton\"\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "base_path = f\"./data/{filename}_{timestamp}\"\n",
    "extracted_folder = os.path.join(base_path, \"05-overton-extracted\")\n",
    "transformed_folder = os.path.join(base_path, \"06-overton-transformed\")\n",
    "if testing_mode:\n",
    "    overton_df = fetch_overton_mentions(df.tail(100), api_key, extracted_folder, transformed_folder)\n",
    "else:\n",
    "    overton_df = fetch_overton_mentions(df, api_key, extracted_folder, transformed_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Get SDG classification labels | Aurora SDG & LLM methods\n",
    "\n",
    "7a. **Extract and clean abstracts**: Query the abstracts first along with the id (to be used as primary keys, connecting the tables later on), strip xml, and keep only abstract with 100+ tokens, \n",
    "\n",
    "7b. **Aurora-SDG Method**: get sdg data by parsing the abstracts with more than 100 tokens over the Aurora SDG API, save the outcomes in a separate parquet file.\n",
    "\n",
    "7c. **LLM-SDG Method**: get sdg data by parsing the abstracts with more than 100 tokens over an LLM API with system prompt, save the outcomes in a separate parquet file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### step 7a: **Extract & clean abstracts:** Get the abstracts, including the record id and the number of tokens in the abstract\n",
    "\n",
    "Number of tokens are important later on, less then 100 tokens in the abstract deliver low quality SDG classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptions with token counts:\n",
      "                                          record_id  \\\n",
      "0    doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef   \n",
      "1    doi_dedup___::e246801fc9ed25782358bac694517f8f   \n",
      "2    doi_dedup___::fe2c8a61b8ccaa6515d3c2996b2144c9   \n",
      "3    doi_dedup___::7bd738ce5851f7e450ebf6388ad51522   \n",
      "4    doi_dedup___::bf1098713a38f89cb9c67a7f59401107   \n",
      "..                                              ...   \n",
      "650  doi_dedup___::3894f0d63b65411c0d289bf831716e48   \n",
      "651  doi_dedup___::a96d857fd60b818f7bde9aa0c99bfc3f   \n",
      "652  doi_dedup___::551f2ca097a75326cc2e7561f831d38b   \n",
      "653  doi_dedup___::da9603d028d4bd4ad2d9c980917fd5ac   \n",
      "654  doi_dedup___::3da738372eaf79174655e4ef24d74cd2   \n",
      "\n",
      "                                           description  token_count  \n",
      "0    Abstract</jats:title><jats:p>The Black Sea, th...          161  \n",
      "1     The early twenty-first centuryâ€™s warming tren...          247  \n",
      "2    Abstract</jats:title><jats:p>The semienclosed ...          226  \n",
      "3    Abstract</jats:title><jats:p>Identification of...          249  \n",
      "4    This study focuses on the interaction between ...          165  \n",
      "..                                                 ...          ...  \n",
      "650  Development of innovative 3D web based Argo Da...            9  \n",
      "651                      Data Management Plan document            4  \n",
      "652         Report of the 2nd Ocean Observers workshop            7  \n",
      "653  Provides Matlab functions as well as example w...           31  \n",
      "654  Argo is a real-time global ocean in situ obser...          120  \n",
      "\n",
      "[655 rows x 3 columns]\n",
      "Description data saved to: ./data/2025-02-19/04_processed/argo-france/argo-france-descriptions-with-tokens.parquet\n",
      "File size: 0.47 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "# Query to extract the ID, description, remove XML tags, and calculate the number of tokens in the description\n",
    "description_data = con.sql(f'''\n",
    "    SELECT \n",
    "        id AS record_id,\n",
    "        regexp_replace(descriptions[1], '<[^>]+>', '') AS description,  -- Remove XML tags\n",
    "        array_length(split(regexp_replace(descriptions[1], '<[^>]+>', ''), ' ')) AS token_count\n",
    "    FROM read_parquet('{master_file}')\n",
    "    WHERE descriptions IS NOT NULL AND array_length(descriptions) > 0\n",
    "''').fetchdf()\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(\"Descriptions with token counts:\")\n",
    "print(description_data)\n",
    "\n",
    "# Save the data to a new Parquet file for later use\n",
    "description_file_path = f\"{processing_folder_path}/{folder_name}-descriptions-with-tokens.parquet\"\n",
    "description_data.to_parquet(description_file_path, index=False)\n",
    "print(f\"Description data saved to: {description_file_path}\")\n",
    "print(f\"File size: {os.path.getsize(description_file_path) / (1024**2):.2f} MB\")\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 7b:  **Aurora SDG Classifier**\n",
    "In this step we use the Aurora SDG classifier to classify all the abstracts.\n",
    "\n",
    "First we set a test_mode parameter, so that the first 3 abstracts with more than 100 tokens are used. If testing mode is False, then use all abstracts with more than 100 tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed record_id: doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef, SDGs: [{'goal_code': '1', 'goal_name': 'No poverty', 'prediction_score': 0.00400781631}, {'goal_code': '2', 'goal_name': 'Zero hunger', 'prediction_score': 0.0237638652}, {'goal_code': '3', 'goal_name': 'Good health and well-being', 'prediction_score': 0.00846537948}, {'goal_code': '4', 'goal_name': 'Quality Education', 'prediction_score': 0.00196021795}, {'goal_code': '5', 'goal_name': 'Gender equality', 'prediction_score': 0.0021187067}, {'goal_code': '6', 'goal_name': 'Clean water and sanitation', 'prediction_score': 0.230926484}, {'goal_code': '7', 'goal_name': 'Affordable and clean energy', 'prediction_score': 0.525941491}, {'goal_code': '8', 'goal_name': 'Decent work and economic growth', 'prediction_score': 0.00499722362}, {'goal_code': '9', 'goal_name': 'Industry, innovation and infrastructure', 'prediction_score': 0.00908589363}, {'goal_code': '10', 'goal_name': 'Reduced inequalities', 'prediction_score': 0.00331610441}, {'goal_code': '11', 'goal_name': 'Sustainable cities and communities', 'prediction_score': 0.109603286}, {'goal_code': '12', 'goal_name': 'Responsible consumption and production', 'prediction_score': 0.712000847}, {'goal_code': '13', 'goal_name': 'Climate action', 'prediction_score': 0.00439527631}, {'goal_code': '14', 'goal_name': 'Life below water', 'prediction_score': 0.996815443}, {'goal_code': '15', 'goal_name': 'Life in Land', 'prediction_score': 0.0924201}, {'goal_code': '16', 'goal_name': 'Peace, Justice and strong institutions', 'prediction_score': 0.0030760169}, {'goal_code': '17', 'goal_name': 'Partnerships for the goals', 'prediction_score': 0.00936922431}]\n",
      "Processed record_id: doi_dedup___::e246801fc9ed25782358bac694517f8f, SDGs: [{'goal_code': '1', 'goal_name': 'No poverty', 'prediction_score': 0.00403636694}, {'goal_code': '2', 'goal_name': 'Zero hunger', 'prediction_score': 0.0423853695}, {'goal_code': '3', 'goal_name': 'Good health and well-being', 'prediction_score': 0.00775146484}, {'goal_code': '4', 'goal_name': 'Quality Education', 'prediction_score': 0.002127707}, {'goal_code': '5', 'goal_name': 'Gender equality', 'prediction_score': 0.00222682953}, {'goal_code': '6', 'goal_name': 'Clean water and sanitation', 'prediction_score': 0.253431618}, {'goal_code': '7', 'goal_name': 'Affordable and clean energy', 'prediction_score': 0.79632473}, {'goal_code': '8', 'goal_name': 'Decent work and economic growth', 'prediction_score': 0.00532493}, {'goal_code': '9', 'goal_name': 'Industry, innovation and infrastructure', 'prediction_score': 0.0094588995}, {'goal_code': '10', 'goal_name': 'Reduced inequalities', 'prediction_score': 0.00360018015}, {'goal_code': '11', 'goal_name': 'Sustainable cities and communities', 'prediction_score': 0.0500312448}, {'goal_code': '12', 'goal_name': 'Responsible consumption and production', 'prediction_score': 0.397682369}, {'goal_code': '13', 'goal_name': 'Climate action', 'prediction_score': 0.302897632}, {'goal_code': '14', 'goal_name': 'Life below water', 'prediction_score': 0.997074187}, {'goal_code': '15', 'goal_name': 'Life in Land', 'prediction_score': 0.0117288232}, {'goal_code': '16', 'goal_name': 'Peace, Justice and strong institutions', 'prediction_score': 0.00315281749}, {'goal_code': '17', 'goal_name': 'Partnerships for the goals', 'prediction_score': 0.00921818614}]\n",
      "Processed record_id: doi_dedup___::fe2c8a61b8ccaa6515d3c2996b2144c9, SDGs: [{'goal_code': '1', 'goal_name': 'No poverty', 'prediction_score': 0.00392723083}, {'goal_code': '2', 'goal_name': 'Zero hunger', 'prediction_score': 0.0320818722}, {'goal_code': '3', 'goal_name': 'Good health and well-being', 'prediction_score': 0.0101379752}, {'goal_code': '4', 'goal_name': 'Quality Education', 'prediction_score': 0.00184515119}, {'goal_code': '5', 'goal_name': 'Gender equality', 'prediction_score': 0.00218731165}, {'goal_code': '6', 'goal_name': 'Clean water and sanitation', 'prediction_score': 0.746479392}, {'goal_code': '7', 'goal_name': 'Affordable and clean energy', 'prediction_score': 0.66791594}, {'goal_code': '8', 'goal_name': 'Decent work and economic growth', 'prediction_score': 0.00686761737}, {'goal_code': '9', 'goal_name': 'Industry, innovation and infrastructure', 'prediction_score': 0.0094563365}, {'goal_code': '10', 'goal_name': 'Reduced inequalities', 'prediction_score': 0.00438901782}, {'goal_code': '11', 'goal_name': 'Sustainable cities and communities', 'prediction_score': 0.139113218}, {'goal_code': '12', 'goal_name': 'Responsible consumption and production', 'prediction_score': 0.0136934519}, {'goal_code': '13', 'goal_name': 'Climate action', 'prediction_score': 0.00463417172}, {'goal_code': '14', 'goal_name': 'Life below water', 'prediction_score': 0.996931374}, {'goal_code': '15', 'goal_name': 'Life in Land', 'prediction_score': 0.0839726}, {'goal_code': '16', 'goal_name': 'Peace, Justice and strong institutions', 'prediction_score': 0.003136307}, {'goal_code': '17', 'goal_name': 'Partnerships for the goals', 'prediction_score': 0.971842468}]\n",
      "Processed record_id: doi_dedup___::7bd738ce5851f7e450ebf6388ad51522, SDGs: [{'goal_code': '1', 'goal_name': 'No poverty', 'prediction_score': 0.00400704145}, {'goal_code': '2', 'goal_name': 'Zero hunger', 'prediction_score': 0.0194792449}, {'goal_code': '3', 'goal_name': 'Good health and well-being', 'prediction_score': 0.0112150311}, {'goal_code': '4', 'goal_name': 'Quality Education', 'prediction_score': 0.00180932879}, {'goal_code': '5', 'goal_name': 'Gender equality', 'prediction_score': 0.00216537714}, {'goal_code': '6', 'goal_name': 'Clean water and sanitation', 'prediction_score': 0.292082071}, {'goal_code': '7', 'goal_name': 'Affordable and clean energy', 'prediction_score': 0.621630311}, {'goal_code': '8', 'goal_name': 'Decent work and economic growth', 'prediction_score': 0.00498831272}, {'goal_code': '9', 'goal_name': 'Industry, innovation and infrastructure', 'prediction_score': 0.00899276137}, {'goal_code': '10', 'goal_name': 'Reduced inequalities', 'prediction_score': 0.00369724631}, {'goal_code': '11', 'goal_name': 'Sustainable cities and communities', 'prediction_score': 0.033012569}, {'goal_code': '12', 'goal_name': 'Responsible consumption and production', 'prediction_score': 0.521796167}, {'goal_code': '13', 'goal_name': 'Climate action', 'prediction_score': 0.845212638}, {'goal_code': '14', 'goal_name': 'Life below water', 'prediction_score': 0.997377515}, {'goal_code': '15', 'goal_name': 'Life in Land', 'prediction_score': 0.0125996172}, {'goal_code': '16', 'goal_name': 'Peace, Justice and strong institutions', 'prediction_score': 0.00306090713}, {'goal_code': '17', 'goal_name': 'Partnerships for the goals', 'prediction_score': 0.00911521912}]\n",
      "Processed record_id: doi_dedup___::bf1098713a38f89cb9c67a7f59401107, SDGs: [{'goal_code': '1', 'goal_name': 'No poverty', 'prediction_score': 0.00378909707}, {'goal_code': '2', 'goal_name': 'Zero hunger', 'prediction_score': 0.0073812604}, {'goal_code': '3', 'goal_name': 'Good health and well-being', 'prediction_score': 0.0107872486}, {'goal_code': '4', 'goal_name': 'Quality Education', 'prediction_score': 0.00170755386}, {'goal_code': '5', 'goal_name': 'Gender equality', 'prediction_score': 0.00232478976}, {'goal_code': '6', 'goal_name': 'Clean water and sanitation', 'prediction_score': 0.533321619}, {'goal_code': '7', 'goal_name': 'Affordable and clean energy', 'prediction_score': 0.818670928}, {'goal_code': '8', 'goal_name': 'Decent work and economic growth', 'prediction_score': 0.0425927937}, {'goal_code': '9', 'goal_name': 'Industry, innovation and infrastructure', 'prediction_score': 0.00916290283}, {'goal_code': '10', 'goal_name': 'Reduced inequalities', 'prediction_score': 0.00353124738}, {'goal_code': '11', 'goal_name': 'Sustainable cities and communities', 'prediction_score': 0.373235047}, {'goal_code': '12', 'goal_name': 'Responsible consumption and production', 'prediction_score': 0.0129176676}, {'goal_code': '13', 'goal_name': 'Climate action', 'prediction_score': 0.00348415971}, {'goal_code': '14', 'goal_name': 'Life below water', 'prediction_score': 0.996276259}, {'goal_code': '15', 'goal_name': 'Life in Land', 'prediction_score': 0.0163819194}, {'goal_code': '16', 'goal_name': 'Peace, Justice and strong institutions', 'prediction_score': 0.00340288877}, {'goal_code': '17', 'goal_name': 'Partnerships for the goals', 'prediction_score': 0.00919798}]\n",
      "Processed record_id: doi_dedup___::bf2bceb2795ae668b763e656619c8b7f, SDGs: [{'goal_code': '1', 'goal_name': 'No poverty', 'prediction_score': 0.00399825}, {'goal_code': '2', 'goal_name': 'Zero hunger', 'prediction_score': 0.0321764052}, {'goal_code': '3', 'goal_name': 'Good health and well-being', 'prediction_score': 0.007386446}, {'goal_code': '4', 'goal_name': 'Quality Education', 'prediction_score': 0.00201770663}, {'goal_code': '5', 'goal_name': 'Gender equality', 'prediction_score': 0.00216558576}, {'goal_code': '6', 'goal_name': 'Clean water and sanitation', 'prediction_score': 0.0622676611}, {'goal_code': '7', 'goal_name': 'Affordable and clean energy', 'prediction_score': 0.900306}, {'goal_code': '8', 'goal_name': 'Decent work and economic growth', 'prediction_score': 0.0061891973}, {'goal_code': '9', 'goal_name': 'Industry, innovation and infrastructure', 'prediction_score': 0.00942757726}, {'goal_code': '10', 'goal_name': 'Reduced inequalities', 'prediction_score': 0.00327140093}, {'goal_code': '11', 'goal_name': 'Sustainable cities and communities', 'prediction_score': 0.247030318}, {'goal_code': '12', 'goal_name': 'Responsible consumption and production', 'prediction_score': 0.00865674}, {'goal_code': '13', 'goal_name': 'Climate action', 'prediction_score': 0.00384321809}, {'goal_code': '14', 'goal_name': 'Life below water', 'prediction_score': 0.996844649}, {'goal_code': '15', 'goal_name': 'Life in Land', 'prediction_score': 0.0985007286}, {'goal_code': '16', 'goal_name': 'Peace, Justice and strong institutions', 'prediction_score': 0.00329840183}, {'goal_code': '17', 'goal_name': 'Partnerships for the goals', 'prediction_score': 0.00929072499}]\n",
      "Processed record_id: doi_dedup___::d010a9bab7e29a2c7f97175b48c9aa84, SDGs: [{'goal_code': '1', 'goal_name': 'No poverty', 'prediction_score': 0.00398716331}, {'goal_code': '2', 'goal_name': 'Zero hunger', 'prediction_score': 0.0128183961}, {'goal_code': '3', 'goal_name': 'Good health and well-being', 'prediction_score': 0.00995472074}, {'goal_code': '4', 'goal_name': 'Quality Education', 'prediction_score': 0.00187459588}, {'goal_code': '5', 'goal_name': 'Gender equality', 'prediction_score': 0.00218573213}, {'goal_code': '6', 'goal_name': 'Clean water and sanitation', 'prediction_score': 0.0236417949}, {'goal_code': '7', 'goal_name': 'Affordable and clean energy', 'prediction_score': 0.00219473243}, {'goal_code': '8', 'goal_name': 'Decent work and economic growth', 'prediction_score': 0.00704199076}, {'goal_code': '9', 'goal_name': 'Industry, innovation and infrastructure', 'prediction_score': 0.00913077593}, {'goal_code': '10', 'goal_name': 'Reduced inequalities', 'prediction_score': 0.00559318066}, {'goal_code': '11', 'goal_name': 'Sustainable cities and communities', 'prediction_score': 0.173951119}, {'goal_code': '12', 'goal_name': 'Responsible consumption and production', 'prediction_score': 0.0187065}, {'goal_code': '13', 'goal_name': 'Climate action', 'prediction_score': 0.0058748126}, {'goal_code': '14', 'goal_name': 'Life below water', 'prediction_score': 0.997141242}, {'goal_code': '15', 'goal_name': 'Life in Land', 'prediction_score': 0.0474996865}, {'goal_code': '16', 'goal_name': 'Peace, Justice and strong institutions', 'prediction_score': 0.00313773751}, {'goal_code': '17', 'goal_name': 'Partnerships for the goals', 'prediction_score': 0.00908848643}]\n",
      "Processed record_id: doi_dedup___::024e02533911ba7f1bcb52de1e16495c, SDGs: [{'goal_code': '1', 'goal_name': 'No poverty', 'prediction_score': 0.00394701958}, {'goal_code': '2', 'goal_name': 'Zero hunger', 'prediction_score': 0.00981497765}, {'goal_code': '3', 'goal_name': 'Good health and well-being', 'prediction_score': 0.00697419047}, {'goal_code': '4', 'goal_name': 'Quality Education', 'prediction_score': 0.00197082758}, {'goal_code': '5', 'goal_name': 'Gender equality', 'prediction_score': 0.00205916166}, {'goal_code': '6', 'goal_name': 'Clean water and sanitation', 'prediction_score': 0.0478467047}, {'goal_code': '7', 'goal_name': 'Affordable and clean energy', 'prediction_score': 0.430780739}, {'goal_code': '8', 'goal_name': 'Decent work and economic growth', 'prediction_score': 0.00489535928}, {'goal_code': '9', 'goal_name': 'Industry, innovation and infrastructure', 'prediction_score': 0.0425343215}, {'goal_code': '10', 'goal_name': 'Reduced inequalities', 'prediction_score': 0.00331774354}, {'goal_code': '11', 'goal_name': 'Sustainable cities and communities', 'prediction_score': 0.0269992948}, {'goal_code': '12', 'goal_name': 'Responsible consumption and production', 'prediction_score': 0.0120030642}, {'goal_code': '13', 'goal_name': 'Climate action', 'prediction_score': 0.0579248667}, {'goal_code': '14', 'goal_name': 'Life below water', 'prediction_score': 0.997427046}, {'goal_code': '15', 'goal_name': 'Life in Land', 'prediction_score': 0.0656118691}, {'goal_code': '16', 'goal_name': 'Peace, Justice and strong institutions', 'prediction_score': 0.00306600332}, {'goal_code': '17', 'goal_name': 'Partnerships for the goals', 'prediction_score': 0.00919672847}]\n",
      "Processed record_id: doi_dedup___::f56993cc50f407e192d0001e44e4b563, SDGs: [{'goal_code': '1', 'goal_name': 'No poverty', 'prediction_score': 0.00400754809}, {'goal_code': '2', 'goal_name': 'Zero hunger', 'prediction_score': 0.0332252681}, {'goal_code': '3', 'goal_name': 'Good health and well-being', 'prediction_score': 0.00824168324}, {'goal_code': '4', 'goal_name': 'Quality Education', 'prediction_score': 0.00179204345}, {'goal_code': '5', 'goal_name': 'Gender equality', 'prediction_score': 0.0021610558}, {'goal_code': '6', 'goal_name': 'Clean water and sanitation', 'prediction_score': 0.386186272}, {'goal_code': '7', 'goal_name': 'Affordable and clean energy', 'prediction_score': 0.932893157}, {'goal_code': '8', 'goal_name': 'Decent work and economic growth', 'prediction_score': 0.00751131773}, {'goal_code': '9', 'goal_name': 'Industry, innovation and infrastructure', 'prediction_score': 0.00884720683}, {'goal_code': '10', 'goal_name': 'Reduced inequalities', 'prediction_score': 0.00445893407}, {'goal_code': '11', 'goal_name': 'Sustainable cities and communities', 'prediction_score': 0.0752163231}, {'goal_code': '12', 'goal_name': 'Responsible consumption and production', 'prediction_score': 0.054440707}, {'goal_code': '13', 'goal_name': 'Climate action', 'prediction_score': 0.936569452}, {'goal_code': '14', 'goal_name': 'Life below water', 'prediction_score': 0.996976912}, {'goal_code': '15', 'goal_name': 'Life in Land', 'prediction_score': 0.0144725144}, {'goal_code': '16', 'goal_name': 'Peace, Justice and strong institutions', 'prediction_score': 0.00311273336}, {'goal_code': '17', 'goal_name': 'Partnerships for the goals', 'prediction_score': 0.980641127}]\n",
      "Processed record_id: doi_dedup___::6d01ec16c6ac92e8f2ae61b5fad6712b, SDGs: [{'goal_code': '1', 'goal_name': 'No poverty', 'prediction_score': 0.0040577352}, {'goal_code': '2', 'goal_name': 'Zero hunger', 'prediction_score': 0.0226988792}, {'goal_code': '3', 'goal_name': 'Good health and well-being', 'prediction_score': 0.00603911281}, {'goal_code': '4', 'goal_name': 'Quality Education', 'prediction_score': 0.00205868483}, {'goal_code': '5', 'goal_name': 'Gender equality', 'prediction_score': 0.00187012553}, {'goal_code': '6', 'goal_name': 'Clean water and sanitation', 'prediction_score': 0.077181071}, {'goal_code': '7', 'goal_name': 'Affordable and clean energy', 'prediction_score': 0.594653785}, {'goal_code': '8', 'goal_name': 'Decent work and economic growth', 'prediction_score': 0.00573313236}, {'goal_code': '9', 'goal_name': 'Industry, innovation and infrastructure', 'prediction_score': 0.00906804204}, {'goal_code': '10', 'goal_name': 'Reduced inequalities', 'prediction_score': 0.0050291121}, {'goal_code': '11', 'goal_name': 'Sustainable cities and communities', 'prediction_score': 0.639131069}, {'goal_code': '12', 'goal_name': 'Responsible consumption and production', 'prediction_score': 0.944433212}, {'goal_code': '13', 'goal_name': 'Climate action', 'prediction_score': 0.876177311}, {'goal_code': '14', 'goal_name': 'Life below water', 'prediction_score': 0.996311426}, {'goal_code': '15', 'goal_name': 'Life in Land', 'prediction_score': 0.00946205854}, {'goal_code': '16', 'goal_name': 'Peace, Justice and strong institutions', 'prediction_score': 0.00299048424}, {'goal_code': '17', 'goal_name': 'Partnerships for the goals', 'prediction_score': 0.0094358325}]\n",
      "SDG classification results:\n",
      "                                        record_id top_predicted_sdgs\n",
      "0  doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef               [14]\n",
      "1  doi_dedup___::e246801fc9ed25782358bac694517f8f               [14]\n",
      "2  doi_dedup___::fe2c8a61b8ccaa6515d3c2996b2144c9           [14, 17]\n",
      "3  doi_dedup___::7bd738ce5851f7e450ebf6388ad51522               [14]\n",
      "4  doi_dedup___::bf1098713a38f89cb9c67a7f59401107               [14]\n",
      "5  doi_dedup___::bf2bceb2795ae668b763e656619c8b7f            [7, 14]\n",
      "6  doi_dedup___::d010a9bab7e29a2c7f97175b48c9aa84               [14]\n",
      "7  doi_dedup___::024e02533911ba7f1bcb52de1e16495c               [14]\n",
      "8  doi_dedup___::f56993cc50f407e192d0001e44e4b563    [7, 13, 14, 17]\n",
      "9  doi_dedup___::6d01ec16c6ac92e8f2ae61b5fad6712b       [12, 13, 14]\n",
      "SDG classification results saved to: ./data/2025-02-19/04_processed/argo-france/argo-france-sdg-results-aurora-sdg.parquet\n"
     ]
    }
   ],
   "source": [
    "# Set the testing mode to True for limited processing\n",
    "testing_mode = True\n",
    "\n",
    "# define the models\n",
    "model = \"aurora-sdg\"  # Use the multi-label model for SDG classification (faster, Aurora definition of SDG's, 104 languages)\n",
    "\n",
    "# other available models:\n",
    "# model = \"aurora-sdg\"  # Use the single-label model for classification of each SDG in the Aurora definition (slower, Aurora definition of SDG's, 104 languages)\n",
    "# model = \"elsevier-multi\"  # Elsevier SDG multi-label mBERT model (fast, Elsevier definition of SDG's, 104 languages)\n",
    "# model = \"osdg\"  # OSDG model (alternative, OSDG definition of SDG's, 15 languages)\n",
    "\n",
    "# Set the base URL for the Aurora SDG classifier\n",
    "base_url = \"https://aurora-sdg.labs.vu.nl/classifier/classify/\" + model\n",
    "\n",
    "# Load the descriptions with token counts\n",
    "description_df = pd.read_parquet(f\"{processing_folder_path}/{folder_name}-descriptions-with-tokens.parquet\")\n",
    "\n",
    "# Filter abstracts with at least 100 tokens\n",
    "description_df = description_df[description_df['token_count'] >= 100]\n",
    "\n",
    "# Set testing mode to limit the number of abstracts\n",
    "if testing_mode:\n",
    "    description_df = description_df.head(10)  # Limit to 10 records for testing\n",
    "\n",
    "# Prepare results list\n",
    "sdg_results = []\n",
    "\n",
    "# Rate limit settings\n",
    "rate_limit = 5  # 5 requests per second\n",
    "delay_between_requests = 1 / rate_limit\n",
    "\n",
    "# Loop through each abstract\n",
    "for idx, row in description_df.iterrows():\n",
    "    record_id = row['record_id']\n",
    "    abstract = row['description']\n",
    "\n",
    "    # Prepare the payload for the API\n",
    "    payload = json.dumps({\"text\": abstract})\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "    try:\n",
    "        # Make the API call\n",
    "        response = requests.post(base_url, headers=headers, data=payload)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the response\n",
    "        result = response.json()\n",
    "        predictions = result.get(\"predictions\", [])\n",
    "\n",
    "        # Extract SDG predictions\n",
    "        sdgs = [\n",
    "            {\n",
    "                \"goal_code\": pred[\"sdg\"][\"code\"],\n",
    "                \"goal_name\": pred[\"sdg\"][\"name\"],\n",
    "                \"prediction_score\": pred[\"prediction\"]\n",
    "            }\n",
    "            for pred in predictions\n",
    "        ]\n",
    "\n",
    "        # Append the result to the list\n",
    "        sdg_results.append({\n",
    "            \"record_id\": record_id,\n",
    "            \"abstract\": abstract,\n",
    "            \"sdgs\": sdgs\n",
    "        })\n",
    "\n",
    "        # Calculate and print the time taken to process the record\n",
    "        start_time = time.time()\n",
    "        print(f\"Processed record_id: {record_id}, SDGs: {sdgs}\")\n",
    "        end_time = time.time()\n",
    "        print(f\"Time taken to process record_id {record_id}: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error processing record_id {record_id}: {e}\")\n",
    "\n",
    "    # Add a delay to respect the rate limit\n",
    "    time.sleep(delay_between_requests)\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "sdg_results_df = pd.DataFrame(sdg_results)\n",
    "\n",
    "# calculate the 90th percentile of the prediction scores for each SDG\n",
    "sdg_scores = []\n",
    "for sdg in sdg_results_df['sdgs']:\n",
    "    for prediction in sdg:\n",
    "        sdg_scores.append(prediction['prediction_score'])  \n",
    "# Calculate the 90th percentile\n",
    "percentile_90 = pd.Series(sdg_scores).quantile(0.9)\n",
    "# Filter the results and append a column top_predicted_sdgs, to include only SDGs (as list of goal_codes) with a prediction score above the 90th percentile\n",
    "sdg_results_df['top_predicted_sdgs'] = sdg_results_df['sdgs'].apply(\n",
    "    lambda x: [sdg['goal_code'] for sdg in x if sdg['prediction_score'] >= percentile_90 and sdg['prediction_score'] > 0.1]\n",
    ")\n",
    "\n",
    "# Print the DataFrame with SDG results\n",
    "print(\"SDG classification results:\")\n",
    "print(sdg_results_df[['record_id', 'top_predicted_sdgs']])\n",
    "\n",
    "# Save the results to a Parquet file including the top predicted SDGs\n",
    "sdg_results_path = f\"{processing_folder_path}/{folder_name}-sdg-results-{model}.parquet\"\n",
    "sdg_results_df.to_parquet(sdg_results_path, index=False)\n",
    "print(f\"SDG classification results saved to: {sdg_results_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### step 7c **LLM SDG  Method** EXPERIMENTAL!!\n",
    "\n",
    "Based on findings from the [SDG Classification Benchmark](https://github.com/SDGClassification/benchmark)\n",
    "Benchmark result shows the OpenAIR LLM's outperforms the Aurora SDG and other classification methods. But this is slow an costly to process 500k abstracts.\n",
    "Opensource and open platforms are used in this aproach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **step 7b-1 Get the official definitions of the SDG's from https://metadata.un.org/sdg/ using the Accept header application/rdf+xml**\n",
    "\n",
    "First we get the links to the top level goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDG definitions saved to: ./data/2025-02-19/04_processed/argo-france/sdg_definitions.rdf\n"
     ]
    }
   ],
   "source": [
    "# URL for the SDG metadata\n",
    "sdg_metadata_url = \"https://metadata.un.org/sdg/\"\n",
    "\n",
    "# Set the headers to request RDF/XML format\n",
    "headers = {\n",
    "    \"Accept\": \"application/rdf+xml\"\n",
    "}\n",
    "\n",
    "# Send the GET request\n",
    "response = requests.get(sdg_metadata_url, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Save the RDF/XML content to a file\n",
    "    rdf_file_path = f\"{processing_folder_path}/sdg_definitions.rdf\"\n",
    "    with open(rdf_file_path, \"wb\") as rdf_file:\n",
    "        rdf_file.write(response.content)\n",
    "    print(f\"SDG definitions saved to: {rdf_file_path}\")\n",
    "else:\n",
    "    print(f\"Failed to fetch SDG definitions. Status code: {response.status_code}\")\n",
    "    print(f\"Response: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top concept URLs found in the RDF/XML:\n",
      "http://metadata.un.org/sdg/1\n",
      "http://metadata.un.org/sdg/2\n",
      "http://metadata.un.org/sdg/3\n",
      "http://metadata.un.org/sdg/4\n",
      "http://metadata.un.org/sdg/5\n",
      "http://metadata.un.org/sdg/6\n",
      "http://metadata.un.org/sdg/7\n",
      "http://metadata.un.org/sdg/8\n",
      "http://metadata.un.org/sdg/9\n",
      "http://metadata.un.org/sdg/10\n",
      "http://metadata.un.org/sdg/11\n",
      "http://metadata.un.org/sdg/12\n",
      "http://metadata.un.org/sdg/13\n",
      "http://metadata.un.org/sdg/14\n",
      "http://metadata.un.org/sdg/15\n",
      "http://metadata.un.org/sdg/16\n",
      "http://metadata.un.org/sdg/17\n"
     ]
    }
   ],
   "source": [
    "# Parse the RDF/XML file\n",
    "tree = ET.parse(rdf_file_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "# Find all skos:hasTopConcept elements and extract their rdf:resource attribute\n",
    "top_concept_urls = []\n",
    "for elem in root.findall('.//{http://www.w3.org/2004/02/skos/core#}hasTopConcept'):\n",
    "    url = elem.attrib.get('{http://www.w3.org/1999/02/22-rdf-syntax-ns#}resource')\n",
    "    if url:\n",
    "        top_concept_urls.append(url)\n",
    "\n",
    "# sort the URLs based on the integer in the last part of the URL\n",
    "top_concept_urls.sort(key=lambda x: int(x.split('/')[-1]))\n",
    "\n",
    "print(\"Top concept URLs found in the RDF/XML:\")\n",
    "for url in top_concept_urls:\n",
    "    print(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we get the goal number, goal name and goal description for each top level goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   goal_code                                goal_name  \\\n",
      "0          1                               No poverty   \n",
      "1          2                              Zero hunger   \n",
      "2          3               Good health and well-being   \n",
      "3          4                        Quality education   \n",
      "4          5                          Gender equality   \n",
      "5          6               Clean water and sanitation   \n",
      "6          7              Affordable and clean energy   \n",
      "7          8          Decent work and economic growth   \n",
      "8          9  Industry, innovation and infrastructure   \n",
      "9         10                     Reduced inequalities   \n",
      "10        11       Sustainable cities and communities   \n",
      "11        12   Responsible consumption and production   \n",
      "12        13                           Climate action   \n",
      "13        14                         Life below water   \n",
      "14        15                             Life on land   \n",
      "15        16   Peace, justice and strong institutions   \n",
      "16        17               Partnerships for the goals   \n",
      "\n",
      "                                     goal_description  \\\n",
      "0             End poverty in all its forms everywhere   \n",
      "1   End hunger, achieve food security and improved...   \n",
      "2   Ensure healthy lives and promote well-being fo...   \n",
      "3   Ensure inclusive and equitable quality educati...   \n",
      "4   Achieve gender equality and empower all women ...   \n",
      "5   Ensure availability and sustainable management...   \n",
      "6   Ensure access to affordable, reliable, sustain...   \n",
      "7   Promote sustained, inclusive and sustainable e...   \n",
      "8   Build resilient infrastructure, promote inclus...   \n",
      "9        Reduce inequality within and among countries   \n",
      "10  Make cities and human settlements inclusive, s...   \n",
      "11  Ensure sustainable consumption and production ...   \n",
      "12  Take urgent action to combat climate change an...   \n",
      "13  Conserve and sustainably use the oceans, seas ...   \n",
      "14  Protect, restore and promote sustainable use o...   \n",
      "15  Promote peaceful and inclusive societies for s...   \n",
      "16  Strengthen the means of implementation and rev...   \n",
      "\n",
      "                         goal_url  \n",
      "0    http://metadata.un.org/sdg/1  \n",
      "1    http://metadata.un.org/sdg/2  \n",
      "2    http://metadata.un.org/sdg/3  \n",
      "3    http://metadata.un.org/sdg/4  \n",
      "4    http://metadata.un.org/sdg/5  \n",
      "5    http://metadata.un.org/sdg/6  \n",
      "6    http://metadata.un.org/sdg/7  \n",
      "7    http://metadata.un.org/sdg/8  \n",
      "8    http://metadata.un.org/sdg/9  \n",
      "9   http://metadata.un.org/sdg/10  \n",
      "10  http://metadata.un.org/sdg/11  \n",
      "11  http://metadata.un.org/sdg/12  \n",
      "12  http://metadata.un.org/sdg/13  \n",
      "13  http://metadata.un.org/sdg/14  \n",
      "14  http://metadata.un.org/sdg/15  \n",
      "15  http://metadata.un.org/sdg/16  \n",
      "16  http://metadata.un.org/sdg/17  \n",
      "SDG goals saved to: ./data/2025-02-19/04_processed/argo-france/sdg_goals.csv\n"
     ]
    }
   ],
   "source": [
    "# Prepare lists to store the results\n",
    "goal_codes = []\n",
    "goal_names = []\n",
    "goal_descriptions = []\n",
    "goal_urls = []\n",
    "\n",
    "# Loop through each top concept URL\n",
    "for url in top_concept_urls:\n",
    "    try:\n",
    "        # Fetch the RDF/XML content\n",
    "        resp = requests.get(url, headers={\"Accept\": \"application/rdf+xml\"})\n",
    "        resp.raise_for_status()\n",
    "        root = ET.fromstring(resp.content)\n",
    "        # Find the main Description element\n",
    "        desc = root.find('.//{http://www.w3.org/1999/02/22-rdf-syntax-ns#}Description')\n",
    "        if desc is None:\n",
    "            continue\n",
    "        # Extract <skos:note xml:lang=\"en\">Goal N</skos:note>\n",
    "        goal_code = None\n",
    "        for note in desc.findall('{http://www.w3.org/2004/02/skos/core#}note'):\n",
    "            if note.attrib.get('{http://www.w3.org/XML/1998/namespace}lang') == 'en' and note.text and note.text.startswith('Goal'):\n",
    "                goal_code = note.text.replace('Goal ', '').strip()\n",
    "                break\n",
    "        # Extract <skos:altLabel xml:lang=\"en\">...</skos:altLabel>\n",
    "        goal_name = None\n",
    "        for alt in desc.findall('{http://www.w3.org/2004/02/skos/core#}altLabel'):\n",
    "            if alt.attrib.get('{http://www.w3.org/XML/1998/namespace}lang') == 'en':\n",
    "                goal_name = alt.text.strip()\n",
    "                break\n",
    "        # Extract <skos:prefLabel xml:lang=\"en\">...</skos:prefLabel>\n",
    "        goal_description = None\n",
    "        for pref in desc.findall('{http://www.w3.org/2004/02/skos/core#}prefLabel'):\n",
    "            if pref.attrib.get('{http://www.w3.org/XML/1998/namespace}lang') == 'en':\n",
    "                goal_description = pref.text.strip()\n",
    "                break\n",
    "        # Store results\n",
    "        goal_codes.append(goal_code)\n",
    "        goal_names.append(goal_name)\n",
    "        goal_descriptions.append(goal_description)\n",
    "        goal_urls.append(url)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {e}\")\n",
    "\n",
    "# Create DataFrame\n",
    "df_sdg_goals = pd.DataFrame({\n",
    "    \"goal_code\": goal_codes,\n",
    "    \"goal_name\": goal_names,\n",
    "    \"goal_description\": goal_descriptions,\n",
    "    \"goal_url\": goal_urls\n",
    "})\n",
    "\n",
    "print(df_sdg_goals)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "sdg_goals_csv_path = f\"{processing_folder_path}/sdg_goals.csv\"\n",
    "df_sdg_goals.to_csv(sdg_goals_csv_path, index=False)\n",
    "print(f\"SDG goals saved to: {sdg_goals_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **step 7b-2 Here we prepare the System and User prompts to be used by an LLM.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text to classify:\n",
      "\n",
      "The United Nations Sustainable Development Goals (SDGs) are a universal call to action to end poverty, protect the planet, and ensure prosperity for all by 2030. They address global challenges such as inequality, climate change, environmental degradation, peace, and justice. The SDGs consist of 17 goals and 169 targets that aim to achieve a better and more sustainable future for all.\n",
      "\n",
      "Example Output Format:\n",
      "\n",
      "{\n",
      "    \"sdgs\": [2, 6, 17],\n",
      "    \"explanation\": \"This text is related to SDG 2 (Zero hunger) because it discusses food security, SDG 6 (Clean water and sanitation) due to references to environmental protection, and SDG 17 (Partnerships for the goals) as it mentions global cooperation.\"\n",
      "}\n",
      "\n",
      "System Prompt:\n",
      "\n",
      "You are an intelligent multi-label classification system designed to map texts to their relevant Sustainable Development Goals.\n",
      "Take the text delimited by triple quotation marks and return a JSON list of relevant SDGs. \n",
      "Example output format: \n",
      "{\n",
      "    \"sdgs\": [2, 6, 17],\n",
      "    \"explanation\": \"This text is related to SDG 2 (Zero hunger) because it discusses food security, SDG 6 (Clean water and sanitation) due to references to environmental protection, and SDG 17 (Partnerships for the goals) as it mentions global cooperation.\"\n",
      "}\n",
      "\n",
      "\n",
      "Here are the SDG goals and their descriptions:\n",
      "1: No poverty - End poverty in all its forms everywhere\n",
      "2: Zero hunger - End hunger, achieve food security and improved nutrition and promote sustainable agriculture\n",
      "3: Good health and well-being - Ensure healthy lives and promote well-being for all at all ages\n",
      "4: Quality education - Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all\n",
      "5: Gender equality - Achieve gender equality and empower all women and girls\n",
      "6: Clean water and sanitation - Ensure availability and sustainable management of water and sanitation for all\n",
      "7: Affordable and clean energy - Ensure access to affordable, reliable, sustainable and modern energy for all\n",
      "8: Decent work and economic growth - Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all\n",
      "9: Industry, innovation and infrastructure - Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation\n",
      "10: Reduced inequalities - Reduce inequality within and among countries\n",
      "11: Sustainable cities and communities - Make cities and human settlements inclusive, safe, resilient and sustainable\n",
      "12: Responsible consumption and production - Ensure sustainable consumption and production patterns\n",
      "13: Climate action - Take urgent action to combat climate change and its impacts\n",
      "14: Life below water - Conserve and sustainably use the oceans, seas and marine resources for sustainable development\n",
      "15: Life on land - Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss\n",
      "16: Peace, justice and strong institutions - Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels\n",
      "17: Partnerships for the goals - Strengthen the means of implementation and revitalize the Global Partnership for Sustainable Development\n",
      "\n",
      "\n",
      "User Prompt:\n",
      "\n",
      "\"Classify the following text in terms of its relevance to the Sustainable Development Goals:\",\n",
      "Text: '''\n",
      "The United Nations Sustainable Development Goals (SDGs) are a universal call to action to end poverty, protect the planet, and ensure prosperity for all by 2030. They address global challenges such as inequality, climate change, environmental degradation, peace, and justice. The SDGs consist of 17 goals and 169 targets that aim to achieve a better and more sustainable future for all.\n",
      "'''\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the text to classify\n",
    "text = \"\"\"\n",
    "The United Nations Sustainable Development Goals (SDGs) are a universal call to action to end poverty, protect the planet, and ensure prosperity for all by 2030. They address global challenges such as inequality, climate change, environmental degradation, peace, and justice. The SDGs consist of 17 goals and 169 targets that aim to achieve a better and more sustainable future for all.\n",
    "\"\"\"\n",
    "# Print the text to classify\n",
    "print(\"Text to classify:\")\n",
    "print(text)\n",
    "\n",
    "# Define the expected output format, now including an explanation field\n",
    "example_output_format = \"\"\"\n",
    "{\n",
    "    \"sdgs\": [2, 6, 17],\n",
    "    \"explanation\": \"This text is related to SDG 2 (Zero hunger) because it discusses food security, SDG 6 (Clean water and sanitation) due to references to environmental protection, and SDG 17 (Partnerships for the goals) as it mentions global cooperation.\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Print the example output format\n",
    "print(\"Example Output Format:\")\n",
    "print(example_output_format)\n",
    "\n",
    "# system_prompt\n",
    "# Build SDG goal info string from df_sdg_goals\n",
    "sdg_goal_info = \"\\n\".join(\n",
    "    f\"{row.goal_code}: {row.goal_name} - {row.goal_description}\"\n",
    "    for _, row in df_sdg_goals.iterrows()\n",
    ")\n",
    "\n",
    "sdg_system_prompt = f\"\"\"\n",
    "You are an intelligent multi-label classification system designed to map texts to their relevant Sustainable Development Goals.\n",
    "Take the text delimited by triple quotation marks and return a JSON list of relevant SDGs. \n",
    "Example output format: {example_output_format}\n",
    "\n",
    "Here are the SDG goals and their descriptions:\n",
    "{sdg_goal_info}\n",
    "\n",
    "\"\"\"\n",
    "# Print the system prompt\n",
    "print(\"System Prompt:\")\n",
    "print(sdg_system_prompt)\n",
    "# user_prompt\n",
    "sdg_user_prompt = f\"\"\"\n",
    "\"Classify the following text in terms of its relevance to the Sustainable Development Goals:\",\n",
    "Text: '''{text}'''\n",
    "\"\"\"\n",
    "# Print the user prompt\n",
    "print(\"User Prompt:\")\n",
    "print(sdg_user_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **step 7b-3: Get the LLM API prepared**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenWebUI API configuration\n",
    "openwebui_base_url = \"https://nebula.cs.vu.nl\"  # Replace with your actual OpenWebUI API base URL\n",
    "openwebui_api_key = \"sk-5b5a024888c14a019c0e9b4857df9329\"  # Replace with your actual API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first get the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curl -X GET 'https://nebula.cs.vu.nl/api/models' -H 'Authorization: Bearer sk-5b5a024888c14a019c0e9b4857df9329'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# print the request in curl\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurl -X GET \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapi_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m -H \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAuthorization: Bearer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mopenwebui_api_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     21\u001b[0m     models_json \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/data/ori_storage/OpenAIRE-tools/.venv/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/data/ori_storage/OpenAIRE-tools/.venv/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/data/ori_storage/OpenAIRE-tools/.venv/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/data/ori_storage/OpenAIRE-tools/.venv/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/data/ori_storage/OpenAIRE-tools/.venv/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/data/ori_storage/OpenAIRE-tools/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/data/ori_storage/OpenAIRE-tools/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/data/ori_storage/OpenAIRE-tools/.venv/lib/python3.10/site-packages/urllib3/connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    513\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1303\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1300\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1301\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1302\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1159\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This script fetches the list of available models from the OpenWebUI API\n",
    "# and prints their IDs, names, and parameter sizes.\n",
    "\n",
    "# Use the existing variables openwebui_base_url and openwebui_api_key\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {openwebui_api_key}\"\n",
    "}\n",
    "\n",
    "# Ensure the base URL does not end with a slash\n",
    "api_url = openwebui_base_url.rstrip('/') + \"/api/models\"\n",
    "\n",
    "# print the request in curl\n",
    "print(f\"curl -X GET '{api_url}' -H 'Authorization: Bearer {openwebui_api_key}'\")\n",
    "\n",
    "response = requests.get(api_url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    models_json = response.json()\n",
    "    models = models_json.get(\"data\", [])\n",
    "    print(\"Available models:\")\n",
    "    for model in models:\n",
    "        print(f\"- id: {model.get('id')}, name: {model.get('name')}, parameter_size: {model.get('ollama', {}).get('details', {}).get('parameter_size')}\")\n",
    "else:\n",
    "    print(f\"Failed to fetch models. Status code: {response.status_code}\")\n",
    "    print(f\"Response: {response.text}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the model to use, when no model is chosen, deepseek-r1:1.5b will be the default (faser & cheaper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvailable models:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mmodels\u001b[49m):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelect the model index to use (default: 2, llama3.1:8b) [timeout 10s]:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "# Select the model to use, when no model is chosen, llama3.1:8b will be the default\n",
    "model = \"llama3.1:8b\"  # Replace with your actual model name\n",
    "\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutError\n",
    "\n",
    "print(\"Available models:\")\n",
    "for i, m in enumerate(models):\n",
    "    print(f\"{i}: {m['id']}\")\n",
    "\n",
    "print(\"Select the model index to use (default: 2, llama3.1:8b) [timeout 10s]:\")\n",
    "signal.signal(signal.SIGALRM, timeout_handler)\n",
    "signal.alarm(10)\n",
    "try:\n",
    "    user_input = input()\n",
    "    if user_input.strip().isdigit():\n",
    "        selected_model_index = int(user_input.strip())\n",
    "        if 0 <= selected_model_index < len(models):\n",
    "            model = models[selected_model_index]['id']\n",
    "        else:\n",
    "            print(\"Invalid index, using default model.\")\n",
    "            model = \"llama3.1:8b\"\n",
    "    else:\n",
    "        print(\"No valid input, using default model.\")\n",
    "        model = \"llama3.1:8b\"\n",
    "except TimeoutError:\n",
    "    print(\"No response received. Using default model.\")\n",
    "    model = \"llama3.1:8b\"\n",
    "finally:\n",
    "    signal.alarm(0)\n",
    "\n",
    "print(f\"Model selected: {model}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **step 7b-4 Run the LLM over abstracts**\n",
    "\n",
    "Finally, for each abstract, run the system and user prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for record_id doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef: {'model': 'llama3.1:8b', 'messages': [{'role': 'system', 'content': '\\nYou are an intelligent multi-label classification system designed to map texts to their relevant Sustainable Development Goals.\\nTake the text delimited by triple quotation marks and return a JSON list of relevant SDGs. \\nExample output format: \\n{\\n    \"sdgs\": [2, 6, 17],\\n    \"explanation\": \"This text is related to SDG 2 (Zero hunger) because it discusses food security, SDG 6 (Clean water and sanitation) due to references to environmental protection, and SDG 17 (Partnerships for the goals) as it mentions global cooperation.\"\\n}\\n\\n\\nHere are the SDG goals and their descriptions:\\n1: No poverty - End poverty in all its forms everywhere\\n2: Zero hunger - End hunger, achieve food security and improved nutrition and promote sustainable agriculture\\n3: Good health and well-being - Ensure healthy lives and promote well-being for all at all ages\\n4: Quality education - Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all\\n5: Gender equality - Achieve gender equality and empower all women and girls\\n6: Clean water and sanitation - Ensure availability and sustainable management of water and sanitation for all\\n7: Affordable and clean energy - Ensure access to affordable, reliable, sustainable and modern energy for all\\n8: Decent work and economic growth - Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all\\n9: Industry, innovation and infrastructure - Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation\\n10: Reduced inequalities - Reduce inequality within and among countries\\n11: Sustainable cities and communities - Make cities and human settlements inclusive, safe, resilient and sustainable\\n12: Responsible consumption and production - Ensure sustainable consumption and production patterns\\n13: Climate action - Take urgent action to combat climate change and its impacts\\n14: Life below water - Conserve and sustainably use the oceans, seas and marine resources for sustainable development\\n15: Life on land - Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss\\n16: Peace, justice and strong institutions - Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels\\n17: Partnerships for the goals - Strengthen the means of implementation and revitalize the Global Partnership for Sustainable Development\\n\\n'}, {'role': 'user', 'content': \"Classify the following text in terms of its relevance to the Sustainable Development Goals:\\nText: '''Abstract</jats:title><jats:p>The Black Sea, the largest semienclosed anoxic basin on Earth, can be considered as an excellent natural laboratory for oxic and anoxic biogeochemical processes. The suboxic zone, a thin interface between oxic and anoxic waters, still remains poorly understood because it has been undersampled. This has led to alternative concepts regarding the underlying processes that create it. Existing hypotheses suggest that the interface originates either by isopycnal intrusions that introduce oxygen or the dynamics of manganese redox cycling that are associated with the sinking of particles or chemosynthetic bacteria. Here we reexamine these concepts using highâ€resolution oxygen, sulfide, nitrate, and particle concentration profiles obtained with sensors deployed on profiling floats. Our results show an extremely stable structure in density space over the entire basin with the exception of areas near the Bosporus plume and in the southern areas dominated by coastal anticyclones. The absence of largeâ€scale horizontal intrusive signatures in the openâ€sea supports a hypothesis prioritizing the role of biogeochemical processes.</jats:p>'''\"}]}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData for record_id \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecord_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Make the API call\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopenwebui_base_url\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/api/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAuthorization\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBearer \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mopenwebui_api_key\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mContent-Type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapplication/json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Check if the request was successful\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "File \u001b[0;32m~/data/ori_storage/OpenAIRE-tools/.venv/lib/python3.10/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/data/ori_storage/OpenAIRE-tools/.venv/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/data/ori_storage/OpenAIRE-tools/.venv/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/data/ori_storage/OpenAIRE-tools/.venv/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/data/ori_storage/OpenAIRE-tools/.venv/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/data/ori_storage/OpenAIRE-tools/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/data/ori_storage/OpenAIRE-tools/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/data/ori_storage/OpenAIRE-tools/.venv/lib/python3.10/site-packages/urllib3/connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    513\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1303\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1300\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1301\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1302\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1159\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Add a testing method to limit the number of abstracts\n",
    "if testing_mode:\n",
    "    # Load only the first 3 abstracts for testing\n",
    "    description_df = pd.read_parquet(f\"{processing_folder_path}/{folder_name}-descriptions-with-tokens.parquet\").head(3)\n",
    "else:\n",
    "    # Load all abstracts for production\n",
    "    description_df = pd.read_parquet(f\"{processing_folder_path}/{folder_name}-descriptions-with-tokens.parquet\")                                                             \n",
    "\n",
    "# Filter abstracts with at least 100 tokens\n",
    "description_df = description_df[description_df['token_count'] >= 100]\n",
    "\n",
    "# Prepare results list\n",
    "sdg_results = []\n",
    "\n",
    "# Loop through each abstract\n",
    "for idx, row in description_df.iterrows():\n",
    "    record_id = row['record_id']\n",
    "    abstract = row['description']\n",
    "\n",
    "    # Prepare the messages for the API\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": sdg_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Classify the following text in terms of its relevance to the Sustainable Development Goals:\\nText: '''{abstract}'''\"}\n",
    "    ]\n",
    "\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages\n",
    "    }\n",
    "\n",
    "    # Print the data variable for debugging\n",
    "    print(f\"Data for record_id {record_id}: {data}\")\n",
    "\n",
    "    # Make the API call\n",
    "    response = requests.post(\n",
    "        openwebui_base_url.rstrip('/') + \"/api/chat/completions\",\n",
    "        headers={\"Authorization\": f\"Bearer {openwebui_api_key}\", \"Content-Type\": \"application/json\"},\n",
    "        json=data\n",
    "    )\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error processing record_id {record_id}: {response.status_code} - {response.text}\")\n",
    "        continue\n",
    "\n",
    "    # Print the response for debugging\n",
    "    print(f\"Response for record_id {record_id}: {response.json()}\")\n",
    "    \n",
    "    # Parse the response\n",
    "    try:\n",
    "        result = response.json()\n",
    "        # Try to extract the SDG list from the response\n",
    "        content = result['choices'][0]['message']['content']\n",
    "        # Try to parse the JSON from the model output\n",
    "        try:\n",
    "            sdg_json = eval(content) if isinstance(content, str) else content\n",
    "            sdgs = sdg_json.get(\"sdgs\", [])\n",
    "            explanation = sdg_json.get(\"explanation\", \"\")\n",
    "        except Exception:\n",
    "            sdgs = []\n",
    "            explanation = \"\"\n",
    "    except Exception:\n",
    "        sdgs = []\n",
    "        explanation = \"\"\n",
    "\n",
    "    # Append to results, including the explanation if available\n",
    "    sdg_results.append({\n",
    "        \"record_id\": record_id,\n",
    "        \"abstract\": abstract,\n",
    "        \"sdgs\": sdgs,\n",
    "        \"explanation\": explanation\n",
    "    })\n",
    "\n",
    "    # Optional: print progress\n",
    "    print(f\"Processed record_id: {record_id}, SDGs: {sdgs}\")\n",
    "\n",
    "    # Optional: delay to avoid rate limits\n",
    "    time.sleep(1)\n",
    "\n",
    "# Print the number of results\n",
    "print(f\"Number of SDG results collected: {len(sdg_results)}\")\n",
    "\n",
    "# Make the value of the model variable suitable for using in the file names\n",
    "model_filename = model.replace(\":\", \"-\").replace(\" \", \"_\")\n",
    "\n",
    "# Save results to parquet\n",
    "sdg_results_df = pd.DataFrame(sdg_results)\n",
    "sdg_results_path = f\"{processing_folder_path}/{folder_name}-sdg-results-{model_filename}.parquet\"\n",
    "sdg_results_df.to_parquet(sdg_results_path, index=False)\n",
    "print(f\"SDG LLM results saved to: {sdg_results_path}\")\n",
    "print(f\"File size: {os.path.getsize(sdg_results_path) / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Get Genderize data\n",
    "a. First Query the authors with country of the affiliation along with the record id (to be used as primary keys, connecting the tables later on), \n",
    "\n",
    "b. get gender data by parsing the author names with country label over an API, \n",
    "\n",
    "c. save the outcomes in a separate parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authors with full names and ORCID IDs:\n",
      "                                        record_id             full_name  \\\n",
      "0  doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef        Emil V. Stanev   \n",
      "1  doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef  Pierreâ€Marie Poulain   \n",
      "2  doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef      Sebastian Grayek   \n",
      "3  doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef    Kenneth S. Johnson   \n",
      "4  doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef        HervÃ© Claustre   \n",
      "5  doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef       James W. Murray   \n",
      "6  doi_dedup___::e246801fc9ed25782358bac694517f8f   Desbruyeres, Damien   \n",
      "7  doi_dedup___::e246801fc9ed25782358bac694517f8f   McDonagh, Elaine L.   \n",
      "8  doi_dedup___::e246801fc9ed25782358bac694517f8f        King, Brian A.   \n",
      "9  doi_dedup___::e246801fc9ed25782358bac694517f8f     Thierry, Virginie   \n",
      "\n",
      "     first_name    last_name                orcid    country_name country_code  \n",
      "0       Emil V.       Stanev  0000-0002-1110-8645          France           FR  \n",
      "1  Pierreâ€Marie      Poulain  0000-0003-1342-8463          France           FR  \n",
      "2     Sebastian       Grayek  0000-0002-2461-757x          France           FR  \n",
      "3    Kenneth S.      Johnson  0000-0001-5513-5584          France           FR  \n",
      "4         HervÃ©     Claustre  0000-0001-6243-0258          France           FR  \n",
      "5      James W.       Murray                 None          France           FR  \n",
      "6        Damien  Desbruyeres                 None  United Kingdom           GB  \n",
      "7     Elaine L.     Mcdonagh  0000-0002-8813-4585  United Kingdom           GB  \n",
      "8      Brian A.         King  0000-0003-1338-3234  United Kingdom           GB  \n",
      "9      Virginie      Thierry  0000-0003-1602-6478  United Kingdom           GB  \n",
      "Authors data saved to: ./data/2025-02-19/04_processed/argo-france/argo-france-authors.parquet\n",
      "File size: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Query to extract authors along with their full names and record IDs\n",
    "authors = con.sql(f'''\n",
    "    SELECT \n",
    "        id AS record_id,\n",
    "        unnest.fullName AS full_name,\n",
    "        unnest.name AS first_name,\n",
    "        unnest.surname AS last_name,\n",
    "        unnest.pid.id.value AS orcid,\n",
    "        countries[1].label AS country_name,\n",
    "        countries[1].code AS country_code\n",
    "    FROM read_parquet('{master_file}')\n",
    "    CROSS JOIN UNNEST(authors) AS unnest\n",
    "    WHERE countries IS NOT NULL AND array_length(countries) > 0\n",
    "''').fetchall()\n",
    "\n",
    "# convert the result to a DataFrame\n",
    "authors_df = pd.DataFrame(authors, columns=['record_id', 'full_name', 'first_name', 'last_name', 'orcid', 'country_name', 'country_code'])\n",
    "\n",
    "# If testing_mode is True, limit to 10 authors\n",
    "if testing_mode:\n",
    "    authors_df = authors_df.head(10)\n",
    "\n",
    "# Print the authors DataFrame\n",
    "print(\"Authors with full names and ORCID IDs:\")\n",
    "print(authors_df)  \n",
    "\n",
    "# Save the authors data to a new Parquet file for later use\n",
    "authors_file_path = f\"{processing_folder_path}/{folder_name}-authors.parquet\"\n",
    "authors_df.to_parquet(authors_file_path, index=False)\n",
    "print(f\"Authors data saved to: {authors_file_path}\")\n",
    "print(f\"File size: {os.path.getsize(authors_file_path) / (1024**2):.2f} MB\")\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique authors linked to record IDs:\n",
      "                                        record_id    first_name country_code\n",
      "0  doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef          Emil           FR\n",
      "1  doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef  Pierreâ€Marie           FR\n",
      "2  doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef     Sebastian           FR\n",
      "3  doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef       Kenneth           FR\n",
      "4  doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef         HervÃ©           FR\n",
      "5  doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef         James           FR\n",
      "6  doi_dedup___::e246801fc9ed25782358bac694517f8f        Damien           GB\n",
      "7  doi_dedup___::e246801fc9ed25782358bac694517f8f        Elaine           GB\n",
      "8  doi_dedup___::e246801fc9ed25782358bac694517f8f         Brian           GB\n",
      "9  doi_dedup___::e246801fc9ed25782358bac694517f8f      Virginie           GB\n"
     ]
    }
   ],
   "source": [
    "# Filter the authors DataFrame to get unique names, countries, and record IDs\n",
    "# Only use the first occurrence of each first name\n",
    "unique_authors = authors_df[['record_id', 'first_name', 'country_code']].copy()\n",
    "unique_authors['first_name'] = unique_authors['first_name'].str.split().str[0]  # Keep only the first word\n",
    "# Remove one-letter names (e.g., \"L.\", \"S.\") that often end with a dot\n",
    "unique_authors = unique_authors[~unique_authors['first_name'].str.match(r'^[A-Z]\\.$', na=False)]\n",
    "# Drop rows where 'first_name' is None or NaN\n",
    "unique_authors = unique_authors.dropna(subset=['first_name'])\n",
    "unique_authors = unique_authors[unique_authors['first_name'] != 'None']\n",
    "unique_authors = unique_authors.drop_duplicates(subset=['first_name', 'record_id'], keep='first')\n",
    "\n",
    "# Print unique authors with record IDs\n",
    "print(\"Unique authors linked to record IDs:\")\n",
    "print(unique_authors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paid Subscription: False\n",
      "Testing Mode: True\n",
      "Rate Limit: 10 requests per second\n",
      "Delay between requests: 0.50 seconds\n"
     ]
    }
   ],
   "source": [
    "# Adding variables to handle rate limiting and API key for Genderize API\n",
    "\n",
    "# Check if the user has a paid subscription\n",
    "paid_subscription = False  # Set this to True if you have a paid subscription\n",
    "\n",
    "# Set the rate limit based on the testing mode\n",
    "if testing_mode:\n",
    "    rate_limit = 10  # Reduced rate limit for testing\n",
    "else:\n",
    "    rate_limit = 1000 if paid_subscription else 100 # Adjust rate limit based on subscription, setting a default for free users\n",
    "\n",
    "# delay between requests in seconds\n",
    "delay_between_requests = 0.5  # Calculate delay based on rate limit\n",
    "\n",
    "# Genderize API key\n",
    "genderize_api_key= \"da1a264b9bab63b46f27ac635dd7d2df\"  # Replace with your actual API key\n",
    "\n",
    "# Initialize request count\n",
    "request_count = 0  # Initialize request count\n",
    "\n",
    "# Base URL for Genderize API\n",
    "base_url = \"https://api.genderize.io\"\n",
    "\n",
    "# print all the above variables\n",
    "print(f\"Paid Subscription: {paid_subscription}\")\n",
    "print(f\"Testing Mode: {testing_mode}\")\n",
    "print(f\"Rate Limit: {rate_limit} requests per second\")\n",
    "print(f\"Delay between requests: {delay_between_requests:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: Emil (FR) - Gender: male\n",
      "Processed: Pierreâ€Marie (FR) - Gender: male\n",
      "Processed: Sebastian (FR) - Gender: male\n",
      "Processed: Kenneth (FR) - Gender: male\n",
      "Processed: HervÃ© (FR) - Gender: male\n",
      "Processed: James (FR) - Gender: male\n",
      "Processed: Damien (GB) - Gender: male\n",
      "Processed: Elaine (GB) - Gender: female\n",
      "Processed: Brian (GB) - Gender: male\n",
      "Processed: Virginie (GB) - Gender: female\n",
      "Gender data saved to: ./data/2025-02-19/04_processed/argo-france/argo-france-gender-data.parquet\n"
     ]
    }
   ],
   "source": [
    "# Initialize the list to store gender results\n",
    "gender_results = []\n",
    "\n",
    "# Iterate over the unique authors\n",
    "for _, row in unique_authors.iterrows():\n",
    "    if request_count >= rate_limit: # type: ignore\n",
    "        print(\"Rate limit reached. Stopping for the day.\")\n",
    "        break\n",
    "\n",
    "    first_name = row['first_name']\n",
    "    country_code = row['country_code']\n",
    "    record_id = row['record_id']  # Add the record ID\n",
    "\n",
    "    # Skip if the first name is missing\n",
    "    if pd.isna(first_name):\n",
    "        continue\n",
    "\n",
    "    # Prepare the API request\n",
    "    params = {\n",
    "        \"name\": first_name,\n",
    "        \"country_id\": country_code\n",
    "    }\n",
    "    if paid_subscription:\n",
    "        params[\"apikey\"] = genderize_api_key\n",
    "\n",
    "    try:\n",
    "        # Send the request to the Genderize API\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        # Append the result to the list\n",
    "        gender_results.append({\n",
    "            \"first_name\": first_name,\n",
    "            \"country_code\": country_code,\n",
    "            \"gender\": data.get(\"gender\"),\n",
    "            \"probability\": data.get(\"probability\"),\n",
    "            \"count\": data.get(\"count\")\n",
    "        })\n",
    "\n",
    "        # Increment the request count\n",
    "        request_count += 1\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Processed: {first_name} ({country_code}) - Gender: {data.get('gender')}\")\n",
    "\n",
    "        # Add a delay between requests to avoid overwhelming the API\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error processing {first_name} ({country_code}): {e}\")\n",
    "\n",
    "        # Increment the request count\n",
    "        request_count += 1\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Processed: {first_name} ({country_code}) - Gender: {data.get('gender')}\")\n",
    "\n",
    "        # Add a small delay to avoid overwhelming the API\n",
    "        time.sleep(1)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error processing {first_name} ({country_code}): {e}\")\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "gender_df = pd.DataFrame(gender_results)\n",
    "\n",
    "# Save the results to a Parquet file\n",
    "gender_file_path = f\"{processing_folder_path}/{folder_name}-gender-data.parquet\"\n",
    "gender_df.to_parquet(gender_file_path, index=False)\n",
    "print(f\"Gender data saved to: {gender_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Get Citizen Science classification labels\n",
    "\n",
    "a. Query the abstracts first along with the id (to be used as primary keys, connecting the tables later on), \n",
    "\n",
    "b. get citizen science labels by parsing the abstract over an LLM API with system prompt, \n",
    "\n",
    "c. save the outcomes in a separate parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Generate SQL schemas for all the parquet files in the processed folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating schema for: ./data/2025-02-19/04_processed/argo-france/argo-france-combined-id-pid.parquet\n",
      "Schema file path: ./data/2025-02-19/04_processed/argo-france/argo-france-combined-id-pid.sql\n",
      "Schema file exists: ./data/2025-02-19/04_processed/argo-france/argo-france-combined-id-pid.sql\n",
      "Schema file content:\n",
      "column_name,column_type,null,key,default,extra\n",
      "record_id,VARCHAR,YES,,,\n",
      "pid_scheme,VARCHAR,YES,,,\n",
      "pid_value,VARCHAR,YES,,,\n",
      "combined_id_pid,VARCHAR,YES,,,\n",
      "\n",
      "Generating schema for: ./data/2025-02-19/04_processed/argo-france/argo-france-descriptions-with-tokens.parquet\n",
      "Schema file path: ./data/2025-02-19/04_processed/argo-france/argo-france-descriptions-with-tokens.sql\n",
      "Schema file exists: ./data/2025-02-19/04_processed/argo-france/argo-france-descriptions-with-tokens.sql\n",
      "Schema file content:\n",
      "column_name,column_type,null,key,default,extra\n",
      "record_id,VARCHAR,YES,,,\n",
      "description,VARCHAR,YES,,,\n",
      "token_count,BIGINT,YES,,,\n",
      "\n",
      "Generating schema for: ./data/2025-02-19/04_processed/argo-france/argo-france-authors.parquet\n",
      "Schema file path: ./data/2025-02-19/04_processed/argo-france/argo-france-authors.sql\n",
      "Schema file exists: ./data/2025-02-19/04_processed/argo-france/argo-france-authors.sql\n",
      "Schema file content:\n",
      "column_name,column_type,null,key,default,extra\n",
      "record_id,VARCHAR,YES,,,\n",
      "full_name,VARCHAR,YES,,,\n",
      "first_name,VARCHAR,YES,,,\n",
      "last_name,VARCHAR,YES,,,\n",
      "orcid,VARCHAR,YES,,,\n",
      "country_name,VARCHAR,YES,,,\n",
      "country_code,VARCHAR,YES,,,\n",
      "\n",
      "Generating schema for: ./data/2025-02-19/04_processed/argo-france/argo-france-gender-data.parquet\n",
      "Schema file path: ./data/2025-02-19/04_processed/argo-france/argo-france-gender-data.sql\n",
      "Schema file exists: ./data/2025-02-19/04_processed/argo-france/argo-france-gender-data.sql\n",
      "Schema file content:\n",
      "column_name,column_type,null,key,default,extra\n",
      "first_name,VARCHAR,YES,,,\n",
      "country_code,VARCHAR,YES,,,\n",
      "gender,VARCHAR,YES,,,\n",
      "probability,DOUBLE,YES,,,\n",
      "count,BIGINT,YES,,,\n",
      "\n",
      "Generating schema for: ./data/2025-02-19/04_processed/argo-france/argo-france-sdg-llm-results.parquet\n",
      "Schema file path: ./data/2025-02-19/04_processed/argo-france/argo-france-sdg-llm-results.sql\n"
     ]
    },
    {
     "ename": "InvalidInputException",
     "evalue": "Invalid Input Error: Failed to read Parquet file './data/2025-02-19/04_processed/argo-france/argo-france-sdg-llm-results.parquet': Need at least one non-root column in the file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidInputException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating schema for: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparquet_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSchema file path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mschema_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mduckdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'''\u001b[39;49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;43m    COPY (\u001b[39;49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;43m        SELECT *\u001b[39;49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;43m        FROM (DESCRIBE \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mparquet_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;43m    )\u001b[39;49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;43m    TO \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mschema_file_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;43m\u001b[39;49m\u001b[38;5;124;43m'''\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(schema_file_path):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSchema file exists: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mschema_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mInvalidInputException\u001b[0m: Invalid Input Error: Failed to read Parquet file './data/2025-02-19/04_processed/argo-france/argo-france-sdg-llm-results.parquet': Need at least one non-root column in the file"
     ]
    }
   ],
   "source": [
    "# List all .parquet files in the processing folder\n",
    "parquet_dir = os.path.join(processing_folder_path)\n",
    "parquet_files = [\n",
    "    f for f in os.listdir(parquet_dir)\n",
    "    if f.endswith('.parquet')\n",
    "]\n",
    "\n",
    "for parquet_file in parquet_files:\n",
    "    parquet_path = os.path.join(processing_folder_path, parquet_file)\n",
    "    schema_file_name = os.path.splitext(parquet_file)[0] + '.sql'\n",
    "    schema_file_path = os.path.join(processing_folder_path, schema_file_name)\n",
    "    \n",
    "    print(f\"Generating schema for: {parquet_path}\")\n",
    "    print(f\"Schema file path: {schema_file_path}\")\n",
    "\n",
    "    duckdb.sql(f'''\n",
    "        COPY (\n",
    "            SELECT *\n",
    "            FROM (DESCRIBE '{parquet_path}')\n",
    "        )\n",
    "        TO '{schema_file_path}'\n",
    "    ''')\n",
    "\n",
    "    if os.path.exists(schema_file_path):\n",
    "        print(f\"Schema file exists: {schema_file_path}\")\n",
    "        with open(schema_file_path, 'r') as schema_file:\n",
    "            schema_content = schema_file.read()\n",
    "            print(\"Schema file content:\")\n",
    "            print(schema_content)\n",
    "    else:\n",
    "        print(f\"Schema file does not exist: {schema_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "custom-kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
