{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAIRE Community Data Dump Handling: Extraction, Tranformation and Enrichment\n",
    "\n",
    "In this notebook we will start with one of the [OpenAIRE Community Subgraphs](https://graph.openaire.eu/docs/downloads/subgraphs) to enrich that informatino for further analysis.\n",
    "\n",
    "This data process will extract an [OpenAIRE community data dump from Zenodo](https://doi.org/10.5281/zenodo.3974604), transforms it in to a portable file format .parquet (and updatable with changes for time seires analysis), that can be used to query with DuckDB, to enrich this with additional data  (also in .parquet, for join queries).\n",
    "\n",
    "This additional data can be societal impact data from [Altmetric.com](https://details-page-api-docs.altmetric.com/) or [Overton.io](https://app.overton.io/swagger.php), Gender data using [genderize.io](https://genderize.io/documentation), sdg classification using [aurora-sdg](https://aurora-universities.eu/sdg-research/sdg-api/)\n",
    "\n",
    "This script needs to be written in a way so that it can run every month using  the latest data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing steps\n",
    "\n",
    "* the folder ./data/ is put in .gitignore to prevent that bulk datais sent to a code repository. So make sure that folder exists, and mkdir if not exists. \n",
    "* The script downloads the lastest Data Dump Tar file from one selected community. See https://doi.org/10.5281/zenodo.3974604 for the latest list. In our case the Aurora tar file. https://zenodo.org/records/14887484/files/aurora.tar?download=1\n",
    "  * Use the json record of zenodo to get to the latest record, and fetch the download link of the aurora.tar file. for example : https://zenodo.org/records/14887484/export/json or https://zenodo.org/api/records/14887484/versions/latest \n",
    "  Make the tar filename a variable, so it can be used for multiple community dumps.\n",
    "  Download the tar file in a target folder ./data/{publication_date}/01-downloaded/{filename} where a subfolder is created using the filename and the timestamp. Make this also as a  variable to use later on.\n",
    "* Extract the tar file, to the compressed .json.gz files and put these in target folder ./data/{publication_date}/02-extracted/{filename}\n",
    "* Transform the compressed .json.gz files into a single .parquet file in target folder ./data/{publication_date}/03-transformed/{filename}\n",
    "Use instructions in sections \"Processing JSON files with DuckDB\" and \"Full dataset, bit by bit\" and \"Splitting and Processing JSON Files in Batches\" https://github.com/mosart/OpenAIRE-tools/blob/main/duckdb-querying.ipynb to start with. (be aware of error messages, and fix the issues to get all the data in)\n",
    "* Extract the SQL schema (schema-datadump.sql) from the .parquet file and put it in target folder ./data/{filename+timestamp}/02-transformed/ This is needed for further processing of the records with DuckDB later on.\n",
    "Use instructions in section \"Extracting Schema from Parquet File\" https://github.com/mosart/OpenAIRE-tools/blob/main/duckdb-querying.ipynb to start with.\n",
    "* Query to get all identifiers: openaire id, doi, isbn, hdl, etc.\n",
    "* **Get Altmetric data:**\n",
    "* Extract the Altmetric data using the Identifiers. Transform keeping the record id in .parquet and put that in target folder ./data/{publication_date}/04-processed/{filename}/\n",
    "* Extract the SQL schema (schema-altmetric.sql) from the .parquet file and put it in target folder ./data/{publication_date}/04-processed/{filename}/\n",
    "* **Get Overton data:** Repeat the altmetric steps, bun than for Overton.\n",
    "* **Get Gender data** query for the Author names and country codes, and run them over the gerderize api\n",
    "* **Get SDG data** query for the abstracts, and run abstracs larger than 100 tokens over the aurora-SDG api."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Mode\n",
    "Testign mode will reduce the number of records to process. Set to False if you want to go for the long haul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mode = True # Set to False for production\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Get the latest Community Dump File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37 .tar files in the dataset.\n",
      "Details of .tar files:\n",
      "[{'filename': 'energy-planning_1.tar', 'size': '6.99 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/energy-planning_1.tar/content', 'checksum': 'md5:0a2f551db46a9e629bb1d0a0098ae5cd'}, {'filename': 'edih-adria_1.tar', 'size': '5.86 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/edih-adria_1.tar/content', 'checksum': 'md5:23559bed5a9023398b431777bdc8a126'}, {'filename': 'uarctic_1.tar', 'size': '9.75 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/uarctic_1.tar/content', 'checksum': 'md5:302e3844ebd041c5f4ed94505eb9a285'}, {'filename': 'netherlands_1.tar', 'size': '3.91 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/netherlands_1.tar/content', 'checksum': 'md5:d1416c058b3961483aac340750ea8726'}, {'filename': 'knowmad_1.tar', 'size': '10.08 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/knowmad_1.tar/content', 'checksum': 'md5:a79573a02f2c9a9d65c33b3f3a2eaab9'}, {'filename': 'argo-france.tar', 'size': '0.00 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/argo-france.tar/content', 'checksum': 'md5:2ce6b0fcc6f876b600207759a0dc9758'}, {'filename': 'civica.tar', 'size': '0.23 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/civica.tar/content', 'checksum': 'md5:d2f24bbef06809a91d124f0b07cb1034'}, {'filename': 'covid-19.tar', 'size': '2.03 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/covid-19.tar/content', 'checksum': 'md5:3b741e8138f39932ca6c13ca106fe5d3'}, {'filename': 'aurora.tar', 'size': '1.73 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/aurora.tar/content', 'checksum': 'md5:9b6a8f38cd6f0ce16a85dfc020c220bf'}, {'filename': 'dh-ch.tar', 'size': '1.16 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/dh-ch.tar/content', 'checksum': 'md5:dbebdcc8ad7fd1dc7894fe03ebe2a978'}, {'filename': 'heritage-science.tar', 'size': '0.03 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/heritage-science.tar/content', 'checksum': 'md5:ffd2537b08c58d78eea4bc23a99b3c07'}, {'filename': 'dth.tar', 'size': '0.01 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/dth.tar/content', 'checksum': 'md5:643894810ac8bfce0f8273cf40d05a7a'}, {'filename': 'egrise.tar', 'size': '0.02 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/egrise.tar/content', 'checksum': 'md5:2f52b49fa8bd983bcf6884d6c4f5e952'}, {'filename': 'lifewatch-eric.tar', 'size': '0.05 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/lifewatch-eric.tar/content', 'checksum': 'md5:213c3f1ac83454ec01560d683a26362d'}, {'filename': 'iperionhs.tar', 'size': '0.00 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/iperionhs.tar/content', 'checksum': 'md5:6ffaf325257d9af5e43daa68505b1797'}, {'filename': 'eutopia.tar', 'size': '1.60 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/eutopia.tar/content', 'checksum': 'md5:f9eb5a1bb86caf6a4f2a7563453fc6df'}, {'filename': 'sdsn-gr.tar', 'size': '0.05 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/sdsn-gr.tar/content', 'checksum': 'md5:696f8b509a12c0bc898e1b9040a53790'}, {'filename': 'north-american-studies.tar', 'size': '0.36 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/north-american-studies.tar/content', 'checksum': 'md5:b677758820184053d9c1e6714394dd70'}, {'filename': 'knowmad_3.tar', 'size': '10.06 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/knowmad_3.tar/content', 'checksum': 'md5:68c5839a6355ea26f979ba781bc26a56'}, {'filename': 'knowmad_2.tar', 'size': '10.07 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/knowmad_2.tar/content', 'checksum': 'md5:95442e14703ba5db573cbf12296d76f4'}, {'filename': 'knowmad_5.tar', 'size': '5.37 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/knowmad_5.tar/content', 'checksum': 'md5:35482782eb621df9ec7365d34ccf3d07'}, {'filename': 'knowmad_4.tar', 'size': '10.04 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/knowmad_4.tar/content', 'checksum': 'md5:d770fcd862d5d7d4d918c59f028e24da'}, {'filename': 'beopen.tar', 'size': '0.20 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/beopen.tar/content', 'checksum': 'md5:447dbe25eaedc20a75568fd340f8e25d'}, {'filename': 'dariah.tar', 'size': '0.02 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/dariah.tar/content', 'checksum': 'md5:814e3a79b29da6b605018009f8575a8c'}, {'filename': 'eu-conexus.tar', 'size': '0.18 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/eu-conexus.tar/content', 'checksum': 'md5:19c86e2b79bde505112fb6b3d6e38eef'}, {'filename': 'elixir-gr.tar', 'size': '0.01 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/elixir-gr.tar/content', 'checksum': 'md5:e1dfe593d40c498e31a6ff5132da5fa4'}, {'filename': 'eut.tar', 'size': '0.21 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/eut.tar/content', 'checksum': 'md5:3da085b997006205c694b023aefafe7c'}, {'filename': 'enermaps.tar', 'size': '1.59 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/enermaps.tar/content', 'checksum': 'md5:b157900f8cb97cf4aa4f82ef59e2ff6e'}, {'filename': 'forthem.tar', 'size': '0.91 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/forthem.tar/content', 'checksum': 'md5:7864a0ddb0b676bb053bbcdf12a12525'}, {'filename': 'inria.tar', 'size': '0.27 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/inria.tar/content', 'checksum': 'md5:66a7e88ddda08626cbf26e182f7eafea'}, {'filename': 'mes.tar', 'size': '0.38 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/mes.tar/content', 'checksum': 'md5:10362f805616e391d350b395d9ae30a3'}, {'filename': 'neanias-underwater.tar', 'size': '0.02 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/neanias-underwater.tar/content', 'checksum': 'md5:4422b9a3be4a1e6cad61da46044a0ca2'}, {'filename': 'neanias-atmospheric.tar', 'size': '0.57 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/neanias-atmospheric.tar/content', 'checksum': 'md5:a3b9edbd956aee813cdfe97655c3fc9e'}, {'filename': 'neanias-space.tar', 'size': '0.73 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/neanias-space.tar/content', 'checksum': 'md5:618fbecca154af288dc1c92246b3d73b'}, {'filename': 'rural-digital-europe.tar', 'size': '0.83 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/rural-digital-europe.tar/content', 'checksum': 'md5:6543f5ead539a8bbad04593b641af0a1'}, {'filename': 'tunet.tar', 'size': '0.05 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/tunet.tar/content', 'checksum': 'md5:178b5a944133ded65d741ea5dc2c5990'}, {'filename': 'ni.tar', 'size': '3.01 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/ni.tar/content', 'checksum': 'md5:e82f580135522acd2da7277ea9389718'}]\n",
      "Publication date: 2025-02-19\n",
      "DOI: 10.5281/zenodo.14887484\n",
      "Title: OpenAIRE Graph: Dataset for research communities and initiatives\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Fetch the JSON data from the URL\n",
    "url = \"https://zenodo.org/api/records/14887484/versions/latest\"\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "# Extract the files information\n",
    "files = data.get(\"files\", [])\n",
    "\n",
    "# Create a list of dictionaries for the .tar files\n",
    "tar_files = []\n",
    "for file in files:\n",
    "    if file[\"key\"].endswith(\".tar\"):\n",
    "        tar_files.append({\n",
    "            \"filename\": file[\"key\"],\n",
    "            \"size\": f\"{file['size'] / (1024**3):.2f} GB\",  # Convert bytes to GB\n",
    "            \"downloadlink\": file[\"links\"][\"self\"],\n",
    "            \"checksum\": file[\"checksum\"]\n",
    "        })\n",
    "\n",
    "# print the tar files\n",
    "# If no tar files found, print a message\n",
    "if not tar_files:\n",
    "    print(\"No .tar files found in the dataset.\")\n",
    "else:\n",
    "    print(f\"Found {len(tar_files)} .tar files in the dataset.\")\n",
    "    print(\"Details of .tar files:\")\n",
    "    print(tar_files)\n",
    "\n",
    "# get and print the publication date\n",
    "publication_date = data.get(\"metadata\", {}).get(\"publication_date\", \"Unknown\")\n",
    "print(f\"Publication date: {publication_date}\")\n",
    "# get and print the DOI\n",
    "doi = data.get(\"doi\", \"Unknown\")\n",
    "print(f\"DOI: {doi}\")\n",
    "# get and print the title\n",
    "title = data.get(\"title\", \"Unknown\")\n",
    "print(f\"Title: {title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      filename      size  \\\n",
      "5              argo-france.tar   0.00 GB   \n",
      "8                   aurora.tar   1.73 GB   \n",
      "22                  beopen.tar   0.20 GB   \n",
      "6                   civica.tar   0.23 GB   \n",
      "7                 covid-19.tar   2.03 GB   \n",
      "23                  dariah.tar   0.02 GB   \n",
      "9                    dh-ch.tar   1.16 GB   \n",
      "11                     dth.tar   0.01 GB   \n",
      "1             edih-adria_1.tar   5.86 GB   \n",
      "12                  egrise.tar   0.02 GB   \n",
      "25               elixir-gr.tar   0.01 GB   \n",
      "0        energy-planning_1.tar   6.99 GB   \n",
      "27                enermaps.tar   1.59 GB   \n",
      "24              eu-conexus.tar   0.18 GB   \n",
      "26                     eut.tar   0.21 GB   \n",
      "15                 eutopia.tar   1.60 GB   \n",
      "28                 forthem.tar   0.91 GB   \n",
      "10        heritage-science.tar   0.03 GB   \n",
      "29                   inria.tar   0.27 GB   \n",
      "14               iperionhs.tar   0.00 GB   \n",
      "4                knowmad_1.tar  10.08 GB   \n",
      "19               knowmad_2.tar  10.07 GB   \n",
      "18               knowmad_3.tar  10.06 GB   \n",
      "21               knowmad_4.tar  10.04 GB   \n",
      "20               knowmad_5.tar   5.37 GB   \n",
      "13          lifewatch-eric.tar   0.05 GB   \n",
      "30                     mes.tar   0.38 GB   \n",
      "32     neanias-atmospheric.tar   0.57 GB   \n",
      "33           neanias-space.tar   0.73 GB   \n",
      "31      neanias-underwater.tar   0.02 GB   \n",
      "3            netherlands_1.tar   3.91 GB   \n",
      "36                      ni.tar   3.01 GB   \n",
      "17  north-american-studies.tar   0.36 GB   \n",
      "34    rural-digital-europe.tar   0.83 GB   \n",
      "16                 sdsn-gr.tar   0.05 GB   \n",
      "35                   tunet.tar   0.05 GB   \n",
      "2                uarctic_1.tar   9.75 GB   \n",
      "\n",
      "                                         downloadlink  \\\n",
      "5   https://zenodo.org/api/records/14887484/files/...   \n",
      "8   https://zenodo.org/api/records/14887484/files/...   \n",
      "22  https://zenodo.org/api/records/14887484/files/...   \n",
      "6   https://zenodo.org/api/records/14887484/files/...   \n",
      "7   https://zenodo.org/api/records/14887484/files/...   \n",
      "23  https://zenodo.org/api/records/14887484/files/...   \n",
      "9   https://zenodo.org/api/records/14887484/files/...   \n",
      "11  https://zenodo.org/api/records/14887484/files/...   \n",
      "1   https://zenodo.org/api/records/14887484/files/...   \n",
      "12  https://zenodo.org/api/records/14887484/files/...   \n",
      "25  https://zenodo.org/api/records/14887484/files/...   \n",
      "0   https://zenodo.org/api/records/14887484/files/...   \n",
      "27  https://zenodo.org/api/records/14887484/files/...   \n",
      "24  https://zenodo.org/api/records/14887484/files/...   \n",
      "26  https://zenodo.org/api/records/14887484/files/...   \n",
      "15  https://zenodo.org/api/records/14887484/files/...   \n",
      "28  https://zenodo.org/api/records/14887484/files/...   \n",
      "10  https://zenodo.org/api/records/14887484/files/...   \n",
      "29  https://zenodo.org/api/records/14887484/files/...   \n",
      "14  https://zenodo.org/api/records/14887484/files/...   \n",
      "4   https://zenodo.org/api/records/14887484/files/...   \n",
      "19  https://zenodo.org/api/records/14887484/files/...   \n",
      "18  https://zenodo.org/api/records/14887484/files/...   \n",
      "21  https://zenodo.org/api/records/14887484/files/...   \n",
      "20  https://zenodo.org/api/records/14887484/files/...   \n",
      "13  https://zenodo.org/api/records/14887484/files/...   \n",
      "30  https://zenodo.org/api/records/14887484/files/...   \n",
      "32  https://zenodo.org/api/records/14887484/files/...   \n",
      "33  https://zenodo.org/api/records/14887484/files/...   \n",
      "31  https://zenodo.org/api/records/14887484/files/...   \n",
      "3   https://zenodo.org/api/records/14887484/files/...   \n",
      "36  https://zenodo.org/api/records/14887484/files/...   \n",
      "17  https://zenodo.org/api/records/14887484/files/...   \n",
      "34  https://zenodo.org/api/records/14887484/files/...   \n",
      "16  https://zenodo.org/api/records/14887484/files/...   \n",
      "35  https://zenodo.org/api/records/14887484/files/...   \n",
      "2   https://zenodo.org/api/records/14887484/files/...   \n",
      "\n",
      "                                checksum  \n",
      "5   md5:2ce6b0fcc6f876b600207759a0dc9758  \n",
      "8   md5:9b6a8f38cd6f0ce16a85dfc020c220bf  \n",
      "22  md5:447dbe25eaedc20a75568fd340f8e25d  \n",
      "6   md5:d2f24bbef06809a91d124f0b07cb1034  \n",
      "7   md5:3b741e8138f39932ca6c13ca106fe5d3  \n",
      "23  md5:814e3a79b29da6b605018009f8575a8c  \n",
      "9   md5:dbebdcc8ad7fd1dc7894fe03ebe2a978  \n",
      "11  md5:643894810ac8bfce0f8273cf40d05a7a  \n",
      "1   md5:23559bed5a9023398b431777bdc8a126  \n",
      "12  md5:2f52b49fa8bd983bcf6884d6c4f5e952  \n",
      "25  md5:e1dfe593d40c498e31a6ff5132da5fa4  \n",
      "0   md5:0a2f551db46a9e629bb1d0a0098ae5cd  \n",
      "27  md5:b157900f8cb97cf4aa4f82ef59e2ff6e  \n",
      "24  md5:19c86e2b79bde505112fb6b3d6e38eef  \n",
      "26  md5:3da085b997006205c694b023aefafe7c  \n",
      "15  md5:f9eb5a1bb86caf6a4f2a7563453fc6df  \n",
      "28  md5:7864a0ddb0b676bb053bbcdf12a12525  \n",
      "10  md5:ffd2537b08c58d78eea4bc23a99b3c07  \n",
      "29  md5:66a7e88ddda08626cbf26e182f7eafea  \n",
      "14  md5:6ffaf325257d9af5e43daa68505b1797  \n",
      "4   md5:a79573a02f2c9a9d65c33b3f3a2eaab9  \n",
      "19  md5:95442e14703ba5db573cbf12296d76f4  \n",
      "18  md5:68c5839a6355ea26f979ba781bc26a56  \n",
      "21  md5:d770fcd862d5d7d4d918c59f028e24da  \n",
      "20  md5:35482782eb621df9ec7365d34ccf3d07  \n",
      "13  md5:213c3f1ac83454ec01560d683a26362d  \n",
      "30  md5:10362f805616e391d350b395d9ae30a3  \n",
      "32  md5:a3b9edbd956aee813cdfe97655c3fc9e  \n",
      "33  md5:618fbecca154af288dc1c92246b3d73b  \n",
      "31  md5:4422b9a3be4a1e6cad61da46044a0ca2  \n",
      "3   md5:d1416c058b3961483aac340750ea8726  \n",
      "36  md5:e82f580135522acd2da7277ea9389718  \n",
      "17  md5:b677758820184053d9c1e6714394dd70  \n",
      "34  md5:6543f5ead539a8bbad04593b641af0a1  \n",
      "16  md5:696f8b509a12c0bc898e1b9040a53790  \n",
      "35  md5:178b5a944133ded65d741ea5dc2c5990  \n",
      "2   md5:302e3844ebd041c5f4ed94505eb9a285  \n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame to hold the tar files information for later use.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df_tar_files = pd.DataFrame(tar_files)\n",
    "\n",
    "# Sort the DataFrame by filename alphabetically\n",
    "df_tar_files = df_tar_files.sort_values(by='filename')\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df_tar_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tar files:\n",
      "    index                    filename      size\n",
      "0       5             argo-france.tar   0.00 GB\n",
      "1       8                  aurora.tar   1.73 GB\n",
      "2      22                  beopen.tar   0.20 GB\n",
      "3       6                  civica.tar   0.23 GB\n",
      "4       7                covid-19.tar   2.03 GB\n",
      "5      23                  dariah.tar   0.02 GB\n",
      "6       9                   dh-ch.tar   1.16 GB\n",
      "7      11                     dth.tar   0.01 GB\n",
      "8       1            edih-adria_1.tar   5.86 GB\n",
      "9      12                  egrise.tar   0.02 GB\n",
      "10     25               elixir-gr.tar   0.01 GB\n",
      "11      0       energy-planning_1.tar   6.99 GB\n",
      "12     27                enermaps.tar   1.59 GB\n",
      "13     24              eu-conexus.tar   0.18 GB\n",
      "14     26                     eut.tar   0.21 GB\n",
      "15     15                 eutopia.tar   1.60 GB\n",
      "16     28                 forthem.tar   0.91 GB\n",
      "17     10        heritage-science.tar   0.03 GB\n",
      "18     29                   inria.tar   0.27 GB\n",
      "19     14               iperionhs.tar   0.00 GB\n",
      "20      4               knowmad_1.tar  10.08 GB\n",
      "21     19               knowmad_2.tar  10.07 GB\n",
      "22     18               knowmad_3.tar  10.06 GB\n",
      "23     21               knowmad_4.tar  10.04 GB\n",
      "24     20               knowmad_5.tar   5.37 GB\n",
      "25     13          lifewatch-eric.tar   0.05 GB\n",
      "26     30                     mes.tar   0.38 GB\n",
      "27     32     neanias-atmospheric.tar   0.57 GB\n",
      "28     33           neanias-space.tar   0.73 GB\n",
      "29     31      neanias-underwater.tar   0.02 GB\n",
      "30      3           netherlands_1.tar   3.91 GB\n",
      "31     36                      ni.tar   3.01 GB\n",
      "32     17  north-american-studies.tar   0.36 GB\n",
      "33     34    rural-digital-europe.tar   0.83 GB\n",
      "34     16                 sdsn-gr.tar   0.05 GB\n",
      "35     35                   tunet.tar   0.05 GB\n",
      "36      2               uarctic_1.tar   9.75 GB\n",
      "Selected file: aurora.tar\n",
      "Download link: https://zenodo.org/api/records/14887484/files/aurora.tar/content\n",
      "Checksum: md5:9b6a8f38cd6f0ce16a85dfc020c220bf\n"
     ]
    }
   ],
   "source": [
    "# Print a reindexed list of available tar files\n",
    "print(\"Available tar files:\")\n",
    "print(df_tar_files[['filename', 'size']].reset_index())\n",
    "\n",
    "import signal\n",
    "\n",
    "# Function to handle timeout\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutError\n",
    "\n",
    "# Set the timeout handler for the input\n",
    "signal.signal(signal.SIGALRM, timeout_handler)\n",
    "signal.alarm(10)  # Set the timeout to 10 seconds\n",
    "\n",
    "try:\n",
    "    # Ask the user to select a tar file by its index\n",
    "    selected_index = int(input(\"Enter the index of the tar file you want to download: \"))\n",
    "except TimeoutError:\n",
    "    print(\"No response received. Defaulting to index 1.\")\n",
    "    selected_index = 1\n",
    "finally:\n",
    "    signal.alarm(0)  # Disable the alarm\n",
    "\n",
    "# Get the selected tar file's download link and checksum\n",
    "selected_file = df_tar_files.iloc[selected_index]\n",
    "downloadlink = selected_file['downloadlink']\n",
    "checksum = selected_file['checksum']\n",
    "\n",
    "print(f\"Selected file: {selected_file['filename']}\")\n",
    "print(f\"Download link: {downloadlink}\")\n",
    "print(f\"Checksum: {checksum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name: aurora.tar\n",
      "Download Path File: ./data/2025-02-19/01_input/aurora.tar\n",
      "Folder Name: aurora\n",
      "Extraction Path Folder: ./data/2025-02-19/02_extracted/aurora\n"
     ]
    }
   ],
   "source": [
    "# Path Variables\n",
    "\n",
    "# Extract the file name from the selected file\n",
    "file_name = selected_file['filename']    \n",
    "\n",
    "# Path to save the downloaded tar file using file_name variable\n",
    "download_path = f\"./data/{publication_date}/01_input/{file_name}\"\n",
    "\n",
    "# Create the folder name by removing the .tar extension\n",
    "folder_name = selected_file['filename'].replace('.tar', '')\n",
    "\n",
    "# Path to save the extracted files using the file_name variable without the .tar extension\n",
    "extraction_path = f\"./data/{publication_date}/02_extracted/{folder_name}\"\n",
    "\n",
    "\n",
    "print(f\"File Name: {file_name}\")\n",
    "print(f\"Download Path File: {download_path}\")\n",
    "print(f\"Folder Name: {folder_name}\")\n",
    "print(f\"Extraction Path Folder: {extraction_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the tar file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file: aurora.tar (1.73 GB)\n",
      "Download URL: https://zenodo.org/api/records/14887484/files/aurora.tar/content\n",
      "Estimated download time: 177.15 seconds\n",
      "Download complete: ./data/2025-02-19/01_input/aurora.tar\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Ensure the directory for the download path exists\n",
    "os.makedirs(os.path.dirname(download_path), exist_ok=True)\n",
    "\n",
    "# Check if the file already exists\n",
    "if not os.path.exists(download_path):\n",
    "    # Get the file size in bytes\n",
    "    file_size_bytes = float(selected_file['size'].split()[0]) * (1024**3)  # Convert GB to bytes\n",
    "    print(f\"Downloading file: {selected_file['filename']} ({selected_file['size']})\")\n",
    "    print(f\"Download URL: {downloadlink}\")\n",
    "    \n",
    "    # Estimate download duration assuming an average speed of 10 MB/s\n",
    "    avg_speed = 10 * (1024**2)  # 10 MB/s in bytes\n",
    "    estimated_duration = file_size_bytes / avg_speed\n",
    "    print(f\"Estimated download time: {estimated_duration:.2f} seconds\")\n",
    "    \n",
    "    # Download the selected tar file\n",
    "    response = requests.get(downloadlink, stream=True)\n",
    "    with open(download_path, 'wb') as file:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            file.write(chunk)\n",
    "else:\n",
    "    print(f\"File already exists: {download_path}\")\n",
    "    print(f\"Download URL: {downloadlink}\")\n",
    "\n",
    "print(f\"Download complete: {download_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checksum verification passed.\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "# Function to calculate the checksum of a file\n",
    "def calculate_checksum(file_path, algorithm):\n",
    "    hash_func = hashlib.new(algorithm)\n",
    "    with open(file_path, 'rb') as f:\n",
    "        while chunk := f.read(8192):\n",
    "            hash_func.update(chunk)\n",
    "    return hash_func.hexdigest()\n",
    "\n",
    "# Extract the checksum algorithm and value\n",
    "checksum_parts = checksum.split(':', 1)\n",
    "checksum_algorithm = checksum_parts[0]\n",
    "expected_checksum = checksum_parts[1]\n",
    "\n",
    "# Calculate the checksum of the downloaded file\n",
    "calculated_checksum = calculate_checksum(download_path, algorithm=checksum_algorithm)\n",
    "\n",
    "# Compare the calculated checksum with the provided checksum\n",
    "if calculated_checksum == expected_checksum:\n",
    "    print(\"Checksum verification passed.\")\n",
    "else:\n",
    "    print(\"Checksum verification failed.\")\n",
    "    print(f\"Expected: {expected_checksum}\")\n",
    "    print(f\"Calculated: {calculated_checksum}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract the tar file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/2025-02-19/01_input/aurora.tar to ./data/2025-02-19/02_extracted/aurora...\n",
      "Extracted only the first 10 files for testing mode.\n",
      "Extraction complete.\n",
      "Files extracted to: ./data/2025-02-19/02_extracted/aurora\n",
      "Number of files extracted: 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if the extraction directory already exists and contains files\n",
    "if os.path.exists(extraction_path) and os.listdir(extraction_path):\n",
    "    print(\"The tar file has already been extracted.\")\n",
    "else:\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(extraction_path, exist_ok=True)\n",
    "\n",
    "    # Extract the tar file in the parent directory of the extraction_path - because the tar file contains a folder structure repeating the name of the tar file\n",
    "    print(f\"Extracting {download_path} to {extraction_path}...\")\n",
    "    parent_extraction_path = os.path.dirname(extraction_path)\n",
    "    with tarfile.open(download_path, 'r') as tar:\n",
    "        if testing_mode:\n",
    "            # Extract only the first 10 files for testing\n",
    "            members = tar.getmembers()[:10]\n",
    "            tar.extractall(path=parent_extraction_path, members=members)\n",
    "            print(\"Extracted only the first 10 files for testing mode.\")\n",
    "        else:\n",
    "            tar.extractall(path=parent_extraction_path)\n",
    "\n",
    "    print(\"Extraction complete.\")\n",
    "    print(f\"Files extracted to: {extraction_path}\")\n",
    "    # print the number of files extracted\n",
    "    extracted_files = os.listdir(extraction_path)\n",
    "    print(f\"Number of files extracted: {len(extracted_files)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 10\n",
      "First 5 files:\n",
      "part-00007-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz\n",
      "part-00005-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz\n",
      "part-00002-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz\n",
      "part-00009-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz\n",
      "part-00004-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz\n",
      "                                            filename\n",
      "8  part-00000-7a70885f-56f2-4cc2-b836-a5bd99ab23c...\n",
      "5  part-00001-7a70885f-56f2-4cc2-b836-a5bd99ab23c...\n",
      "2  part-00002-7a70885f-56f2-4cc2-b836-a5bd99ab23c...\n",
      "6  part-00003-7a70885f-56f2-4cc2-b836-a5bd99ab23c...\n",
      "4  part-00004-7a70885f-56f2-4cc2-b836-a5bd99ab23c...\n",
      "1  part-00005-7a70885f-56f2-4cc2-b836-a5bd99ab23c...\n",
      "7  part-00006-7a70885f-56f2-4cc2-b836-a5bd99ab23c...\n",
      "0  part-00007-7a70885f-56f2-4cc2-b836-a5bd99ab23c...\n",
      "9  part-00008-7a70885f-56f2-4cc2-b836-a5bd99ab23c...\n",
      "3  part-00009-7a70885f-56f2-4cc2-b836-a5bd99ab23c...\n",
      "DataFrame dimensions: (10, 1)\n",
      "Randomly selected files with full paths for testing:\n",
      "./data/2025-02-19/02_extracted/aurora/part-00002-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz\n",
      "./data/2025-02-19/02_extracted/aurora/part-00007-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz\n",
      "./data/2025-02-19/02_extracted/aurora/part-00008-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz\n",
      "./data/2025-02-19/02_extracted/aurora/part-00009-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz\n",
      "./data/2025-02-19/02_extracted/aurora/part-00001-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz\n",
      "Random file selected for later use: part-00006-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz\n",
      "Path to the random file: ./data/2025-02-19/02_extracted/aurora/part-00006-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz\n",
      "The random file exists: ./data/2025-02-19/02_extracted/aurora/part-00006-7a70885f-56f2-4cc2-b836-a5bd99ab23c3-c000.json.gz\n"
     ]
    }
   ],
   "source": [
    "# List the extracted files\n",
    "extracted_files = os.listdir(extraction_path)\n",
    "\n",
    "# if testing_mode is True, limit the number of files to 10 for testing purposes\n",
    "if testing_mode:\n",
    "    extracted_files = extracted_files[:10]\n",
    "\n",
    "# add the path to the extracted files\n",
    "extracted_files_with_path = [os.path.join(extraction_path, file) for file in extracted_files]\n",
    "\n",
    "# count the number of files in the extracted folder\n",
    "num_files = len(extracted_files)\n",
    "print(f\"Number of files: {num_files}\")\n",
    "\n",
    "# print the first 5 files\n",
    "print(\"First 5 files:\")\n",
    "for file in extracted_files[:5]:\n",
    "    print(file) \n",
    "\n",
    "# make a DataFrame for the extracted files\n",
    "df_extracted_files = pd.DataFrame(extracted_files, columns=['filename'])\n",
    "# Sort the DataFrame by filename alphabetically\n",
    "df_extracted_files = df_extracted_files.sort_values(by='filename')\n",
    "# Print the DataFrame\n",
    "print(df_extracted_files)\n",
    "\n",
    "# print the dimensions of the DataFrame\n",
    "print(f\"DataFrame dimensions: {df_extracted_files.shape}\")\n",
    "\n",
    "# print a random 5 files, to be used for testing, and use in a variable for later use\n",
    "import random\n",
    "random_files = random.sample(extracted_files, 5)\n",
    "random_files_with_path = [os.path.join(extraction_path, file) for file in random_files]\n",
    "print(\"Randomly selected files with full paths for testing:\")\n",
    "for file in random_files_with_path:\n",
    "    print(file)\n",
    "\n",
    "# one random file for later use\n",
    "random_file = random.choice(extracted_files)\n",
    "print(f\"Random file selected for later use: {random_file}\")\n",
    "# Define the path to the random file\n",
    "random_file_path = os.path.join(extraction_path, random_file)\n",
    "print(f\"Path to the random file: {random_file_path}\")\n",
    "# Check if the random file exists\n",
    "if os.path.exists(random_file_path):\n",
    "    print(f\"The random file exists: {random_file_path}\")\n",
    "else:\n",
    "    print(f\"The random file does not exist: {random_file_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Get a data sample to generate parquetfile and the SQL schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master file path: ./data/2025-02-19/03_transformed/aurora/aurora-master.parquet\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "transformation_folder_path = f\"./data/{publication_date}/03_transformed/{folder_name}\"\n",
    "\n",
    "# Ensure the target directory exists\n",
    "os.makedirs(transformation_folder_path, exist_ok=True)\n",
    "\n",
    "# define and print the target output master file for all extracted files\n",
    "master_file = f\"{transformation_folder_path}/{folder_name}-master.parquet\"\n",
    "print(f\"Master file path: {master_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing extracted files into one master parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed data saved to: ./data/2025-02-19/03_transformed/aurora/aurora-master.parquet\n",
      "File size: 59.46 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import signal\n",
    "\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutError\n",
    "\n",
    "# Check if the master file already exists\n",
    "if os.path.exists(master_file):\n",
    "    print(f\"Master file already exists: {master_file}\")\n",
    "    print(\"Do you want to overwrite it? (y/n) [Default: n, timeout 10s]:\")\n",
    "    signal.signal(signal.SIGALRM, timeout_handler)\n",
    "    signal.alarm(10)\n",
    "    try:\n",
    "        user_input = input()\n",
    "        overwrite = user_input.strip().lower() == 'y'\n",
    "    except TimeoutError:\n",
    "        print(\"No response received. Continuing with the existing master file.\")\n",
    "        overwrite = False\n",
    "    finally:\n",
    "        signal.alarm(0)\n",
    "    if not overwrite:\n",
    "        print(\"Using the existing master file.\")\n",
    "    else:\n",
    "        # Overwrite: regenerate the master file\n",
    "        con = duckdb.connect()\n",
    "        file_paths = ','.join(f\"'{file}'\" for file in extracted_files_with_path)\n",
    "        con.sql(f'''\n",
    "            COPY (\n",
    "                SELECT *\n",
    "                FROM read_json([{file_paths}], sample_size=-1, union_by_name=true)\n",
    "            )\n",
    "            TO '{master_file}' (FORMAT parquet, COMPRESSION gzip)\n",
    "        ''')\n",
    "        print(f\"Transformed data saved to: {master_file}\")\n",
    "        print(f\"File size: {os.path.getsize(master_file) / (1024**2):.2f} MB\")\n",
    "        con.close()\n",
    "else:\n",
    "    # Master file does not exist, create it\n",
    "    con = duckdb.connect()\n",
    "    file_paths = ','.join(f\"'{file}'\" for file in extracted_files_with_path)\n",
    "    con.sql(f'''\n",
    "        COPY (\n",
    "            SELECT *\n",
    "            FROM read_json([{file_paths}], sample_size=-1, union_by_name=true)\n",
    "        )\n",
    "        TO '{master_file}' (FORMAT parquet, COMPRESSION gzip)\n",
    "    ''')\n",
    "    print(f\"Transformed data saved to: {master_file}\")\n",
    "    print(f\"File size: {os.path.getsize(master_file) / (1024**2):.2f} MB\")\n",
    "    con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema file path: ./data/2025-02-19/03_transformed/aurora/aurora-schema.sql\n",
      "Schema file exists: ./data/2025-02-19/03_transformed/aurora/aurora-schema.sql\n",
      "Schema file content:\n",
      "column_name,column_type,null,key,default,extra\n",
      "authors,\"STRUCT(fullName VARCHAR, \"\"name\"\" VARCHAR, pid STRUCT(id STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR), provenance STRUCT(provenance VARCHAR, trust VARCHAR)), rank BIGINT, surname VARCHAR)[]\",YES,,,\n",
      "bestAccessRight,\"STRUCT(code VARCHAR, \"\"label\"\" VARCHAR, scheme VARCHAR)\",YES,,,\n",
      "collectedFrom,\"STRUCT(\"\"key\"\" VARCHAR, \"\"value\"\" VARCHAR)[]\",YES,,,\n",
      "communities,\"STRUCT(code VARCHAR, \"\"label\"\" VARCHAR, provenance STRUCT(provenance VARCHAR, trust VARCHAR)[])[]\",YES,,,\n",
      "contributors,VARCHAR[],YES,,,\n",
      "countries,\"STRUCT(code VARCHAR, \"\"label\"\" VARCHAR, provenance STRUCT(provenance VARCHAR, trust VARCHAR))[]\",YES,,,\n",
      "coverages,VARCHAR[],YES,,,\n",
      "dateOfCollection,VARCHAR,YES,,,\n",
      "descriptions,VARCHAR[],YES,,,\n",
      "formats,VARCHAR[],YES,,,\n",
      "id,VARCHAR,YES,,,\n",
      "indicators,\"STRUCT(citationImpact STRUCT(citationClass VARCHAR, citationCount DOUBLE, impulse DOUBLE, impulseClass VARCHAR, influence DOUBLE, influenceClass VARCHAR, popularity DOUBLE, popularityClass VARCHAR), usageCounts STRUCT(downloads BIGINT, \"\"views\"\" BIGINT))\",YES,,,\n",
      "instances,\"STRUCT(alternateIdentifiers STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR)[], collectedFrom STRUCT(\"\"key\"\" VARCHAR, \"\"value\"\" VARCHAR), hostedBy STRUCT(\"\"key\"\" VARCHAR, \"\"value\"\" VARCHAR), pids STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR)[], publicationDate DATE, refereed VARCHAR, \"\"type\"\" VARCHAR, urls VARCHAR[], accessRight STRUCT(code VARCHAR, \"\"label\"\" VARCHAR, scheme VARCHAR, openAccessRoute VARCHAR), license VARCHAR, articleProcessingCharge STRUCT(amount VARCHAR, currency VARCHAR))[]\",YES,,,\n",
      "isGreen,BOOLEAN,YES,,,\n",
      "isInDiamondJournal,BOOLEAN,YES,,,\n",
      "language,\"STRUCT(code VARCHAR, \"\"label\"\" VARCHAR)\",YES,,,\n",
      "lastUpdateTimeStamp,BIGINT,YES,,,\n",
      "mainTitle,VARCHAR,YES,,,\n",
      "organizations,\"STRUCT(id VARCHAR, legalName VARCHAR, pids STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR)[])[]\",YES,,,\n",
      "originalIds,VARCHAR[],YES,,,\n",
      "pids,\"STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR)[]\",YES,,,\n",
      "publicationDate,DATE,YES,,,\n",
      "publiclyFunded,BOOLEAN,YES,,,\n",
      "publisher,VARCHAR,YES,,,\n",
      "sources,VARCHAR[],YES,,,\n",
      "subjects,\"STRUCT(provenance STRUCT(provenance VARCHAR, trust VARCHAR), subject STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR))[]\",YES,,,\n",
      "type,VARCHAR,YES,,,\n",
      "container,\"STRUCT(ep VARCHAR, iss VARCHAR, issnPrinted VARCHAR, \"\"name\"\" VARCHAR, sp VARCHAR, vol VARCHAR, edition VARCHAR, issnLinking VARCHAR, issnOnline VARCHAR, conferenceDate VARCHAR)\",YES,,,\n",
      "openAccessColor,VARCHAR,YES,,,\n",
      "subTitle,VARCHAR,YES,,,\n",
      "projects,\"STRUCT(acronym VARCHAR, code VARCHAR, funder STRUCT(fundingStream VARCHAR, jurisdiction VARCHAR, \"\"name\"\" VARCHAR, shortName VARCHAR), id VARCHAR, provenance STRUCT(provenance VARCHAR, trust VARCHAR), title VARCHAR, validated STRUCT(validatedByFunder BOOLEAN, validationDate DATE))[]\",YES,,,\n",
      "embargoEndDate,DATE,YES,,,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# schema path\n",
    "schema_file_path = f\"{transformation_folder_path}/{folder_name}-schema.sql\"\n",
    "\n",
    "#print the schema file path\n",
    "print(f\"Schema file path: {schema_file_path}\")\n",
    "\n",
    "duckdb.sql(f'''\n",
    "    COPY (\n",
    "        SELECT *\n",
    "        FROM (DESCRIBE '{master_file}')\n",
    "    )\n",
    "    TO '{schema_file_path}'\n",
    "''')\n",
    "# check if the schema file exists\n",
    "if os.path.exists(schema_file_path):\n",
    "    print(f\"Schema file exists: {schema_file_path}\")\n",
    "else:\n",
    "    print(f\"Schema file does not exist: {schema_file_path}\")\n",
    "# Print the schema file content\n",
    "with open(schema_file_path, 'r') as schema_file:\n",
    "    schema_content = schema_file.read()\n",
    "    print(\"Schema file content:\")\n",
    "    print(schema_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in the Parquet file: 47483\n"
     ]
    }
   ],
   "source": [
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Count the number of records in the Parquet file\n",
    "record_count = con.sql(f'''\n",
    "    SELECT COUNT(*)\n",
    "    FROM read_parquet('{master_file}')\n",
    "''').fetchone()[0]\n",
    "\n",
    "# Print the record count\n",
    "print(f\"Number of records in the Parquet file: {record_count}\")\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Random Titles in the Parquet file:\n",
      "The effect of individualized NUTritional counseling on muscle mass and treatment outcome in patients with metastatic COLOrectal cancer undergoing chemotherapy: a randomized controlled trial protocol\n",
      "Genade zonder afzender\n",
      "Italian external quality assessment program for cystic fibrosis sweat chloride test: a 2015 and 2016 results comparison.\n",
      "Rethinking Subthreshold Effects in Regulatory Chemical Risk Assessments\n",
      "TMEM16A (ANO1) as a therapeutic target in cystic fibrosis\n",
      "Socially Responsible Resistance towards Consumption: Theoretical Legitimation and Implications on Marketing Practices\n",
      "Conducteur geslagen: wegkijken of helpen?\n",
      "Significados da utilização de plantas medicinais nas práticas de autoatenção à saúde\n",
      "Achievements and Challenges in Sedimentary Basin Dynamics: A Review\n",
      "VARIABILITY IN SOIL FOOD WEB STRUCTURE ACROSS TIME AND SPACE\n",
      "Number of unique titles in the Parquet file: 47312\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Query all titles from the Parquet file\n",
    "titles = con.sql(f'''\n",
    "    SELECT mainTitle\n",
    "    FROM read_parquet('{master_file}')\n",
    "''').fetchall()\n",
    "\n",
    "# Select 10 random titles\n",
    "random_titles = random.sample([title[0] for title in titles if title[0]], min(10, len(titles)))\n",
    "\n",
    "print(\"10 Random Titles in the Parquet file:\")\n",
    "for title in random_titles:\n",
    "    print(title)\n",
    "\n",
    "# print the number of unique titles\n",
    "unique_titles = set(title[0] for title in titles if title[0])\n",
    "print(f\"Number of unique titles in the Parquet file: {len(unique_titles)}\")\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Random DOIs:\n",
      "10.1007/bf00263291\n",
      "10.1155/2014/809741\n",
      "10.1016/j.pce.2011.06.007\n",
      "10.1088/0004-637x/769/2/151\n",
      "10.1016/s1002-0160(18)60022-0\n",
      "10.17863/cam.81832\n",
      "10.1111/imm.12335\n",
      "10.1016/j.mex.2023.102239\n",
      "10.1111/jocd.12540\n",
      "10.1016/s1040-8428(99)00052-9\n",
      "Total number of DOIs: 34925\n",
      "Number of unique DOIs: 34925\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "# Query to extract DOIs from the pids column\n",
    "dois = con.sql(f'''\n",
    "    SELECT unnest.value AS doi\n",
    "    FROM read_parquet('{master_file}')\n",
    "    CROSS JOIN UNNEST(pids) AS unnest\n",
    "    WHERE unnest.scheme = 'doi'\n",
    "''').fetchall()\n",
    "\n",
    "# Select 10 random DOIs\n",
    "random_dois = random.sample([doi[0] for doi in dois if doi[0]], min(10, len(dois)))\n",
    "\n",
    "# Print the 10 random DOIs\n",
    "print(\"10 Random DOIs:\")\n",
    "for doi in random_dois:\n",
    "    print(doi)\n",
    "\n",
    "# print total number of DOIs\n",
    "print(f\"Total number of DOIs: {len(dois)}\")\n",
    "\n",
    "# print the number of unique DOIs\n",
    "unique_dois = set(doi[0] for doi in dois if doi[0])\n",
    "print(f\"Number of unique DOIs: {len(unique_dois)}\")\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct PID schemes in the master table:\n",
      "mag_id\n",
      "doi\n",
      "arXiv\n",
      "handle\n",
      "pmc\n",
      "pmid\n"
     ]
    }
   ],
   "source": [
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Query to extract distinct PID schemes from the master file\n",
    "pid_schemes = con.sql(f'''\n",
    "    SELECT DISTINCT unnest.scheme AS scheme\n",
    "    FROM read_parquet('{master_file}')\n",
    "    CROSS JOIN UNNEST(pids) AS unnest\n",
    "''').fetchall()\n",
    "\n",
    "# Print the distinct PID schemes\n",
    "print(\"Distinct PID schemes in the master table:\")\n",
    "for scheme in pid_schemes:\n",
    "    print(scheme[0])\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIDs grouped by schemes:\n",
      "   scheme                                               pids\n",
      "0  handle  [11588/164365, 11591/208934, 11588/633072, 115...\n",
      "1     pmc  [PMC10478064, PMC8750824, PMC8970603, PMC10442...\n",
      "2    pmid  [35349665, 38001043, 36322395, 37452799, 36174...\n",
      "3     doi  [10.4337/9781848445987.00016, 10.6093/unina/fe...\n",
      "4  mag_id  [2907305326, 2014694079, 1974512246, 232785792...\n",
      "5   arXiv  [http://arxiv.org/abs/2112.14427, http://arxiv...\n"
     ]
    }
   ],
   "source": [
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Query to extract PIDs grouped by their schemes\n",
    "pids_by_scheme = con.sql(f'''\n",
    "    SELECT unnest.scheme AS scheme, LIST(unnest.value) AS pids\n",
    "    FROM read_parquet('{master_file}')\n",
    "    CROSS JOIN UNNEST(pids) AS unnest\n",
    "    GROUP BY unnest.scheme\n",
    "''').fetchall()\n",
    "\n",
    "# dataframe to hold the PIDs grouped by their schemes\n",
    "df_pids_by_scheme = pd.DataFrame(pids_by_scheme, columns=['scheme', 'pids'])\n",
    "# Print the DataFrame of PIDs grouped by their schemes\n",
    "print(\"PIDs grouped by schemes:\")  \n",
    "print(df_pids_by_scheme)\n",
    " \n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geting ready for further processing the master data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create a folder for processed data\n",
    "processing_folder_path = f\"./data/{publication_date}/04_processed/{folder_name}\"\n",
    "\n",
    "# Ensure the target directory exists\n",
    "os.makedirs(processing_folder_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Get the DOI's and other identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined id and pid:\n",
      "                                             record_id pid_scheme  \\\n",
      "0       dedup_wf_002::00a855101519be433c2a40000190d007     handle   \n",
      "1       dedup_wf_002::00ec3f4e5f2f9b99df9f87875ab9a9e2     handle   \n",
      "2       dedup_wf_002::00fab457e86aa3257212d739a7f83995     handle   \n",
      "3       dedup_wf_002::014e7a41f66c80fbb2908c9477856715     handle   \n",
      "4       dedup_wf_002::018d3a6f3d7674ba51cd90707cc74652     handle   \n",
      "...                                                ...        ...   \n",
      "123152  pmid_dedup__::281ac6f71b7a14a3b743d46e2cf9cb61       pmid   \n",
      "123153  pmid_dedup__::281ac6f71b7a14a3b743d46e2cf9cb61        pmc   \n",
      "123154  pmid_dedup__::281ac6f71b7a14a3b743d46e2cf9cb61       pmid   \n",
      "123155  pmid_dedup__::281ac6f71b7a14a3b743d46e2cf9cb61        pmc   \n",
      "123156  pmid_dedup__::281ac6f71b7a14a3b743d46e2cf9cb61       pmid   \n",
      "\n",
      "           pid_value                                    combined_id_pid  \n",
      "0       11588/164365  dedup_wf_002::00a855101519be433c2a40000190d007...  \n",
      "1       11591/208934  dedup_wf_002::00ec3f4e5f2f9b99df9f87875ab9a9e2...  \n",
      "2       11588/633072  dedup_wf_002::00fab457e86aa3257212d739a7f83995...  \n",
      "3       11588/113575  dedup_wf_002::014e7a41f66c80fbb2908c9477856715...  \n",
      "4       11588/709565  dedup_wf_002::018d3a6f3d7674ba51cd90707cc74652...  \n",
      "...              ...                                                ...  \n",
      "123152      28752830  pmid_dedup__::281ac6f71b7a14a3b743d46e2cf9cb61...  \n",
      "123153    PMC6357590  pmid_dedup__::281ac6f71b7a14a3b743d46e2cf9cb61...  \n",
      "123154      28752835  pmid_dedup__::281ac6f71b7a14a3b743d46e2cf9cb61...  \n",
      "123155    PMC6357591  pmid_dedup__::281ac6f71b7a14a3b743d46e2cf9cb61...  \n",
      "123156      28752829  pmid_dedup__::281ac6f71b7a14a3b743d46e2cf9cb61...  \n",
      "\n",
      "[123157 rows x 4 columns]\n",
      "Combined data saved to: ./data/2025-02-19/04_processed/aurora/aurora-combined-id-pid.parquet\n",
      "File size: 9.68 MB\n"
     ]
    }
   ],
   "source": [
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Query to combine the id with each pid\n",
    "combined_data = con.sql(f'''\n",
    "    SELECT \n",
    "        id AS record_id,\n",
    "        unnest.scheme AS pid_scheme,\n",
    "        unnest.value AS pid_value,\n",
    "        CONCAT(id, '_', unnest.value) AS combined_id_pid\n",
    "    FROM read_parquet('{master_file}')\n",
    "    CROSS JOIN UNNEST(pids) AS unnest\n",
    "''').fetchdf()\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(\"Combined id and pid:\")\n",
    "print(combined_data)\n",
    "\n",
    "\n",
    "# Save the combined data to a new Parquet file for later use\n",
    "combined_file_path = f\"{processing_folder_path}/{folder_name}-combined-id-pid.parquet\"\n",
    "combined_data.to_parquet(combined_file_path, index=False)\n",
    "print(f\"Combined data saved to: {combined_file_path}\")\n",
    "print(f\"File size: {os.path.getsize(combined_file_path) / (1024**2):.2f} MB\")\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Get Altmetric data\n",
    "\n",
    "a. use the PIDS (df_pids_by_scheme) along with the record id (to be used as primary keys, connecting the tables later on), \n",
    "\n",
    "b. get mention data by parsing the pids over the altmetric API,\n",
    "\n",
    "c. save the outcomes in a separate parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import shutil\n",
    "import tempfile\n",
    "import requests\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Supported PID schemes (from script 1)\n",
    "supported_schemes = [\n",
    "    'dimensions_publication_id',\n",
    "    'doi', 'pmid', 'handle', 'arxiv', 'ads',\n",
    "    'ssrn', 'repec', 'isbn', 'id',\n",
    "    'nct_id', 'urn'\n",
    "]\n",
    "\n",
    "# Map PID schemes to Altmetric endpoints (from script 2)\n",
    "endpoint_map = {\n",
    "    'doi': 'doi',\n",
    "    'handle': 'handle',\n",
    "    'pmid': 'pmid',\n",
    "    'arxiv': 'arxiv',\n",
    "    'ads': 'ads',\n",
    "    'ssrn': 'ssrn',\n",
    "    'repec': 'repec',\n",
    "    'isbn': 'isbn',\n",
    "    'id': 'id',\n",
    "    'nct_id': 'nct_id',\n",
    "    'urn': 'urn',\n",
    "    'uri': 'uri'\n",
    "}\n",
    "\n",
    "def estimate_enrichment_time(n_items, rate_per_minute):\n",
    "    secs_per = 60 / rate_per_minute\n",
    "    total_secs = secs_per * n_items\n",
    "    print(f\"Estimated time for {n_items} items at {rate_per_minute}/min: {total_secs / 60:.1f} minutes\")\n",
    "\n",
    "def atomic_write_json(data, path):\n",
    "    \"\"\"Write JSON atomically to avoid corruption if interrupted.\"\"\"\n",
    "    dirpath = os.path.dirname(path)\n",
    "    with tempfile.NamedTemporaryFile('w', delete=False, dir=dirpath) as tf:\n",
    "        json.dump(data, tf)\n",
    "        tempname = tf.name\n",
    "    shutil.move(tempname, path)\n",
    "\n",
    "def fetch_altmetric_data(combined_file_path,\n",
    "                         processing_folder_path,\n",
    "                         folder_name,\n",
    "                         batch_size=100,\n",
    "                         sleep_sec=1.0):\n",
    "    \"\"\"\n",
    "    Hybrid function:\n",
    "    - Loads combined parquet (with record_id, pid_scheme, pid_value).\n",
    "    - Filters/sanitizes inputs (script 1).\n",
    "    - Uses checkpointing & periodic saving (script 2).\n",
    "    - Produces parquet + SQL schema.\n",
    "    \"\"\"\n",
    "\n",
    "    # Folders\n",
    "    extracted_folder = os.path.join(processing_folder_path, folder_name, \"03-altmetric-extracted\")\n",
    "    transformed_folder = os.path.join(processing_folder_path, folder_name, \"04-altmetric-transformed\")\n",
    "    os.makedirs(extracted_folder, exist_ok=True)\n",
    "    os.makedirs(transformed_folder, exist_ok=True)\n",
    "\n",
    "    json_path = os.path.join(extracted_folder, \"altmetric_results.json\")\n",
    "    parquet_path = os.path.join(transformed_folder, \"altmetric_results.parquet\")\n",
    "    schema_path = os.path.join(transformed_folder, \"schema-altmetric.sql\")\n",
    "\n",
    "    # Load combined data\n",
    "    df = pd.read_parquet(combined_file_path)\n",
    "    df = df[df['pid_value'].notna() & (df['pid_value'] != '')]\n",
    "    df['pid_scheme'] = df['pid_scheme'].str.lower()\n",
    "    df['pid_value'] = df['pid_value'].str.lower()\n",
    "    df = df[df['pid_scheme'].isin(supported_schemes)]\n",
    "    print(f\"Number of records to be processed: {len(df)}\")\n",
    "\n",
    "    # Resume checkpoint\n",
    "    if os.path.exists(json_path):\n",
    "        print(f\"Resuming from saved JSON: {json_path}\")\n",
    "        with open(json_path, 'r') as f:\n",
    "            results = json.load(f)\n",
    "    else:\n",
    "        results = []\n",
    "\n",
    "    processed_keys = {(r.get('scheme'), r.get('value')) for r in results}\n",
    "    df_to_process = df[~df.apply(lambda row: (row['pid_scheme'], row['pid_value']) in processed_keys, axis=1)]\n",
    "    df_to_process = df_to_process.reset_index(drop=True)\n",
    "\n",
    "    total = len(df_to_process)\n",
    "    print(f\"Remaining to process: {total}\")\n",
    "    if total > 0:\n",
    "        estimate_enrichment_time(total, rate_per_minute=(60 / sleep_sec))\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i, row in enumerate(tqdm(df_to_process.itertuples(index=False), total=total, desc=\"Fetching Altmetric data\")):\n",
    "        record_id = row.record_id\n",
    "        scheme = row.pid_scheme\n",
    "        value = row.pid_value\n",
    "\n",
    "        if scheme in endpoint_map:\n",
    "            endpoint = endpoint_map[scheme]\n",
    "            url = f\"https://api.altmetric.com/v1/{endpoint}/{value}\"\n",
    "            print(f\"\\nRequesting Altmetric data for {scheme}:{value} → {url}\")\n",
    "\n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    print(f\"  ✔ 200 OK: Data received.\")\n",
    "                    if isinstance(data, dict):\n",
    "                        altmetric_score = data.get('score')\n",
    "                        if altmetric_score is not None:\n",
    "                            print(f\"    Altmetric score: {altmetric_score}\")\n",
    "                        else:\n",
    "                            print(\"    Altmetric score not found.\")\n",
    "                    data['record_id'] = record_id\n",
    "                    data['scheme'] = scheme\n",
    "                    data['value'] = value\n",
    "                    results.append(data)\n",
    "\n",
    "                elif response.status_code == 403:\n",
    "                    print(\"403 Forbidden: Not authorized (API key may be required).\")\n",
    "                elif response.status_code == 404:\n",
    "                    print(\"404 Not Found: No Altmetric details available.\")\n",
    "                elif response.status_code == 429:\n",
    "                    print(\"429 Too Many Requests: You are being rate limited.\")\n",
    "                elif response.status_code == 502:\n",
    "                    print(\"502 Bad Gateway: Altmetric API maintenance.\")\n",
    "                else:\n",
    "                    print(f\"Error {response.status_code} for {scheme}:{value}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Exception for {scheme}:{value}: {e}\")\n",
    "\n",
    "            time.sleep(sleep_sec)\n",
    "\n",
    "        # Save every batch_size or at end\n",
    "        if (i + 1) % batch_size == 0 or (i + 1) == total:\n",
    "            atomic_write_json(results, json_path)\n",
    "\n",
    "            altmetric_df = pd.json_normalize(results)\n",
    "            altmetric_df = altmetric_df.astype(str)\n",
    "            altmetric_df.to_parquet(parquet_path, index=False)\n",
    "\n",
    "            elapsed = time.time() - start_time\n",
    "            completed = i + 1\n",
    "            remaining = total - completed\n",
    "            avg_time_per = elapsed / completed if completed else 0\n",
    "            eta_sec = avg_time_per * remaining\n",
    "            eta_str = time.strftime(\"%H:%M:%S\", time.gmtime(eta_sec))\n",
    "            print(f\"\\n💾 Saved at {completed} items | ETA remaining: {eta_str}\")\n",
    "\n",
    "    print(f\"\\nAltmetric enrichment completed. Total records: {len(results)}\")\n",
    "\n",
    "    # Extract SQL schema\n",
    "    con = duckdb.connect()\n",
    "    con.execute(f\"DESCRIBE SELECT * FROM parquet_scan('{parquet_path}')\")\n",
    "    schema_df = con.fetchdf()\n",
    "    with open(schema_path, \"w\") as f:\n",
    "        for _, row in schema_df.iterrows():\n",
    "            f.write(f\"{row['column_name']} {row['column_type']},\\n\")\n",
    "    con.close()\n",
    "    print(f\"SQL schema written to: {schema_path}\")\n",
    "\n",
    "    return pd.json_normalize(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records to be processed: 79\n",
      "Remaining to process: 79\n",
      "Estimated time for 79 items at 60.0/min: 1.3 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:   0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for doi:10.1002/9780470725207.ch6 → https://api.altmetric.com/v1/doi/10.1002/9780470725207.ch6\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:   1%|▏         | 1/79 [00:01<01:25,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for handle:20.500.11755/50623997-3854-4efe-879c-62a8a13fef72 → https://api.altmetric.com/v1/handle/20.500.11755/50623997-3854-4efe-879c-62a8a13fef72\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:   3%|▎         | 2/79 [00:02<01:27,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for doi:10.1023/a:1014403119794 → https://api.altmetric.com/v1/doi/10.1023/a:1014403119794\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:   4%|▍         | 3/79 [00:03<01:25,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for handle:11588/133016 → https://api.altmetric.com/v1/handle/11588/133016\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:   5%|▌         | 4/79 [00:04<01:23,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for handle:11588/489596 → https://api.altmetric.com/v1/handle/11588/489596\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:   6%|▋         | 5/79 [00:05<01:22,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for handle:11588/483779 → https://api.altmetric.com/v1/handle/11588/483779\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:   8%|▊         | 6/79 [00:06<01:20,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for doi:10.1023/a:1006090807678 → https://api.altmetric.com/v1/doi/10.1023/a:1006090807678\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:   9%|▉         | 7/79 [00:07<01:19,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for handle:11588/338523 → https://api.altmetric.com/v1/handle/11588/338523\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  10%|█         | 8/79 [00:08<01:18,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for doi:10.22037/uj.v0i0.5633 → https://api.altmetric.com/v1/doi/10.22037/uj.v0i0.5633\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  11%|█▏        | 9/79 [00:09<01:17,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for handle:11571/135571 → https://api.altmetric.com/v1/handle/11571/135571\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  13%|█▎        | 10/79 [00:11<01:16,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for handle:20.500.11769/23610 → https://api.altmetric.com/v1/handle/20.500.11769/23610\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  14%|█▍        | 11/79 [00:12<01:14,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for handle:11588/165668 → https://api.altmetric.com/v1/handle/11588/165668\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  15%|█▌        | 12/79 [00:13<01:13,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for doi:10.23812/21-86-l → https://api.altmetric.com/v1/doi/10.23812/21-86-l\n",
      "  ✔ 200 OK: Data received.\n",
      "    Altmetric score: 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  16%|█▋        | 13/79 [00:14<01:12,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for handle:11379/33267 → https://api.altmetric.com/v1/handle/11379/33267\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  18%|█▊        | 14/79 [00:15<01:11,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for pmid:21172342 → https://api.altmetric.com/v1/pmid/21172342\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  19%|█▉        | 15/79 [00:16<01:10,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for handle:11588/364415 → https://api.altmetric.com/v1/handle/11588/364415\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  20%|██        | 16/79 [00:17<01:09,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for handle:11591/235527 → https://api.altmetric.com/v1/handle/11591/235527\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  22%|██▏       | 17/79 [00:18<01:08,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for handle:11591/365500 → https://api.altmetric.com/v1/handle/11591/365500\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  23%|██▎       | 18/79 [00:19<01:07,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for pmid:31461927 → https://api.altmetric.com/v1/pmid/31461927\n",
      "  ✔ 200 OK: Data received.\n",
      "    Altmetric score: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  24%|██▍       | 19/79 [00:20<01:06,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for pmid:6603090 → https://api.altmetric.com/v1/pmid/6603090\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  25%|██▌       | 20/79 [00:22<01:05,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for pmid:25277619 → https://api.altmetric.com/v1/pmid/25277619\n",
      "  ✔ 200 OK: Data received.\n",
      "    Altmetric score: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  27%|██▋       | 21/79 [00:23<01:03,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for pmid:2291015 → https://api.altmetric.com/v1/pmid/2291015\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  28%|██▊       | 22/79 [00:24<01:05,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for handle:2108/293046 → https://api.altmetric.com/v1/handle/2108/293046\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  29%|██▉       | 23/79 [00:25<01:03,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for handle:11588/135169 → https://api.altmetric.com/v1/handle/11588/135169\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  30%|███       | 24/79 [00:26<01:02,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for handle:1871.1/b4bee7e0-d8f1-4d53-8bdd-e1a28b1e0b6d → https://api.altmetric.com/v1/handle/1871.1/b4bee7e0-d8f1-4d53-8bdd-e1a28b1e0b6d\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  32%|███▏      | 25/79 [00:27<01:01,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for handle:11591/227814 → https://api.altmetric.com/v1/handle/11591/227814\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  33%|███▎      | 26/79 [00:28<00:59,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for handle:11381/2833704 → https://api.altmetric.com/v1/handle/11381/2833704\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  34%|███▍      | 27/79 [00:30<00:58,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for handle:11588/356484 → https://api.altmetric.com/v1/handle/11588/356484\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  35%|███▌      | 28/79 [00:31<00:56,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for pmid:6103698 → https://api.altmetric.com/v1/pmid/6103698\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  37%|███▋      | 29/79 [00:32<00:55,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for handle:1871.1/ec3d7c98-0e74-4819-9609-4d923c6dd5a9 → https://api.altmetric.com/v1/handle/1871.1/ec3d7c98-0e74-4819-9609-4d923c6dd5a9\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  38%|███▊      | 30/79 [00:33<00:54,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for pmid:7938749 → https://api.altmetric.com/v1/pmid/7938749\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  39%|███▉      | 31/79 [00:34<00:53,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for pmid:6939959 → https://api.altmetric.com/v1/pmid/6939959\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  41%|████      | 32/79 [00:35<00:53,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for pmid:160180 → https://api.altmetric.com/v1/pmid/160180\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  42%|████▏     | 33/79 [00:36<00:51,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for pmid:7857601 → https://api.altmetric.com/v1/pmid/7857601\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  43%|████▎     | 34/79 [00:37<00:50,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for handle:11591/231669 → https://api.altmetric.com/v1/handle/11591/231669\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  44%|████▍     | 35/79 [00:38<00:48,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for handle:11591/217650 → https://api.altmetric.com/v1/handle/11591/217650\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  46%|████▌     | 36/79 [00:40<00:47,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for pmid:10726233 → https://api.altmetric.com/v1/pmid/10726233\n",
      "  ✔ 200 OK: Data received.\n",
      "    Altmetric score: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  47%|████▋     | 37/79 [00:41<00:46,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for handle:11588/421561 → https://api.altmetric.com/v1/handle/11588/421561\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  48%|████▊     | 38/79 [00:42<00:45,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for pmid:19935540 → https://api.altmetric.com/v1/pmid/19935540\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  49%|████▉     | 39/79 [00:43<00:44,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for handle:11588/460489 → https://api.altmetric.com/v1/handle/11588/460489\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  51%|█████     | 40/79 [00:44<00:43,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for handle:11386/4582067 → https://api.altmetric.com/v1/handle/11386/4582067\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  52%|█████▏    | 41/79 [00:45<00:41,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for pmid:24334780 → https://api.altmetric.com/v1/pmid/24334780\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  53%|█████▎    | 42/79 [00:46<00:42,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for handle:20.500.11768/149682 → https://api.altmetric.com/v1/handle/20.500.11768/149682\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  54%|█████▍    | 43/79 [00:47<00:40,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for pmid:12063997 → https://api.altmetric.com/v1/pmid/12063997\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  56%|█████▌    | 44/79 [00:49<00:39,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for pmid:23515036 → https://api.altmetric.com/v1/pmid/23515036\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  57%|█████▋    | 45/79 [00:50<00:38,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for handle:11588/863623 → https://api.altmetric.com/v1/handle/11588/863623\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  58%|█████▊    | 46/79 [00:51<00:36,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for handle:11584/219628 → https://api.altmetric.com/v1/handle/11584/219628\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  59%|█████▉    | 47/79 [00:52<00:35,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for pmid:19845110 → https://api.altmetric.com/v1/pmid/19845110\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  61%|██████    | 48/79 [00:53<00:34,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for pmid:18074632 → https://api.altmetric.com/v1/pmid/18074632\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  62%|██████▏   | 49/79 [00:54<00:33,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for pmid:10541474 → https://api.altmetric.com/v1/pmid/10541474\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  63%|██████▎   | 50/79 [00:55<00:32,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for pmid:11998888 → https://api.altmetric.com/v1/pmid/11998888\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  65%|██████▍   | 51/79 [00:56<00:30,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for pmid:10066098 → https://api.altmetric.com/v1/pmid/10066098\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  66%|██████▌   | 52/79 [00:57<00:29,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for pmid:31587252 → https://api.altmetric.com/v1/pmid/31587252\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  67%|██████▋   | 53/79 [00:58<00:28,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for handle:11562/346885 → https://api.altmetric.com/v1/handle/11562/346885\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  68%|██████▊   | 54/79 [01:00<00:27,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for handle:11588/476041 → https://api.altmetric.com/v1/handle/11588/476041\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  70%|██████▉   | 55/79 [01:01<00:26,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for handle:11588/867678 → https://api.altmetric.com/v1/handle/11588/867678\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  71%|███████   | 56/79 [01:02<00:25,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for pmid:19771745 → https://api.altmetric.com/v1/pmid/19771745\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  72%|███████▏  | 57/79 [01:03<00:24,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for pmid:8532376 → https://api.altmetric.com/v1/pmid/8532376\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  73%|███████▎  | 58/79 [01:04<00:23,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for handle:11588/586221 → https://api.altmetric.com/v1/handle/11588/586221\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  75%|███████▍  | 59/79 [01:05<00:22,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for doi:10.26355/eurrev_202011_23627 → https://api.altmetric.com/v1/doi/10.26355/eurrev_202011_23627\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  76%|███████▌  | 60/79 [01:06<00:20,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for pmid:22610121 → https://api.altmetric.com/v1/pmid/22610121\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  77%|███████▋  | 61/79 [01:07<00:19,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for handle:11380/1142834 → https://api.altmetric.com/v1/handle/11380/1142834\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  78%|███████▊  | 62/79 [01:08<00:18,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for handle:11588/167247 → https://api.altmetric.com/v1/handle/11588/167247\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  80%|███████▉  | 63/79 [01:09<00:17,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for pmid:9177614 → https://api.altmetric.com/v1/pmid/9177614\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  81%|████████  | 64/79 [01:11<00:16,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for pmid:34121372 → https://api.altmetric.com/v1/pmid/34121372\n",
      "  ✔ 200 OK: Data received.\n",
      "    Altmetric score: 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  82%|████████▏ | 65/79 [01:12<00:15,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for pmid:24817301 → https://api.altmetric.com/v1/pmid/24817301\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  84%|████████▎ | 66/79 [01:13<00:14,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for handle:11588/169412 → https://api.altmetric.com/v1/handle/11588/169412\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  85%|████████▍ | 67/79 [01:14<00:13,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for handle:11588/949490 → https://api.altmetric.com/v1/handle/11588/949490\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  86%|████████▌ | 68/79 [01:15<00:12,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for pmid:12796363 → https://api.altmetric.com/v1/pmid/12796363\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  87%|████████▋ | 69/79 [01:16<00:10,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for pmid:33215456 → https://api.altmetric.com/v1/pmid/33215456\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  89%|████████▊ | 70/79 [01:17<00:09,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for doi:10.23750/abm.v88i3 → https://api.altmetric.com/v1/doi/10.23750/abm.v88i3\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  90%|████████▉ | 71/79 [01:18<00:08,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for handle:11588/686114 → https://api.altmetric.com/v1/handle/11588/686114\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  91%|█████████ | 72/79 [01:19<00:07,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for pmid:28752832 → https://api.altmetric.com/v1/pmid/28752832\n",
      "  ✔ 200 OK: Data received.\n",
      "    Altmetric score: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  92%|█████████▏| 73/79 [01:20<00:06,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for pmid:28752831 → https://api.altmetric.com/v1/pmid/28752831\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  94%|█████████▎| 74/79 [01:22<00:05,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for pmid:28752828 → https://api.altmetric.com/v1/pmid/28752828\n",
      "  ✔ 200 OK: Data received.\n",
      "    Altmetric score: 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  95%|█████████▍| 75/79 [01:23<00:04,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for pmid:28752827 → https://api.altmetric.com/v1/pmid/28752827\n",
      "  ✔ 200 OK: Data received.\n",
      "    Altmetric score: 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  96%|█████████▌| 76/79 [01:24<00:03,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for pmid:28752830 → https://api.altmetric.com/v1/pmid/28752830\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  97%|█████████▋| 77/79 [01:25<00:02,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for pmid:28752835 → https://api.altmetric.com/v1/pmid/28752835\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data:  99%|█████████▊| 78/79 [01:26<00:01,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting Altmetric data for pmid:28752829 → https://api.altmetric.com/v1/pmid/28752829\n",
      "404 Not Found: No Altmetric details available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Altmetric data: 100%|██████████| 79/79 [01:27<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 Saved at 79 items | ETA remaining: 00:00:00\n",
      "\n",
      "Altmetric enrichment completed. Total records: 8\n",
      "SQL schema written to: ./data/2025-02-19/05_altmetric/aurora_altmetric_20250919_102721/04-altmetric-transformed/schema-altmetric.sql\n",
      "Altmetric enrichment completed. Results stored under: ./data/2025-02-19/05_altmetric/aurora_altmetric_20250919_102721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Path to the combined PID parquet\n",
    "combined_file_path = (\n",
    "    f\"./data/{publication_date}/04_processed/{folder_name}/{folder_name}-combined-id-pid.parquet\"\n",
    ")\n",
    "\n",
    "# Altmetric output folder (05_altmetric)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "altmetric_folder_name = f\"{folder_name}_altmetric_{timestamp}\"\n",
    "processing_folder_path = f\"./data/{publication_date}/05_altmetric\"\n",
    "os.makedirs(processing_folder_path, exist_ok=True)\n",
    "\n",
    "if testing_mode:\n",
    "    df = pd.read_parquet(combined_file_path).tail(100)\n",
    "    test_path = os.path.join(processing_folder_path, \"test_combined.parquet\")\n",
    "    df.to_parquet(test_path, index=False)\n",
    "    results_df = fetch_altmetric_data(\n",
    "        test_path,\n",
    "        processing_folder_path,\n",
    "        altmetric_folder_name\n",
    "    )\n",
    "else:\n",
    "    results_df = fetch_altmetric_data(\n",
    "        combined_file_path,\n",
    "        processing_folder_path,\n",
    "        altmetric_folder_name\n",
    "    )\n",
    "\n",
    "print(f\"Altmetric enrichment completed. Results stored under: {processing_folder_path}/{altmetric_folder_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Get Overton data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waiting for the Overton dump to be available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Get SDG classification labels\n",
    "\n",
    "a. Query the abstracts first along with the id (to be used as primary keys, connecting the tables later on), \n",
    "\n",
    "b. get sdg data by parsing the abstracts with more than 100 tokens over an LLM API with system prompt, \n",
    "\n",
    "c. save the outcomes in a separate parquet file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### step 7a: Get the abstracts, including the record id and the number of tokens i nthe abstract\n",
    "\n",
    "Number of tokens are important later on, less then 100 tokens in the abstract deliver low quality SDG classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptions with token counts:\n",
      "                                            record_id  \\\n",
      "0      dedup_wf_002::00fa9a6806de20c970d77677b43220be   \n",
      "1      dedup_wf_002::022b5a9d85b0c38cffef0fb145b3645a   \n",
      "2      dedup_wf_002::04463b5f4ad9cc6e1863d76407a3e424   \n",
      "3      dedup_wf_002::053440b6265dfce1da1a31deb1a6436b   \n",
      "4      dedup_wf_002::062128904997f80c9ca88cec1e30d482   \n",
      "...                                               ...   \n",
      "31403  unidue___bib::f055fc26e50fb169b575160b700d473a   \n",
      "31404  unidue___bib::f10f6cd926f9969659108b66fecd04f4   \n",
      "31405  unidue___bib::f5be71256cb2f521cf37c442a38ea64e   \n",
      "31406  unidue___bib::f688bcf16fc9154255868f8143a02275   \n",
      "31407  unidue___bib::fb7a759f2e12b8c6cb2f7cfc98cb4ff2   \n",
      "\n",
      "                                             description  token_count  \n",
      "0      Dans quelle mesure l'accompagnement par un men...          153  \n",
      "1      Etant membre de la communauté internationale, ...          197  \n",
      "2      This paper examines the small but growing lite...           89  \n",
      "3      This paper gives a review of operational multi...           74  \n",
      "4      Può la messa in scena sulla pagina di una teor...          242  \n",
      "...                                                  ...          ...  \n",
      "31403     Dissertation, Universität Duisburg-Essen, 2017            4  \n",
      "31404                              [Online-Videobeitrag]            1  \n",
      "31405                                      Habiblitation            1  \n",
      "31406                Duisburg, Essen, Univ., Diss., 2005            5  \n",
      "31407         Zugl.: Duisburg, Essen, Univ., Diss., 2013            6  \n",
      "\n",
      "[31408 rows x 3 columns]\n",
      "Description data saved to: ./data/2025-02-19/04_processed/aurora/aurora-descriptions-with-tokens.parquet\n",
      "File size: 23.23 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "# Query to extract the ID, description, remove XML tags, and calculate the number of tokens in the description\n",
    "description_data = con.sql(f'''\n",
    "    SELECT \n",
    "        id AS record_id,\n",
    "        regexp_replace(descriptions[1], '<[^>]+>', '') AS description,  -- Remove XML tags\n",
    "        array_length(split(regexp_replace(descriptions[1], '<[^>]+>', ''), ' ')) AS token_count\n",
    "    FROM read_parquet('{master_file}')\n",
    "    WHERE descriptions IS NOT NULL AND array_length(descriptions) > 0\n",
    "''').fetchdf()\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(\"Descriptions with token counts:\")\n",
    "print(description_data)\n",
    "\n",
    "# Save the data to a new Parquet file for later use\n",
    "description_file_path = f\"{processing_folder_path}/{folder_name}-descriptions-with-tokens.parquet\"\n",
    "description_data.to_parquet(description_file_path, index=False)\n",
    "print(f\"Description data saved to: {description_file_path}\")\n",
    "print(f\"File size: {os.path.getsize(description_file_path) / (1024**2):.2f} MB\")\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 7b-1:  Aurora SDG Classifier\n",
    "In this step we use the Aurora SDG classifier to classify all the abstracts.\n",
    "\n",
    "First we set a test_mode parameter, so that the first 3 abstracts with more than 100 tokens are used. If testing mode is False, then use all abstracts with more than 100 tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed record_id: dedup_wf_002::00fa9a6806de20c970d77677b43220be, SDGs: [{'goal_code': '1', 'goal_name': 'No poverty', 'prediction_score': 0.179525405}, {'goal_code': '2', 'goal_name': 'Zero hunger', 'prediction_score': 0.0224823952}, {'goal_code': '3', 'goal_name': 'Good health and well-being', 'prediction_score': 0.0757595897}, {'goal_code': '4', 'goal_name': 'Quality Education', 'prediction_score': 0.693393469}, {'goal_code': '5', 'goal_name': 'Gender equality', 'prediction_score': 0.740161419}, {'goal_code': '6', 'goal_name': 'Clean water and sanitation', 'prediction_score': 0.0137088597}, {'goal_code': '7', 'goal_name': 'Affordable and clean energy', 'prediction_score': 0.0231188238}, {'goal_code': '8', 'goal_name': 'Decent work and economic growth', 'prediction_score': 0.934973717}, {'goal_code': '9', 'goal_name': 'Industry, innovation and infrastructure', 'prediction_score': 0.75080055}, {'goal_code': '10', 'goal_name': 'Reduced inequalities', 'prediction_score': 0.670219898}, {'goal_code': '11', 'goal_name': 'Sustainable cities and communities', 'prediction_score': 0.182557195}, {'goal_code': '12', 'goal_name': 'Responsible consumption and production', 'prediction_score': 0.13102439}, {'goal_code': '13', 'goal_name': 'Climate action', 'prediction_score': 0.0251685381}, {'goal_code': '14', 'goal_name': 'Life below water', 'prediction_score': 0.0057580471}, {'goal_code': '15', 'goal_name': 'Life in Land', 'prediction_score': 0.656423151}, {'goal_code': '16', 'goal_name': 'Peace, Justice and strong institutions', 'prediction_score': 0.532813668}, {'goal_code': '17', 'goal_name': 'Partnerships for the goals', 'prediction_score': 0.640333354}]\n",
      "Time taken to process record_id dedup_wf_002::00fa9a6806de20c970d77677b43220be: 0.00 seconds\n",
      "Processed record_id: dedup_wf_002::022b5a9d85b0c38cffef0fb145b3645a, SDGs: [{'goal_code': '1', 'goal_name': 'No poverty', 'prediction_score': 0.330218196}, {'goal_code': '2', 'goal_name': 'Zero hunger', 'prediction_score': 0.115035832}, {'goal_code': '3', 'goal_name': 'Good health and well-being', 'prediction_score': 0.0683116}, {'goal_code': '4', 'goal_name': 'Quality Education', 'prediction_score': 0.0462566614}, {'goal_code': '5', 'goal_name': 'Gender equality', 'prediction_score': 0.18845579}, {'goal_code': '6', 'goal_name': 'Clean water and sanitation', 'prediction_score': 0.0260967016}, {'goal_code': '7', 'goal_name': 'Affordable and clean energy', 'prediction_score': 0.0289147496}, {'goal_code': '8', 'goal_name': 'Decent work and economic growth', 'prediction_score': 0.178879321}, {'goal_code': '9', 'goal_name': 'Industry, innovation and infrastructure', 'prediction_score': 0.388687342}, {'goal_code': '10', 'goal_name': 'Reduced inequalities', 'prediction_score': 0.62443763}, {'goal_code': '11', 'goal_name': 'Sustainable cities and communities', 'prediction_score': 0.301458418}, {'goal_code': '12', 'goal_name': 'Responsible consumption and production', 'prediction_score': 0.162411273}, {'goal_code': '13', 'goal_name': 'Climate action', 'prediction_score': 0.683750868}, {'goal_code': '14', 'goal_name': 'Life below water', 'prediction_score': 0.0139535367}, {'goal_code': '15', 'goal_name': 'Life in Land', 'prediction_score': 0.0511029661}, {'goal_code': '16', 'goal_name': 'Peace, Justice and strong institutions', 'prediction_score': 0.995899379}, {'goal_code': '17', 'goal_name': 'Partnerships for the goals', 'prediction_score': 0.960606039}]\n",
      "Time taken to process record_id dedup_wf_002::022b5a9d85b0c38cffef0fb145b3645a: 0.00 seconds\n",
      "Processed record_id: dedup_wf_002::062128904997f80c9ca88cec1e30d482, SDGs: [{'goal_code': '1', 'goal_name': 'No poverty', 'prediction_score': 0.0636311173}, {'goal_code': '2', 'goal_name': 'Zero hunger', 'prediction_score': 0.0321019}, {'goal_code': '3', 'goal_name': 'Good health and well-being', 'prediction_score': 0.117759615}, {'goal_code': '4', 'goal_name': 'Quality Education', 'prediction_score': 0.806252}, {'goal_code': '5', 'goal_name': 'Gender equality', 'prediction_score': 0.0730051696}, {'goal_code': '6', 'goal_name': 'Clean water and sanitation', 'prediction_score': 0.217483699}, {'goal_code': '7', 'goal_name': 'Affordable and clean energy', 'prediction_score': 0.182683349}, {'goal_code': '8', 'goal_name': 'Decent work and economic growth', 'prediction_score': 0.198991716}, {'goal_code': '9', 'goal_name': 'Industry, innovation and infrastructure', 'prediction_score': 0.183970928}, {'goal_code': '10', 'goal_name': 'Reduced inequalities', 'prediction_score': 0.101113796}, {'goal_code': '11', 'goal_name': 'Sustainable cities and communities', 'prediction_score': 0.223213911}, {'goal_code': '12', 'goal_name': 'Responsible consumption and production', 'prediction_score': 0.175293446}, {'goal_code': '13', 'goal_name': 'Climate action', 'prediction_score': 0.00844502449}, {'goal_code': '14', 'goal_name': 'Life below water', 'prediction_score': 0.0239962637}, {'goal_code': '15', 'goal_name': 'Life in Land', 'prediction_score': 0.168501079}, {'goal_code': '16', 'goal_name': 'Peace, Justice and strong institutions', 'prediction_score': 0.775521398}, {'goal_code': '17', 'goal_name': 'Partnerships for the goals', 'prediction_score': 0.231764972}]\n",
      "Time taken to process record_id dedup_wf_002::062128904997f80c9ca88cec1e30d482: 0.00 seconds\n",
      "Processed record_id: dedup_wf_002::064ad2cbf09799ec4bffa98f74a3fc70, SDGs: [{'goal_code': '1', 'goal_name': 'No poverty', 'prediction_score': 0.00405427814}, {'goal_code': '2', 'goal_name': 'Zero hunger', 'prediction_score': 0.0234039128}, {'goal_code': '3', 'goal_name': 'Good health and well-being', 'prediction_score': 0.576950431}, {'goal_code': '4', 'goal_name': 'Quality Education', 'prediction_score': 0.188819647}, {'goal_code': '5', 'goal_name': 'Gender equality', 'prediction_score': 0.00215274096}, {'goal_code': '6', 'goal_name': 'Clean water and sanitation', 'prediction_score': 0.949908793}, {'goal_code': '7', 'goal_name': 'Affordable and clean energy', 'prediction_score': 0.0565896928}, {'goal_code': '8', 'goal_name': 'Decent work and economic growth', 'prediction_score': 0.0156666934}, {'goal_code': '9', 'goal_name': 'Industry, innovation and infrastructure', 'prediction_score': 0.529509485}, {'goal_code': '10', 'goal_name': 'Reduced inequalities', 'prediction_score': 0.0529684424}, {'goal_code': '11', 'goal_name': 'Sustainable cities and communities', 'prediction_score': 0.958141804}, {'goal_code': '12', 'goal_name': 'Responsible consumption and production', 'prediction_score': 0.260577202}, {'goal_code': '13', 'goal_name': 'Climate action', 'prediction_score': 0.00342011452}, {'goal_code': '14', 'goal_name': 'Life below water', 'prediction_score': 0.0181024373}, {'goal_code': '15', 'goal_name': 'Life in Land', 'prediction_score': 0.616219282}, {'goal_code': '16', 'goal_name': 'Peace, Justice and strong institutions', 'prediction_score': 0.0131295919}, {'goal_code': '17', 'goal_name': 'Partnerships for the goals', 'prediction_score': 0.503517509}]\n",
      "Time taken to process record_id dedup_wf_002::064ad2cbf09799ec4bffa98f74a3fc70: 0.00 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     44\u001b[39m headers = {\u001b[33m'\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mapplication/json\u001b[39m\u001b[33m'\u001b[39m}\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     47\u001b[39m     \u001b[38;5;66;03m# Make the API call\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m     response.raise_for_status()\n\u001b[32m     51\u001b[39m     \u001b[38;5;66;03m# Parse the response\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3/dist-packages/requests/api.py:119\u001b[39m, in \u001b[36mpost\u001b[39m\u001b[34m(url, data, json, **kwargs)\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(url, data=\u001b[38;5;28;01mNone\u001b[39;00m, json=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    108\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[32m    109\u001b[39m \n\u001b[32m    110\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    116\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m    117\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3/dist-packages/requests/api.py:61\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3/dist-packages/requests/sessions.py:544\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    539\u001b[39m send_kwargs = {\n\u001b[32m    540\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m'\u001b[39m: timeout,\n\u001b[32m    541\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m'\u001b[39m: allow_redirects,\n\u001b[32m    542\u001b[39m }\n\u001b[32m    543\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m544\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3/dist-packages/requests/sessions.py:657\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m start = preferred_clock()\n\u001b[32m    656\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    660\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3/dist-packages/requests/adapters.py:439\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    437\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    438\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunked:\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m         resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m            \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m            \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m            \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    452\u001b[39m     \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m    453\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    454\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(conn, \u001b[33m'\u001b[39m\u001b[33mproxy_pool\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py:700\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[39m\n\u001b[32m    697\u001b[39m     \u001b[38;5;28mself\u001b[39m._prepare_proxy(conn)\n\u001b[32m    699\u001b[39m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m700\u001b[39m httplib_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    706\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[32m    711\u001b[39m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[32m    712\u001b[39m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[32m    713\u001b[39m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[32m    714\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py:446\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[39m\n\u001b[32m    441\u001b[39m             httplib_response = conn.getresponse()\n\u001b[32m    442\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    443\u001b[39m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[32m    444\u001b[39m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[32m    445\u001b[39m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m446\u001b[39m             \u001b[43msix\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    448\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:3\u001b[39m, in \u001b[36mraise_from\u001b[39m\u001b[34m(value, from_value)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py:441\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[39m\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    439\u001b[39m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[32m    440\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m441\u001b[39m         httplib_response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    442\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    443\u001b[39m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[32m    444\u001b[39m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[32m    445\u001b[39m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[32m    446\u001b[39m         six.raise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/http/client.py:1374\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1372\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1373\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1374\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1375\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1376\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/http/client.py:318\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    316\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    320\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/http/client.py:279\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.fp.readline(_MAXLINE + \u001b[32m1\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    280\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    281\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/socket.py:705\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    704\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    706\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    707\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/ssl.py:1278\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1274\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1275\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1276\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1277\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1278\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1279\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1280\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/ssl.py:1134\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1132\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1133\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1134\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1135\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1136\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Set the testing mode to True for limited processing\n",
    "testing_mode = True\n",
    "\n",
    "# define the models\n",
    "model = \"aurora-sdg\"  # Use the multi-label model for SDG classification (faster, Aurora definition of SDG's, 104 languages)\n",
    "\n",
    "# other available models:\n",
    "# model = \"aurora-sdg\"  # Use the single-label model for classification of each SDG in the Aurora definition (slower, Aurora definition of SDG's, 104 languages)\n",
    "# model = \"elsevier-multi\"  # Elsevier SDG multi-label mBERT model (fast, Elsevier definition of SDG's, 104 languages)\n",
    "# model = \"osdg\"  # OSDG model (alternative, OSDG definition of SDG's, 15 languages)\n",
    "\n",
    "# Set the base URL for the Aurora SDG classifier\n",
    "base_url = \"https://aurora-sdg.labs.vu.nl/classifier/classify/\" + model\n",
    "\n",
    "# Load the descriptions with token counts\n",
    "description_df = pd.read_parquet(f\"{processing_folder_path}/{folder_name}-descriptions-with-tokens.parquet\")\n",
    "\n",
    "# Filter abstracts with at least 100 tokens\n",
    "description_df = description_df[description_df['token_count'] >= 100]\n",
    "\n",
    "# Set testing mode to limit the number of abstracts\n",
    "if testing_mode:\n",
    "    description_df = description_df.head(10)  # Limit to 10 records for testing\n",
    "\n",
    "# Prepare results list\n",
    "sdg_results = []\n",
    "\n",
    "# Rate limit settings\n",
    "rate_limit = 5  # 5 requests per second\n",
    "delay_between_requests = 1 / rate_limit\n",
    "\n",
    "# Loop through each abstract\n",
    "for idx, row in description_df.iterrows():\n",
    "    record_id = row['record_id']\n",
    "    abstract = row['description']\n",
    "\n",
    "    # Prepare the payload for the API\n",
    "    payload = json.dumps({\"text\": abstract})\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "    try:\n",
    "        # Make the API call\n",
    "        response = requests.post(base_url, headers=headers, data=payload)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the response\n",
    "        result = response.json()\n",
    "        predictions = result.get(\"predictions\", [])\n",
    "\n",
    "        # Extract SDG predictions\n",
    "        sdgs = [\n",
    "            {\n",
    "                \"goal_code\": pred[\"sdg\"][\"code\"],\n",
    "                \"goal_name\": pred[\"sdg\"][\"name\"],\n",
    "                \"prediction_score\": pred[\"prediction\"]\n",
    "            }\n",
    "            for pred in predictions\n",
    "        ]\n",
    "\n",
    "        # Append the result to the list\n",
    "        sdg_results.append({\n",
    "            \"record_id\": record_id,\n",
    "            \"abstract\": abstract,\n",
    "            \"sdgs\": sdgs\n",
    "        })\n",
    "\n",
    "        # Calculate and print the time taken to process the record\n",
    "        start_time = time.time()\n",
    "        print(f\"Processed record_id: {record_id}, SDGs: {sdgs}\")\n",
    "        end_time = time.time()\n",
    "        print(f\"Time taken to process record_id {record_id}: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error processing record_id {record_id}: {e}\")\n",
    "\n",
    "    # Add a delay to respect the rate limit\n",
    "    time.sleep(delay_between_requests)\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "sdg_results_df = pd.DataFrame(sdg_results)\n",
    "\n",
    "# calculate the 90th percentile of the prediction scores for each SDG\n",
    "sdg_scores = []\n",
    "for sdg in sdg_results_df['sdgs']:\n",
    "    for prediction in sdg:\n",
    "        sdg_scores.append(prediction['prediction_score'])  \n",
    "# Calculate the 90th percentile\n",
    "percentile_90 = pd.Series(sdg_scores).quantile(0.9)\n",
    "# Filter the results and append a column top_predicted_sdgs, to include only SDGs (as list of goal_codes) with a prediction score above the 90th percentile\n",
    "sdg_results_df['top_predicted_sdgs'] = sdg_results_df['sdgs'].apply(\n",
    "    lambda x: [sdg['goal_code'] for sdg in x if sdg['prediction_score'] >= percentile_90 and sdg['prediction_score'] > 0.1]\n",
    ")\n",
    "\n",
    "# Print the DataFrame with SDG results\n",
    "print(\"SDG classification results:\")\n",
    "print(sdg_results_df[['record_id', 'top_predicted_sdgs']])\n",
    "\n",
    "# Save the results to a Parquet file including the top predicted SDGs\n",
    "sdg_results_path = f\"{processing_folder_path}/{folder_name}-sdg-results-{model}.parquet\"\n",
    "sdg_results_df.to_parquet(sdg_results_path, index=False)\n",
    "print(f\"SDG classification results saved to: {sdg_results_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### step 7b Get the official definitions of the SDG's from https://metadata.un.org/sdg/ using the Accept header application/rdf+xml\n",
    "\n",
    "First we get the links to the top level goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDG definitions saved to: ./data/2025-02-19/04_processed/argo-france/sdg_definitions.rdf\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL for the SDG metadata\n",
    "sdg_metadata_url = \"https://metadata.un.org/sdg/\"\n",
    "\n",
    "# Set the headers to request RDF/XML format\n",
    "headers = {\n",
    "    \"Accept\": \"application/rdf+xml\"\n",
    "}\n",
    "\n",
    "# Send the GET request\n",
    "response = requests.get(sdg_metadata_url, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Save the RDF/XML content to a file\n",
    "    rdf_file_path = f\"{processing_folder_path}/sdg_definitions.rdf\"\n",
    "    with open(rdf_file_path, \"wb\") as rdf_file:\n",
    "        rdf_file.write(response.content)\n",
    "    print(f\"SDG definitions saved to: {rdf_file_path}\")\n",
    "else:\n",
    "    print(f\"Failed to fetch SDG definitions. Status code: {response.status_code}\")\n",
    "    print(f\"Response: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top concept URLs found in the RDF/XML:\n",
      "http://metadata.un.org/sdg/1\n",
      "http://metadata.un.org/sdg/2\n",
      "http://metadata.un.org/sdg/3\n",
      "http://metadata.un.org/sdg/4\n",
      "http://metadata.un.org/sdg/5\n",
      "http://metadata.un.org/sdg/6\n",
      "http://metadata.un.org/sdg/7\n",
      "http://metadata.un.org/sdg/8\n",
      "http://metadata.un.org/sdg/9\n",
      "http://metadata.un.org/sdg/10\n",
      "http://metadata.un.org/sdg/11\n",
      "http://metadata.un.org/sdg/12\n",
      "http://metadata.un.org/sdg/13\n",
      "http://metadata.un.org/sdg/14\n",
      "http://metadata.un.org/sdg/15\n",
      "http://metadata.un.org/sdg/16\n",
      "http://metadata.un.org/sdg/17\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Parse the RDF/XML file\n",
    "tree = ET.parse(rdf_file_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "# Find all skos:hasTopConcept elements and extract their rdf:resource attribute\n",
    "top_concept_urls = []\n",
    "for elem in root.findall('.//{http://www.w3.org/2004/02/skos/core#}hasTopConcept'):\n",
    "    url = elem.attrib.get('{http://www.w3.org/1999/02/22-rdf-syntax-ns#}resource')\n",
    "    if url:\n",
    "        top_concept_urls.append(url)\n",
    "\n",
    "# sort the URLs based on the integer in the last part of the URL\n",
    "top_concept_urls.sort(key=lambda x: int(x.split('/')[-1]))\n",
    "\n",
    "print(\"Top concept URLs found in the RDF/XML:\")\n",
    "for url in top_concept_urls:\n",
    "    print(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we get the goal number, goal name and goal description for each top level goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   goal_code                                goal_name  \\\n",
      "0          1                               No poverty   \n",
      "1          2                              Zero hunger   \n",
      "2          3               Good health and well-being   \n",
      "3          4                        Quality education   \n",
      "4          5                          Gender equality   \n",
      "5          6               Clean water and sanitation   \n",
      "6          7              Affordable and clean energy   \n",
      "7          8          Decent work and economic growth   \n",
      "8          9  Industry, innovation and infrastructure   \n",
      "9         10                     Reduced inequalities   \n",
      "10        11       Sustainable cities and communities   \n",
      "11        12   Responsible consumption and production   \n",
      "12        13                           Climate action   \n",
      "13        14                         Life below water   \n",
      "14        15                             Life on land   \n",
      "15        16   Peace, justice and strong institutions   \n",
      "16        17               Partnerships for the goals   \n",
      "\n",
      "                                     goal_description  \\\n",
      "0             End poverty in all its forms everywhere   \n",
      "1   End hunger, achieve food security and improved...   \n",
      "2   Ensure healthy lives and promote well-being fo...   \n",
      "3   Ensure inclusive and equitable quality educati...   \n",
      "4   Achieve gender equality and empower all women ...   \n",
      "5   Ensure availability and sustainable management...   \n",
      "6   Ensure access to affordable, reliable, sustain...   \n",
      "7   Promote sustained, inclusive and sustainable e...   \n",
      "8   Build resilient infrastructure, promote inclus...   \n",
      "9        Reduce inequality within and among countries   \n",
      "10  Make cities and human settlements inclusive, s...   \n",
      "11  Ensure sustainable consumption and production ...   \n",
      "12  Take urgent action to combat climate change an...   \n",
      "13  Conserve and sustainably use the oceans, seas ...   \n",
      "14  Protect, restore and promote sustainable use o...   \n",
      "15  Promote peaceful and inclusive societies for s...   \n",
      "16  Strengthen the means of implementation and rev...   \n",
      "\n",
      "                         goal_url  \n",
      "0    http://metadata.un.org/sdg/1  \n",
      "1    http://metadata.un.org/sdg/2  \n",
      "2    http://metadata.un.org/sdg/3  \n",
      "3    http://metadata.un.org/sdg/4  \n",
      "4    http://metadata.un.org/sdg/5  \n",
      "5    http://metadata.un.org/sdg/6  \n",
      "6    http://metadata.un.org/sdg/7  \n",
      "7    http://metadata.un.org/sdg/8  \n",
      "8    http://metadata.un.org/sdg/9  \n",
      "9   http://metadata.un.org/sdg/10  \n",
      "10  http://metadata.un.org/sdg/11  \n",
      "11  http://metadata.un.org/sdg/12  \n",
      "12  http://metadata.un.org/sdg/13  \n",
      "13  http://metadata.un.org/sdg/14  \n",
      "14  http://metadata.un.org/sdg/15  \n",
      "15  http://metadata.un.org/sdg/16  \n",
      "16  http://metadata.un.org/sdg/17  \n",
      "SDG goals saved to: ./data/2025-02-19/04_processed/argo-france/sdg_goals.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Prepare lists to store the results\n",
    "goal_codes = []\n",
    "goal_names = []\n",
    "goal_descriptions = []\n",
    "goal_urls = []\n",
    "\n",
    "# Loop through each top concept URL\n",
    "for url in top_concept_urls:\n",
    "    try:\n",
    "        # Fetch the RDF/XML content\n",
    "        resp = requests.get(url, headers={\"Accept\": \"application/rdf+xml\"})\n",
    "        resp.raise_for_status()\n",
    "        root = ET.fromstring(resp.content)\n",
    "        # Find the main Description element\n",
    "        desc = root.find('.//{http://www.w3.org/1999/02/22-rdf-syntax-ns#}Description')\n",
    "        if desc is None:\n",
    "            continue\n",
    "        # Extract <skos:note xml:lang=\"en\">Goal N</skos:note>\n",
    "        goal_code = None\n",
    "        for note in desc.findall('{http://www.w3.org/2004/02/skos/core#}note'):\n",
    "            if note.attrib.get('{http://www.w3.org/XML/1998/namespace}lang') == 'en' and note.text and note.text.startswith('Goal'):\n",
    "                goal_code = note.text.replace('Goal ', '').strip()\n",
    "                break\n",
    "        # Extract <skos:altLabel xml:lang=\"en\">...</skos:altLabel>\n",
    "        goal_name = None\n",
    "        for alt in desc.findall('{http://www.w3.org/2004/02/skos/core#}altLabel'):\n",
    "            if alt.attrib.get('{http://www.w3.org/XML/1998/namespace}lang') == 'en':\n",
    "                goal_name = alt.text.strip()\n",
    "                break\n",
    "        # Extract <skos:prefLabel xml:lang=\"en\">...</skos:prefLabel>\n",
    "        goal_description = None\n",
    "        for pref in desc.findall('{http://www.w3.org/2004/02/skos/core#}prefLabel'):\n",
    "            if pref.attrib.get('{http://www.w3.org/XML/1998/namespace}lang') == 'en':\n",
    "                goal_description = pref.text.strip()\n",
    "                break\n",
    "        # Store results\n",
    "        goal_codes.append(goal_code)\n",
    "        goal_names.append(goal_name)\n",
    "        goal_descriptions.append(goal_description)\n",
    "        goal_urls.append(url)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {e}\")\n",
    "\n",
    "# Create DataFrame\n",
    "df_sdg_goals = pd.DataFrame({\n",
    "    \"goal_code\": goal_codes,\n",
    "    \"goal_name\": goal_names,\n",
    "    \"goal_description\": goal_descriptions,\n",
    "    \"goal_url\": goal_urls\n",
    "})\n",
    "\n",
    "print(df_sdg_goals)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "sdg_goals_csv_path = f\"{processing_folder_path}/sdg_goals.csv\"\n",
    "df_sdg_goals.to_csv(sdg_goals_csv_path, index=False)\n",
    "print(f\"SDG goals saved to: {sdg_goals_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 7c Here we prepare the System and User prompts to be used by an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text to classify:\n",
      "\n",
      "The United Nations Sustainable Development Goals (SDGs) are a universal call to action to end poverty, protect the planet, and ensure prosperity for all by 2030. They address global challenges such as inequality, climate change, environmental degradation, peace, and justice. The SDGs consist of 17 goals and 169 targets that aim to achieve a better and more sustainable future for all.\n",
      "\n",
      "Example Output Format:\n",
      "\n",
      "{\n",
      "    \"sdgs\": [2, 6, 17],\n",
      "    \"explanation\": \"This text is related to SDG 2 (Zero hunger) because it discusses food security, SDG 6 (Clean water and sanitation) due to references to environmental protection, and SDG 17 (Partnerships for the goals) as it mentions global cooperation.\"\n",
      "}\n",
      "\n",
      "System Prompt:\n",
      "\n",
      "You are an intelligent multi-label classification system designed to map texts to their relevant Sustainable Development Goals.\n",
      "Take the text delimited by triple quotation marks and return a JSON list of relevant SDGs. \n",
      "Example output format: \n",
      "{\n",
      "    \"sdgs\": [2, 6, 17],\n",
      "    \"explanation\": \"This text is related to SDG 2 (Zero hunger) because it discusses food security, SDG 6 (Clean water and sanitation) due to references to environmental protection, and SDG 17 (Partnerships for the goals) as it mentions global cooperation.\"\n",
      "}\n",
      "\n",
      "\n",
      "Here are the SDG goals and their descriptions:\n",
      "1: No poverty - End poverty in all its forms everywhere\n",
      "2: Zero hunger - End hunger, achieve food security and improved nutrition and promote sustainable agriculture\n",
      "3: Good health and well-being - Ensure healthy lives and promote well-being for all at all ages\n",
      "4: Quality education - Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all\n",
      "5: Gender equality - Achieve gender equality and empower all women and girls\n",
      "6: Clean water and sanitation - Ensure availability and sustainable management of water and sanitation for all\n",
      "7: Affordable and clean energy - Ensure access to affordable, reliable, sustainable and modern energy for all\n",
      "8: Decent work and economic growth - Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all\n",
      "9: Industry, innovation and infrastructure - Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation\n",
      "10: Reduced inequalities - Reduce inequality within and among countries\n",
      "11: Sustainable cities and communities - Make cities and human settlements inclusive, safe, resilient and sustainable\n",
      "12: Responsible consumption and production - Ensure sustainable consumption and production patterns\n",
      "13: Climate action - Take urgent action to combat climate change and its impacts\n",
      "14: Life below water - Conserve and sustainably use the oceans, seas and marine resources for sustainable development\n",
      "15: Life on land - Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss\n",
      "16: Peace, justice and strong institutions - Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels\n",
      "17: Partnerships for the goals - Strengthen the means of implementation and revitalize the Global Partnership for Sustainable Development\n",
      "\n",
      "\n",
      "User Prompt:\n",
      "\n",
      "\"Classify the following text in terms of its relevance to the Sustainable Development Goals:\",\n",
      "Text: '''\n",
      "The United Nations Sustainable Development Goals (SDGs) are a universal call to action to end poverty, protect the planet, and ensure prosperity for all by 2030. They address global challenges such as inequality, climate change, environmental degradation, peace, and justice. The SDGs consist of 17 goals and 169 targets that aim to achieve a better and more sustainable future for all.\n",
      "'''\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the text to classify\n",
    "text = \"\"\"\n",
    "The United Nations Sustainable Development Goals (SDGs) are a universal call to action to end poverty, protect the planet, and ensure prosperity for all by 2030. They address global challenges such as inequality, climate change, environmental degradation, peace, and justice. The SDGs consist of 17 goals and 169 targets that aim to achieve a better and more sustainable future for all.\n",
    "\"\"\"\n",
    "# Print the text to classify\n",
    "print(\"Text to classify:\")\n",
    "print(text)\n",
    "\n",
    "# Define the expected output format, now including an explanation field\n",
    "example_output_format = \"\"\"\n",
    "{\n",
    "    \"sdgs\": [2, 6, 17],\n",
    "    \"explanation\": \"This text is related to SDG 2 (Zero hunger) because it discusses food security, SDG 6 (Clean water and sanitation) due to references to environmental protection, and SDG 17 (Partnerships for the goals) as it mentions global cooperation.\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Print the example output format\n",
    "print(\"Example Output Format:\")\n",
    "print(example_output_format)\n",
    "\n",
    "# system_prompt\n",
    "# Build SDG goal info string from df_sdg_goals\n",
    "sdg_goal_info = \"\\n\".join(\n",
    "    f\"{row.goal_code}: {row.goal_name} - {row.goal_description}\"\n",
    "    for _, row in df_sdg_goals.iterrows()\n",
    ")\n",
    "\n",
    "sdg_system_prompt = f\"\"\"\n",
    "You are an intelligent multi-label classification system designed to map texts to their relevant Sustainable Development Goals.\n",
    "Take the text delimited by triple quotation marks and return a JSON list of relevant SDGs. \n",
    "Example output format: {example_output_format}\n",
    "\n",
    "Here are the SDG goals and their descriptions:\n",
    "{sdg_goal_info}\n",
    "\n",
    "\"\"\"\n",
    "# Print the system prompt\n",
    "print(\"System Prompt:\")\n",
    "print(sdg_system_prompt)\n",
    "# user_prompt\n",
    "sdg_user_prompt = f\"\"\"\n",
    "\"Classify the following text in terms of its relevance to the Sustainable Development Goals:\",\n",
    "Text: '''{text}'''\n",
    "\"\"\"\n",
    "# Print the user prompt\n",
    "print(\"User Prompt:\")\n",
    "print(sdg_user_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 7d: Get the LLM API prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenWebUI API configuration\n",
    "openwebui_base_url = \"https://nebula.cs.vu.nl\"  # Replace with your actual OpenWebUI API base URL\n",
    "openwebui_api_key = \"sk-5b5a024888c14a019c0e9b4857df9329\"  # Replace with your actual API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first get the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curl -X GET 'https://nebula.cs.vu.nl/api/models' -H 'Authorization: Bearer sk-5b5a024888c14a019c0e9b4857df9329'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# print the request in curl\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurl -X GET \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapi_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m -H \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAuthorization: Bearer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mopenwebui_api_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     21\u001b[0m     models_json \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/data/ori_storage/OpenAIRE-tools/.venv/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/data/ori_storage/OpenAIRE-tools/.venv/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/data/ori_storage/OpenAIRE-tools/.venv/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/data/ori_storage/OpenAIRE-tools/.venv/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/data/ori_storage/OpenAIRE-tools/.venv/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/data/ori_storage/OpenAIRE-tools/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/data/ori_storage/OpenAIRE-tools/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/data/ori_storage/OpenAIRE-tools/.venv/lib/python3.10/site-packages/urllib3/connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    513\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1303\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1300\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1301\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1302\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1159\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This script fetches the list of available models from the OpenWebUI API\n",
    "# and prints their IDs, names, and parameter sizes.\n",
    "\n",
    "import requests\n",
    "\n",
    "# Use the existing variables openwebui_base_url and openwebui_api_key\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {openwebui_api_key}\"\n",
    "}\n",
    "\n",
    "# Ensure the base URL does not end with a slash\n",
    "api_url = openwebui_base_url.rstrip('/') + \"/api/models\"\n",
    "\n",
    "# print the request in curl\n",
    "print(f\"curl -X GET '{api_url}' -H 'Authorization: Bearer {openwebui_api_key}'\")\n",
    "\n",
    "response = requests.get(api_url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    models_json = response.json()\n",
    "    models = models_json.get(\"data\", [])\n",
    "    print(\"Available models:\")\n",
    "    for model in models:\n",
    "        print(f\"- id: {model.get('id')}, name: {model.get('name')}, parameter_size: {model.get('ollama', {}).get('details', {}).get('parameter_size')}\")\n",
    "else:\n",
    "    print(f\"Failed to fetch models. Status code: {response.status_code}\")\n",
    "    print(f\"Response: {response.text}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the model to use, when no model is chosen, deepseek-r1:1.5b will be the default (faser & cheaper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvailable models:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mmodels\u001b[49m):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelect the model index to use (default: 2, llama3.1:8b) [timeout 10s]:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "import signal\n",
    "\n",
    "# Select the model to use, when no model is chosen, llama3.1:8b will be the default\n",
    "model = \"llama3.1:8b\"  # Replace with your actual model name\n",
    "\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutError\n",
    "\n",
    "print(\"Available models:\")\n",
    "for i, m in enumerate(models):\n",
    "    print(f\"{i}: {m['id']}\")\n",
    "\n",
    "print(\"Select the model index to use (default: 2, llama3.1:8b) [timeout 10s]:\")\n",
    "signal.signal(signal.SIGALRM, timeout_handler)\n",
    "signal.alarm(10)\n",
    "try:\n",
    "    user_input = input()\n",
    "    if user_input.strip().isdigit():\n",
    "        selected_model_index = int(user_input.strip())\n",
    "        if 0 <= selected_model_index < len(models):\n",
    "            model = models[selected_model_index]['id']\n",
    "        else:\n",
    "            print(\"Invalid index, using default model.\")\n",
    "            model = \"llama3.1:8b\"\n",
    "    else:\n",
    "        print(\"No valid input, using default model.\")\n",
    "        model = \"llama3.1:8b\"\n",
    "except TimeoutError:\n",
    "    print(\"No response received. Using default model.\")\n",
    "    model = \"llama3.1:8b\"\n",
    "finally:\n",
    "    signal.alarm(0)\n",
    "\n",
    "print(f\"Model selected: {model}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for each abstract, run the system and user prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for record_id doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef: {'model': 'llama3.1:8b', 'messages': [{'role': 'system', 'content': '\\nYou are an intelligent multi-label classification system designed to map texts to their relevant Sustainable Development Goals.\\nTake the text delimited by triple quotation marks and return a JSON list of relevant SDGs. \\nExample output format: \\n{\\n    \"sdgs\": [2, 6, 17],\\n    \"explanation\": \"This text is related to SDG 2 (Zero hunger) because it discusses food security, SDG 6 (Clean water and sanitation) due to references to environmental protection, and SDG 17 (Partnerships for the goals) as it mentions global cooperation.\"\\n}\\n\\n\\nHere are the SDG goals and their descriptions:\\n1: No poverty - End poverty in all its forms everywhere\\n2: Zero hunger - End hunger, achieve food security and improved nutrition and promote sustainable agriculture\\n3: Good health and well-being - Ensure healthy lives and promote well-being for all at all ages\\n4: Quality education - Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all\\n5: Gender equality - Achieve gender equality and empower all women and girls\\n6: Clean water and sanitation - Ensure availability and sustainable management of water and sanitation for all\\n7: Affordable and clean energy - Ensure access to affordable, reliable, sustainable and modern energy for all\\n8: Decent work and economic growth - Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all\\n9: Industry, innovation and infrastructure - Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation\\n10: Reduced inequalities - Reduce inequality within and among countries\\n11: Sustainable cities and communities - Make cities and human settlements inclusive, safe, resilient and sustainable\\n12: Responsible consumption and production - Ensure sustainable consumption and production patterns\\n13: Climate action - Take urgent action to combat climate change and its impacts\\n14: Life below water - Conserve and sustainably use the oceans, seas and marine resources for sustainable development\\n15: Life on land - Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss\\n16: Peace, justice and strong institutions - Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels\\n17: Partnerships for the goals - Strengthen the means of implementation and revitalize the Global Partnership for Sustainable Development\\n\\n'}, {'role': 'user', 'content': \"Classify the following text in terms of its relevance to the Sustainable Development Goals:\\nText: '''Abstract</jats:title><jats:p>The Black Sea, the largest semienclosed anoxic basin on Earth, can be considered as an excellent natural laboratory for oxic and anoxic biogeochemical processes. The suboxic zone, a thin interface between oxic and anoxic waters, still remains poorly understood because it has been undersampled. This has led to alternative concepts regarding the underlying processes that create it. Existing hypotheses suggest that the interface originates either by isopycnal intrusions that introduce oxygen or the dynamics of manganese redox cycling that are associated with the sinking of particles or chemosynthetic bacteria. Here we reexamine these concepts using high‐resolution oxygen, sulfide, nitrate, and particle concentration profiles obtained with sensors deployed on profiling floats. Our results show an extremely stable structure in density space over the entire basin with the exception of areas near the Bosporus plume and in the southern areas dominated by coastal anticyclones. The absence of large‐scale horizontal intrusive signatures in the open‐sea supports a hypothesis prioritizing the role of biogeochemical processes.</jats:p>'''\"}]}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData for record_id \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecord_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Make the API call\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopenwebui_base_url\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/api/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAuthorization\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBearer \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mopenwebui_api_key\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mContent-Type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapplication/json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Check if the request was successful\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "File \u001b[0;32m~/data/ori_storage/OpenAIRE-tools/.venv/lib/python3.10/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/data/ori_storage/OpenAIRE-tools/.venv/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/data/ori_storage/OpenAIRE-tools/.venv/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/data/ori_storage/OpenAIRE-tools/.venv/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/data/ori_storage/OpenAIRE-tools/.venv/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/data/ori_storage/OpenAIRE-tools/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/data/ori_storage/OpenAIRE-tools/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/data/ori_storage/OpenAIRE-tools/.venv/lib/python3.10/site-packages/urllib3/connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    513\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1303\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1300\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1301\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1302\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1159\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Add a testing method to limit the number of abstracts\n",
    "if testing_mode:\n",
    "    # Load only the first 3 abstracts for testing\n",
    "    description_df = pd.read_parquet(f\"{processing_folder_path}/{folder_name}-descriptions-with-tokens.parquet\").head(3)\n",
    "else:\n",
    "    # Load all abstracts for production\n",
    "    description_df = pd.read_parquet(f\"{processing_folder_path}/{folder_name}-descriptions-with-tokens.parquet\")                                                             \n",
    "\n",
    "# Filter abstracts with at least 100 tokens\n",
    "description_df = description_df[description_df['token_count'] >= 100]\n",
    "\n",
    "# Prepare results list\n",
    "sdg_results = []\n",
    "\n",
    "# Loop through each abstract\n",
    "for idx, row in description_df.iterrows():\n",
    "    record_id = row['record_id']\n",
    "    abstract = row['description']\n",
    "\n",
    "    # Prepare the messages for the API\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": sdg_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Classify the following text in terms of its relevance to the Sustainable Development Goals:\\nText: '''{abstract}'''\"}\n",
    "    ]\n",
    "\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages\n",
    "    }\n",
    "\n",
    "    # Print the data variable for debugging\n",
    "    print(f\"Data for record_id {record_id}: {data}\")\n",
    "\n",
    "    # Make the API call\n",
    "    response = requests.post(\n",
    "        openwebui_base_url.rstrip('/') + \"/api/chat/completions\",\n",
    "        headers={\"Authorization\": f\"Bearer {openwebui_api_key}\", \"Content-Type\": \"application/json\"},\n",
    "        json=data\n",
    "    )\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error processing record_id {record_id}: {response.status_code} - {response.text}\")\n",
    "        continue\n",
    "\n",
    "    # Print the response for debugging\n",
    "    print(f\"Response for record_id {record_id}: {response.json()}\")\n",
    "    \n",
    "    # Parse the response\n",
    "    try:\n",
    "        result = response.json()\n",
    "        # Try to extract the SDG list from the response\n",
    "        content = result['choices'][0]['message']['content']\n",
    "        # Try to parse the JSON from the model output\n",
    "        try:\n",
    "            sdg_json = eval(content) if isinstance(content, str) else content\n",
    "            sdgs = sdg_json.get(\"sdgs\", [])\n",
    "            explanation = sdg_json.get(\"explanation\", \"\")\n",
    "        except Exception:\n",
    "            sdgs = []\n",
    "            explanation = \"\"\n",
    "    except Exception:\n",
    "        sdgs = []\n",
    "        explanation = \"\"\n",
    "\n",
    "    # Append to results, including the explanation if available\n",
    "    sdg_results.append({\n",
    "        \"record_id\": record_id,\n",
    "        \"abstract\": abstract,\n",
    "        \"sdgs\": sdgs,\n",
    "        \"explanation\": explanation\n",
    "    })\n",
    "\n",
    "    # Optional: print progress\n",
    "    print(f\"Processed record_id: {record_id}, SDGs: {sdgs}\")\n",
    "\n",
    "    # Optional: delay to avoid rate limits\n",
    "    time.sleep(1)\n",
    "\n",
    "# Print the number of results\n",
    "print(f\"Number of SDG results collected: {len(sdg_results)}\")\n",
    "\n",
    "# Make the value of the model variable suitable for using in the file names\n",
    "model_filename = model.replace(\":\", \"-\").replace(\" \", \"_\")\n",
    "\n",
    "# Save results to parquet\n",
    "sdg_results_df = pd.DataFrame(sdg_results)\n",
    "sdg_results_path = f\"{processing_folder_path}/{folder_name}-sdg-results-{model_filename}.parquet\"\n",
    "sdg_results_df.to_parquet(sdg_results_path, index=False)\n",
    "print(f\"SDG LLM results saved to: {sdg_results_path}\")\n",
    "print(f\"File size: {os.path.getsize(sdg_results_path) / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Get Genderize data\n",
    "a. First Query the authors with country of the affiliation along with the record id (to be used as primary keys, connecting the tables later on), \n",
    "\n",
    "b. get gender data by parsing the author names with country label over an API, \n",
    "\n",
    "c. save the outcomes in a separate parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique first_name+country combinations: 49466\n",
      "Processed: Cécile (FR) - Gender: female\n",
      "Processed: Gabriele (IT) - Gender: male\n",
      "Processed: Renata (IT) - Gender: female\n",
      "Processed: Antonio (IT) - Gender: male\n",
      "Processed: Aiman (FR) - Gender: male\n",
      "Processed: Fabio (IT) - Gender: male\n",
      "Processed: Emanuela (IT) - Gender: female\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 105\u001b[39m\n\u001b[32m    102\u001b[39m     params[\u001b[33m\"\u001b[39m\u001b[33mapikey\u001b[39m\u001b[33m\"\u001b[39m] = genderize_api_key\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m     response.raise_for_status()\n\u001b[32m    107\u001b[39m     data = response.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3/dist-packages/requests/api.py:76\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     66\u001b[39m \n\u001b[32m     67\u001b[39m \u001b[33;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     72\u001b[39m \u001b[33;03m:rtype: requests.Response\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     75\u001b[39m kwargs.setdefault(\u001b[33m'\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3/dist-packages/requests/api.py:61\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3/dist-packages/requests/sessions.py:544\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    539\u001b[39m send_kwargs = {\n\u001b[32m    540\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m'\u001b[39m: timeout,\n\u001b[32m    541\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m'\u001b[39m: allow_redirects,\n\u001b[32m    542\u001b[39m }\n\u001b[32m    543\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m544\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3/dist-packages/requests/sessions.py:657\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m start = preferred_clock()\n\u001b[32m    656\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    660\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3/dist-packages/requests/adapters.py:439\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    437\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    438\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunked:\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m         resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m            \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m            \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m            \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    452\u001b[39m     \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m    453\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    454\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(conn, \u001b[33m'\u001b[39m\u001b[33mproxy_pool\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py:700\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[39m\n\u001b[32m    697\u001b[39m     \u001b[38;5;28mself\u001b[39m._prepare_proxy(conn)\n\u001b[32m    699\u001b[39m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m700\u001b[39m httplib_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    706\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[32m    711\u001b[39m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[32m    712\u001b[39m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[32m    713\u001b[39m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[32m    714\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py:383\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[39m\n\u001b[32m    381\u001b[39m \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    385\u001b[39m     \u001b[38;5;66;03m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n\u001b[32m    386\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=conn.timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py:1017\u001b[39m, in \u001b[36mHTTPSConnectionPool._validate_conn\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(conn, \u001b[33m\"\u001b[39m\u001b[33msock\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1017\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.is_verified:\n\u001b[32m   1020\u001b[39m     warnings.warn(\n\u001b[32m   1021\u001b[39m         (\n\u001b[32m   1022\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mUnverified HTTPS request is being made to host \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1027\u001b[39m         InsecureRequestWarning,\n\u001b[32m   1028\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3/dist-packages/urllib3/connection.py:411\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    403\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ca_certs\n\u001b[32m    404\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ca_cert_dir\n\u001b[32m   (...)\u001b[39m\u001b[32m    407\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(context, \u001b[33m\"\u001b[39m\u001b[33mload_default_certs\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    408\u001b[39m ):\n\u001b[32m    409\u001b[39m     context.load_default_certs()\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m \u001b[38;5;28mself\u001b[39m.sock = \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[38;5;66;03m# If we're using all defaults and the connection\u001b[39;00m\n\u001b[32m    425\u001b[39m \u001b[38;5;66;03m# is TLSv1 or TLSv1.1 we throw a DeprecationWarning\u001b[39;00m\n\u001b[32m    426\u001b[39m \u001b[38;5;66;03m# for the host.\u001b[39;00m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    428\u001b[39m     default_ssl_context\n\u001b[32m    429\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ssl_version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    430\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mversion\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    431\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.sock.version() \u001b[38;5;129;01min\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mTLSv1\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mTLSv1.1\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m    432\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3/dist-packages/urllib3/util/ssl_.py:402\u001b[39m, in \u001b[36mssl_wrap_socket\u001b[39m\u001b[34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[39m\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ca_certs \u001b[38;5;129;01mor\u001b[39;00m ca_cert_dir \u001b[38;5;129;01mor\u001b[39;00m ca_cert_data:\n\u001b[32m    401\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m         \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_verify_locations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIOError\u001b[39;00m, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    404\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import unicodedata\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "import duckdb\n",
    "\n",
    "# --- Settings ---\n",
    "paid_subscription = False \n",
    "genderize_api_key = \"da1a264b9bab63b46f27ac635dd7d2df\"\n",
    "\n",
    "# --- Paths ---\n",
    "processing_folder_path = f\"./data/{publication_date}/04_processed/{folder_name}\"\n",
    "combined_file_path = f\"{processing_folder_path}/{folder_name}-combined-id-pid.parquet\"\n",
    "\n",
    "# Output folder\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "gender_output_folder = f\"./data/{publication_date}/05_gender/{folder_name}_gender_{timestamp}\"\n",
    "os.makedirs(gender_output_folder, exist_ok=True)\n",
    "\n",
    "# --- DuckDB query to extract authors ---\n",
    "master_file = f\"./data/{publication_date}/03_transformed/{folder_name}/{folder_name}-master.parquet\"\n",
    "\n",
    "con = duckdb.connect()\n",
    "authors = con.sql(f\"\"\"\n",
    "    SELECT \n",
    "        id AS record_id,\n",
    "        unnest.fullName AS full_name,\n",
    "        unnest.name AS first_name,\n",
    "        unnest.surname AS last_name,\n",
    "        unnest.pid.id.value AS orcid,\n",
    "        countries[1].label AS country_name,\n",
    "        countries[1].code AS country_code\n",
    "    FROM read_parquet('{master_file}')\n",
    "    CROSS JOIN UNNEST(authors) AS unnest\n",
    "    WHERE countries IS NOT NULL AND array_length(countries) > 0\n",
    "\"\"\").fetchdf()\n",
    "con.close()\n",
    "\n",
    "# Save raw authors data\n",
    "authors_file_path = os.path.join(processing_folder_path, f\"{folder_name}-authors.parquet\")\n",
    "authors.to_parquet(authors_file_path, index=False)\n",
    "\n",
    "# --- Prepare unique authors ---\n",
    "unique_authors = authors[['first_name', 'country_code']].copy()\n",
    "unique_authors['first_name'] = unique_authors['first_name'].str.split().str[0]\n",
    "unique_authors = unique_authors.dropna(subset=['first_name'])\n",
    "unique_authors = unique_authors[unique_authors['first_name'].str.lower() != 'none']\n",
    "\n",
    "# --- Advanced cleaning ---\n",
    "JUNK_TOKENS = {'-', 'prof', 'prof.', 'professore', 'professor', 'dr', 'dr.', 'none'}\n",
    "def is_garbage(name):\n",
    "    if not isinstance(name, str):\n",
    "        return True\n",
    "    name_lower = name.strip().lower()\n",
    "    return '.' in name or name_lower in JUNK_TOKENS or len(name.strip()) <= 1\n",
    "\n",
    "unique_authors = unique_authors[~unique_authors['first_name'].apply(is_garbage)]\n",
    "\n",
    "def clean_symbols(name: str):\n",
    "    if not isinstance(name, str):\n",
    "        return None\n",
    "    name = unicodedata.normalize(\"NFKC\", name)\n",
    "    name = re.sub(r'\\p{C}+', '', name)\n",
    "    name = re.sub(r\"^[\\\"'()\\[\\]{}<>]+|[\\\"'()\\[\\]{}<>]+$\", \"\", name)\n",
    "    name = re.sub(r\"[^ \\p{L}-]\", \"\", name)\n",
    "    name = re.sub(r\"\\s+\", \" \", name).strip()\n",
    "    if name.startswith(\"-\"):\n",
    "        name = name.lstrip(\"-\").strip()\n",
    "    if re.fullmatch(r\"(?:[A-Z]-)+[A-Z]\", name, flags=re.I):\n",
    "        return None\n",
    "    return name.capitalize() if name else None\n",
    "\n",
    "unique_authors['first_name'] = unique_authors['first_name'].apply(clean_symbols)\n",
    "unique_authors = unique_authors.dropna(subset=['first_name'])\n",
    "unique_authors = unique_authors[unique_authors['first_name'].str.len() > 1]\n",
    "\n",
    "# --- Keep only unique first_name + country_code combinations ---\n",
    "unique_combinations = unique_authors.drop_duplicates(subset=['first_name', 'country_code'])\n",
    "print(f\"Number of unique first_name+country combinations: {len(unique_combinations)}\")\n",
    "\n",
    "# --- Genderize API settings ---\n",
    "rate_limit = 1000 if paid_subscription else 100\n",
    "delay_between_requests = 0.5\n",
    "base_url = \"https://api.genderize.io\"\n",
    "request_count = 0\n",
    "gender_results = []\n",
    "\n",
    "# --- Iterate over unique name-country combinations ---\n",
    "for _, row in unique_combinations.iterrows():\n",
    "    if request_count >= rate_limit:\n",
    "        print(\"Rate limit reached. Stopping for the day.\")\n",
    "        break\n",
    "\n",
    "    first_name = row['first_name']\n",
    "    country_code = row['country_code']\n",
    "\n",
    "    params = {\"name\": first_name, \"country_id\": country_code}\n",
    "    if paid_subscription:\n",
    "        params[\"apikey\"] = genderize_api_key\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        gender_results.append({\n",
    "            \"first_name\": first_name,\n",
    "            \"country_code\": country_code,\n",
    "            \"gender\": data.get(\"gender\"),\n",
    "            \"probability\": data.get(\"probability\"),\n",
    "            \"count\": data.get(\"count\")\n",
    "        })\n",
    "        request_count += 1\n",
    "        print(f\"Processed: {first_name} ({country_code}) - Gender: {data.get('gender')}\")\n",
    "        time.sleep(delay_between_requests)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error processing {first_name} ({country_code}): {e}\")\n",
    "        request_count += 1\n",
    "        time.sleep(1)\n",
    "\n",
    "# --- Save results ---\n",
    "gender_df = pd.DataFrame(gender_results)\n",
    "gender_file_path = os.path.join(gender_output_folder, f\"{folder_name}-gender-data.parquet\")\n",
    "gender_df.to_parquet(gender_file_path, index=False)\n",
    "print(f\"Gender data saved to: {gender_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Get Citizen Science classification labels\n",
    "\n",
    "a. Query the abstracts first along with the id (to be used as primary keys, connecting the tables later on), \n",
    "\n",
    "b. get citizen science labels by parsing the abstract over an LLM API with system prompt, \n",
    "\n",
    "c. save the outcomes in a separate parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 10: Generate SQL schemas for all the parquet files in the processed folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating schema for: ./data/2025-02-19/04_processed/argo-france/argo-france-combined-id-pid.parquet\n",
      "Schema file path: ./data/2025-02-19/04_processed/argo-france/argo-france-combined-id-pid.sql\n",
      "Schema file exists: ./data/2025-02-19/04_processed/argo-france/argo-france-combined-id-pid.sql\n",
      "Schema file content:\n",
      "column_name,column_type,null,key,default,extra\n",
      "record_id,VARCHAR,YES,,,\n",
      "pid_scheme,VARCHAR,YES,,,\n",
      "pid_value,VARCHAR,YES,,,\n",
      "combined_id_pid,VARCHAR,YES,,,\n",
      "\n",
      "Generating schema for: ./data/2025-02-19/04_processed/argo-france/argo-france-descriptions-with-tokens.parquet\n",
      "Schema file path: ./data/2025-02-19/04_processed/argo-france/argo-france-descriptions-with-tokens.sql\n",
      "Schema file exists: ./data/2025-02-19/04_processed/argo-france/argo-france-descriptions-with-tokens.sql\n",
      "Schema file content:\n",
      "column_name,column_type,null,key,default,extra\n",
      "record_id,VARCHAR,YES,,,\n",
      "description,VARCHAR,YES,,,\n",
      "token_count,BIGINT,YES,,,\n",
      "\n",
      "Generating schema for: ./data/2025-02-19/04_processed/argo-france/argo-france-authors.parquet\n",
      "Schema file path: ./data/2025-02-19/04_processed/argo-france/argo-france-authors.sql\n",
      "Schema file exists: ./data/2025-02-19/04_processed/argo-france/argo-france-authors.sql\n",
      "Schema file content:\n",
      "column_name,column_type,null,key,default,extra\n",
      "record_id,VARCHAR,YES,,,\n",
      "full_name,VARCHAR,YES,,,\n",
      "first_name,VARCHAR,YES,,,\n",
      "last_name,VARCHAR,YES,,,\n",
      "orcid,VARCHAR,YES,,,\n",
      "country_name,VARCHAR,YES,,,\n",
      "country_code,VARCHAR,YES,,,\n",
      "\n",
      "Generating schema for: ./data/2025-02-19/04_processed/argo-france/argo-france-gender-data.parquet\n",
      "Schema file path: ./data/2025-02-19/04_processed/argo-france/argo-france-gender-data.sql\n",
      "Schema file exists: ./data/2025-02-19/04_processed/argo-france/argo-france-gender-data.sql\n",
      "Schema file content:\n",
      "column_name,column_type,null,key,default,extra\n",
      "first_name,VARCHAR,YES,,,\n",
      "country_code,VARCHAR,YES,,,\n",
      "gender,VARCHAR,YES,,,\n",
      "probability,DOUBLE,YES,,,\n",
      "count,BIGINT,YES,,,\n",
      "\n",
      "Generating schema for: ./data/2025-02-19/04_processed/argo-france/argo-france-sdg-llm-results.parquet\n",
      "Schema file path: ./data/2025-02-19/04_processed/argo-france/argo-france-sdg-llm-results.sql\n"
     ]
    },
    {
     "ename": "InvalidInputException",
     "evalue": "Invalid Input Error: Failed to read Parquet file './data/2025-02-19/04_processed/argo-france/argo-france-sdg-llm-results.parquet': Need at least one non-root column in the file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidInputException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating schema for: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparquet_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSchema file path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mschema_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mduckdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'''\u001b[39;49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;43m    COPY (\u001b[39;49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;43m        SELECT *\u001b[39;49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;43m        FROM (DESCRIBE \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mparquet_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;43m    )\u001b[39;49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;43m    TO \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mschema_file_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;43m\u001b[39;49m\u001b[38;5;124;43m'''\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(schema_file_path):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSchema file exists: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mschema_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mInvalidInputException\u001b[0m: Invalid Input Error: Failed to read Parquet file './data/2025-02-19/04_processed/argo-france/argo-france-sdg-llm-results.parquet': Need at least one non-root column in the file"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import duckdb\n",
    "\n",
    "# List all .parquet files in the processing folder\n",
    "parquet_dir = os.path.join(processing_folder_path)\n",
    "parquet_files = [\n",
    "    f for f in os.listdir(parquet_dir)\n",
    "    if f.endswith('.parquet')\n",
    "]\n",
    "\n",
    "for parquet_file in parquet_files:\n",
    "    parquet_path = os.path.join(processing_folder_path, parquet_file)\n",
    "    schema_file_name = os.path.splitext(parquet_file)[0] + '.sql'\n",
    "    schema_file_path = os.path.join(processing_folder_path, schema_file_name)\n",
    "    \n",
    "    print(f\"Generating schema for: {parquet_path}\")\n",
    "    print(f\"Schema file path: {schema_file_path}\")\n",
    "\n",
    "    duckdb.sql(f'''\n",
    "        COPY (\n",
    "            SELECT *\n",
    "            FROM (DESCRIBE '{parquet_path}')\n",
    "        )\n",
    "        TO '{schema_file_path}'\n",
    "    ''')\n",
    "\n",
    "    if os.path.exists(schema_file_path):\n",
    "        print(f\"Schema file exists: {schema_file_path}\")\n",
    "        with open(schema_file_path, 'r') as schema_file:\n",
    "            schema_content = schema_file.read()\n",
    "            print(\"Schema file content:\")\n",
    "            print(schema_content)\n",
    "    else:\n",
    "        print(f\"Schema file does not exist: {schema_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
