{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAIRE Community Data Dump Handling: Extraction, Tranformation and Enrichment\n",
    "\n",
    "In this notebook we will start with one of the [OpenAIRE Community Subgraphs](https://graph.openaire.eu/docs/downloads/subgraphs) to enrich that informatino for further analysis.\n",
    "\n",
    "This data process will extract an [OpenAIRE community data dump from Zenodo](https://doi.org/10.5281/zenodo.3974604), transforms it in to a portable file format .parquet (and updatable with changes for time seires analysis), that can be used to query with DuckDB, to enrich this with additional data  (also in .parquet, for join queries).\n",
    "\n",
    "This additional data can be societal impact data from [Altmetric.com](https://details-page-api-docs.altmetric.com/) or [Overton.io](https://app.overton.io/swagger.php), Gender data using [genderize.io](https://genderize.io/documentation), sdg classification using [aurora-sdg](https://aurora-universities.eu/sdg-research/sdg-api/)\n",
    "\n",
    "This script needs to be written in a way so that it can run every month using  the latest data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing steps\n",
    "\n",
    "* the folder ./data/ is put in .gitignore to prevent that bulk datais sent to a code repository. So make sure that folder exists, and mkdir if not exists. \n",
    "* The script downloads the lastest Data Dump Tar file from one selected community. See https://doi.org/10.5281/zenodo.3974604 for the latest list. In our case the Aurora tar file. https://zenodo.org/records/14887484/files/aurora.tar?download=1\n",
    "  * Use the json record of zenodo to get to the latest record, and fetch the download link of the aurora.tar file. for example : https://zenodo.org/records/14887484/export/json or https://zenodo.org/api/records/14887484/versions/latest \n",
    "  Make the tar filename a variable, so it can be used for multiple community dumps.\n",
    "  Download the tar file in a target folder ./data/{publication_date}/01-downloaded/{filename} where a subfolder is created using the filename and the timestamp. Make this also as a  variable to use later on.\n",
    "* Extract the tar file, to the compressed .json.gz files and put these in target folder ./data/{publication_date}/02-extracted/{filename}\n",
    "* Transform the compressed .json.gz files into a single .parquet file in target folder ./data/{publication_date}/03-transformed/{filename}\n",
    "Use instructions in sections \"Processing JSON files with DuckDB\" and \"Full dataset, bit by bit\" and \"Splitting and Processing JSON Files in Batches\" https://github.com/mosart/OpenAIRE-tools/blob/main/duckdb-querying.ipynb to start with. (be aware of error messages, and fix the issues to get all the data in)\n",
    "* Extract the SQL schema (schema-datadump.sql) from the .parquet file and put it in target folder ./data/{filename+timestamp}/02-transformed/ This is needed for further processing of the records with DuckDB later on.\n",
    "Use instructions in section \"Extracting Schema from Parquet File\" https://github.com/mosart/OpenAIRE-tools/blob/main/duckdb-querying.ipynb to start with.\n",
    "* Query to get all identifiers: openaire id, doi, isbn, hdl, etc.\n",
    "* **Get Altmetric data:**\n",
    "* Extract the Altmetric data using the Identifiers. Transform keeping the record id in .parquet and put that in target folder ./data/{publication_date}/04-processed/{filename}/\n",
    "* Extract the SQL schema (schema-altmetric.sql) from the .parquet file and put it in target folder ./data/{publication_date}/04-processed/{filename}/\n",
    "* **Get Overton data:** Repeat the altmetric steps, bun than for Overton.\n",
    "* **Get Gender data** query for the Author names and country codes, and run them over the gerderize api\n",
    "* **Get SDG data** query for the abstracts, and run abstracs larger than 100 tokens over the aurora-SDG api."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Mode\n",
    "Testign mode will reduce the number of records to process. Set to False if you want to go for the long haul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mode = True # Set to False for production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Get the latest Community Dump File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37 .tar files in the dataset.\n",
      "Details of .tar files:\n",
      "[{'filename': 'energy-planning_1.tar', 'size': '6.99 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/energy-planning_1.tar/content', 'checksum': 'md5:0a2f551db46a9e629bb1d0a0098ae5cd'}, {'filename': 'edih-adria_1.tar', 'size': '5.86 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/edih-adria_1.tar/content', 'checksum': 'md5:23559bed5a9023398b431777bdc8a126'}, {'filename': 'uarctic_1.tar', 'size': '9.75 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/uarctic_1.tar/content', 'checksum': 'md5:302e3844ebd041c5f4ed94505eb9a285'}, {'filename': 'netherlands_1.tar', 'size': '3.91 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/netherlands_1.tar/content', 'checksum': 'md5:d1416c058b3961483aac340750ea8726'}, {'filename': 'knowmad_1.tar', 'size': '10.08 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/knowmad_1.tar/content', 'checksum': 'md5:a79573a02f2c9a9d65c33b3f3a2eaab9'}, {'filename': 'argo-france.tar', 'size': '0.00 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/argo-france.tar/content', 'checksum': 'md5:2ce6b0fcc6f876b600207759a0dc9758'}, {'filename': 'civica.tar', 'size': '0.23 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/civica.tar/content', 'checksum': 'md5:d2f24bbef06809a91d124f0b07cb1034'}, {'filename': 'covid-19.tar', 'size': '2.03 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/covid-19.tar/content', 'checksum': 'md5:3b741e8138f39932ca6c13ca106fe5d3'}, {'filename': 'aurora.tar', 'size': '1.73 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/aurora.tar/content', 'checksum': 'md5:9b6a8f38cd6f0ce16a85dfc020c220bf'}, {'filename': 'dh-ch.tar', 'size': '1.16 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/dh-ch.tar/content', 'checksum': 'md5:dbebdcc8ad7fd1dc7894fe03ebe2a978'}, {'filename': 'heritage-science.tar', 'size': '0.03 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/heritage-science.tar/content', 'checksum': 'md5:ffd2537b08c58d78eea4bc23a99b3c07'}, {'filename': 'dth.tar', 'size': '0.01 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/dth.tar/content', 'checksum': 'md5:643894810ac8bfce0f8273cf40d05a7a'}, {'filename': 'egrise.tar', 'size': '0.02 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/egrise.tar/content', 'checksum': 'md5:2f52b49fa8bd983bcf6884d6c4f5e952'}, {'filename': 'lifewatch-eric.tar', 'size': '0.05 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/lifewatch-eric.tar/content', 'checksum': 'md5:213c3f1ac83454ec01560d683a26362d'}, {'filename': 'iperionhs.tar', 'size': '0.00 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/iperionhs.tar/content', 'checksum': 'md5:6ffaf325257d9af5e43daa68505b1797'}, {'filename': 'eutopia.tar', 'size': '1.60 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/eutopia.tar/content', 'checksum': 'md5:f9eb5a1bb86caf6a4f2a7563453fc6df'}, {'filename': 'sdsn-gr.tar', 'size': '0.05 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/sdsn-gr.tar/content', 'checksum': 'md5:696f8b509a12c0bc898e1b9040a53790'}, {'filename': 'north-american-studies.tar', 'size': '0.36 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/north-american-studies.tar/content', 'checksum': 'md5:b677758820184053d9c1e6714394dd70'}, {'filename': 'knowmad_3.tar', 'size': '10.06 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/knowmad_3.tar/content', 'checksum': 'md5:68c5839a6355ea26f979ba781bc26a56'}, {'filename': 'knowmad_2.tar', 'size': '10.07 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/knowmad_2.tar/content', 'checksum': 'md5:95442e14703ba5db573cbf12296d76f4'}, {'filename': 'knowmad_5.tar', 'size': '5.37 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/knowmad_5.tar/content', 'checksum': 'md5:35482782eb621df9ec7365d34ccf3d07'}, {'filename': 'knowmad_4.tar', 'size': '10.04 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/knowmad_4.tar/content', 'checksum': 'md5:d770fcd862d5d7d4d918c59f028e24da'}, {'filename': 'beopen.tar', 'size': '0.20 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/beopen.tar/content', 'checksum': 'md5:447dbe25eaedc20a75568fd340f8e25d'}, {'filename': 'dariah.tar', 'size': '0.02 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/dariah.tar/content', 'checksum': 'md5:814e3a79b29da6b605018009f8575a8c'}, {'filename': 'eu-conexus.tar', 'size': '0.18 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/eu-conexus.tar/content', 'checksum': 'md5:19c86e2b79bde505112fb6b3d6e38eef'}, {'filename': 'elixir-gr.tar', 'size': '0.01 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/elixir-gr.tar/content', 'checksum': 'md5:e1dfe593d40c498e31a6ff5132da5fa4'}, {'filename': 'eut.tar', 'size': '0.21 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/eut.tar/content', 'checksum': 'md5:3da085b997006205c694b023aefafe7c'}, {'filename': 'enermaps.tar', 'size': '1.59 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/enermaps.tar/content', 'checksum': 'md5:b157900f8cb97cf4aa4f82ef59e2ff6e'}, {'filename': 'forthem.tar', 'size': '0.91 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/forthem.tar/content', 'checksum': 'md5:7864a0ddb0b676bb053bbcdf12a12525'}, {'filename': 'inria.tar', 'size': '0.27 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/inria.tar/content', 'checksum': 'md5:66a7e88ddda08626cbf26e182f7eafea'}, {'filename': 'mes.tar', 'size': '0.38 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/mes.tar/content', 'checksum': 'md5:10362f805616e391d350b395d9ae30a3'}, {'filename': 'neanias-underwater.tar', 'size': '0.02 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/neanias-underwater.tar/content', 'checksum': 'md5:4422b9a3be4a1e6cad61da46044a0ca2'}, {'filename': 'neanias-atmospheric.tar', 'size': '0.57 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/neanias-atmospheric.tar/content', 'checksum': 'md5:a3b9edbd956aee813cdfe97655c3fc9e'}, {'filename': 'neanias-space.tar', 'size': '0.73 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/neanias-space.tar/content', 'checksum': 'md5:618fbecca154af288dc1c92246b3d73b'}, {'filename': 'rural-digital-europe.tar', 'size': '0.83 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/rural-digital-europe.tar/content', 'checksum': 'md5:6543f5ead539a8bbad04593b641af0a1'}, {'filename': 'tunet.tar', 'size': '0.05 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/tunet.tar/content', 'checksum': 'md5:178b5a944133ded65d741ea5dc2c5990'}, {'filename': 'ni.tar', 'size': '3.01 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/ni.tar/content', 'checksum': 'md5:e82f580135522acd2da7277ea9389718'}]\n",
      "Publication date: 2025-02-19\n",
      "DOI: 10.5281/zenodo.14887484\n",
      "Title: OpenAIRE Graph: Dataset for research communities and initiatives\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Fetch the JSON data from the URL\n",
    "url = \"https://zenodo.org/api/records/14887484/versions/latest\"\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "# Extract the files information\n",
    "files = data.get(\"files\", [])\n",
    "\n",
    "# Create a list of dictionaries for the .tar files\n",
    "tar_files = []\n",
    "for file in files:\n",
    "    if file[\"key\"].endswith(\".tar\"):\n",
    "        tar_files.append({\n",
    "            \"filename\": file[\"key\"],\n",
    "            \"size\": f\"{file['size'] / (1024**3):.2f} GB\",  # Convert bytes to GB\n",
    "            \"downloadlink\": file[\"links\"][\"self\"],\n",
    "            \"checksum\": file[\"checksum\"]\n",
    "        })\n",
    "\n",
    "# print the tar files\n",
    "# If no tar files found, print a message\n",
    "if not tar_files:\n",
    "    print(\"No .tar files found in the dataset.\")\n",
    "else:\n",
    "    print(f\"Found {len(tar_files)} .tar files in the dataset.\")\n",
    "    print(\"Details of .tar files:\")\n",
    "    print(tar_files)\n",
    "\n",
    "# get and print the publication date\n",
    "publication_date = data.get(\"metadata\", {}).get(\"publication_date\", \"Unknown\")\n",
    "print(f\"Publication date: {publication_date}\")\n",
    "# get and print the DOI\n",
    "doi = data.get(\"doi\", \"Unknown\")\n",
    "print(f\"DOI: {doi}\")\n",
    "# get and print the title\n",
    "title = data.get(\"title\", \"Unknown\")\n",
    "print(f\"Title: {title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      filename      size  \\\n",
      "5              argo-france.tar   0.00 GB   \n",
      "8                   aurora.tar   1.73 GB   \n",
      "22                  beopen.tar   0.20 GB   \n",
      "6                   civica.tar   0.23 GB   \n",
      "7                 covid-19.tar   2.03 GB   \n",
      "23                  dariah.tar   0.02 GB   \n",
      "9                    dh-ch.tar   1.16 GB   \n",
      "11                     dth.tar   0.01 GB   \n",
      "1             edih-adria_1.tar   5.86 GB   \n",
      "12                  egrise.tar   0.02 GB   \n",
      "25               elixir-gr.tar   0.01 GB   \n",
      "0        energy-planning_1.tar   6.99 GB   \n",
      "27                enermaps.tar   1.59 GB   \n",
      "24              eu-conexus.tar   0.18 GB   \n",
      "26                     eut.tar   0.21 GB   \n",
      "15                 eutopia.tar   1.60 GB   \n",
      "28                 forthem.tar   0.91 GB   \n",
      "10        heritage-science.tar   0.03 GB   \n",
      "29                   inria.tar   0.27 GB   \n",
      "14               iperionhs.tar   0.00 GB   \n",
      "4                knowmad_1.tar  10.08 GB   \n",
      "19               knowmad_2.tar  10.07 GB   \n",
      "18               knowmad_3.tar  10.06 GB   \n",
      "21               knowmad_4.tar  10.04 GB   \n",
      "20               knowmad_5.tar   5.37 GB   \n",
      "13          lifewatch-eric.tar   0.05 GB   \n",
      "30                     mes.tar   0.38 GB   \n",
      "32     neanias-atmospheric.tar   0.57 GB   \n",
      "33           neanias-space.tar   0.73 GB   \n",
      "31      neanias-underwater.tar   0.02 GB   \n",
      "3            netherlands_1.tar   3.91 GB   \n",
      "36                      ni.tar   3.01 GB   \n",
      "17  north-american-studies.tar   0.36 GB   \n",
      "34    rural-digital-europe.tar   0.83 GB   \n",
      "16                 sdsn-gr.tar   0.05 GB   \n",
      "35                   tunet.tar   0.05 GB   \n",
      "2                uarctic_1.tar   9.75 GB   \n",
      "\n",
      "                                         downloadlink  \\\n",
      "5   https://zenodo.org/api/records/14887484/files/...   \n",
      "8   https://zenodo.org/api/records/14887484/files/...   \n",
      "22  https://zenodo.org/api/records/14887484/files/...   \n",
      "6   https://zenodo.org/api/records/14887484/files/...   \n",
      "7   https://zenodo.org/api/records/14887484/files/...   \n",
      "23  https://zenodo.org/api/records/14887484/files/...   \n",
      "9   https://zenodo.org/api/records/14887484/files/...   \n",
      "11  https://zenodo.org/api/records/14887484/files/...   \n",
      "1   https://zenodo.org/api/records/14887484/files/...   \n",
      "12  https://zenodo.org/api/records/14887484/files/...   \n",
      "25  https://zenodo.org/api/records/14887484/files/...   \n",
      "0   https://zenodo.org/api/records/14887484/files/...   \n",
      "27  https://zenodo.org/api/records/14887484/files/...   \n",
      "24  https://zenodo.org/api/records/14887484/files/...   \n",
      "26  https://zenodo.org/api/records/14887484/files/...   \n",
      "15  https://zenodo.org/api/records/14887484/files/...   \n",
      "28  https://zenodo.org/api/records/14887484/files/...   \n",
      "10  https://zenodo.org/api/records/14887484/files/...   \n",
      "29  https://zenodo.org/api/records/14887484/files/...   \n",
      "14  https://zenodo.org/api/records/14887484/files/...   \n",
      "4   https://zenodo.org/api/records/14887484/files/...   \n",
      "19  https://zenodo.org/api/records/14887484/files/...   \n",
      "18  https://zenodo.org/api/records/14887484/files/...   \n",
      "21  https://zenodo.org/api/records/14887484/files/...   \n",
      "20  https://zenodo.org/api/records/14887484/files/...   \n",
      "13  https://zenodo.org/api/records/14887484/files/...   \n",
      "30  https://zenodo.org/api/records/14887484/files/...   \n",
      "32  https://zenodo.org/api/records/14887484/files/...   \n",
      "33  https://zenodo.org/api/records/14887484/files/...   \n",
      "31  https://zenodo.org/api/records/14887484/files/...   \n",
      "3   https://zenodo.org/api/records/14887484/files/...   \n",
      "36  https://zenodo.org/api/records/14887484/files/...   \n",
      "17  https://zenodo.org/api/records/14887484/files/...   \n",
      "34  https://zenodo.org/api/records/14887484/files/...   \n",
      "16  https://zenodo.org/api/records/14887484/files/...   \n",
      "35  https://zenodo.org/api/records/14887484/files/...   \n",
      "2   https://zenodo.org/api/records/14887484/files/...   \n",
      "\n",
      "                                checksum  \n",
      "5   md5:2ce6b0fcc6f876b600207759a0dc9758  \n",
      "8   md5:9b6a8f38cd6f0ce16a85dfc020c220bf  \n",
      "22  md5:447dbe25eaedc20a75568fd340f8e25d  \n",
      "6   md5:d2f24bbef06809a91d124f0b07cb1034  \n",
      "7   md5:3b741e8138f39932ca6c13ca106fe5d3  \n",
      "23  md5:814e3a79b29da6b605018009f8575a8c  \n",
      "9   md5:dbebdcc8ad7fd1dc7894fe03ebe2a978  \n",
      "11  md5:643894810ac8bfce0f8273cf40d05a7a  \n",
      "1   md5:23559bed5a9023398b431777bdc8a126  \n",
      "12  md5:2f52b49fa8bd983bcf6884d6c4f5e952  \n",
      "25  md5:e1dfe593d40c498e31a6ff5132da5fa4  \n",
      "0   md5:0a2f551db46a9e629bb1d0a0098ae5cd  \n",
      "27  md5:b157900f8cb97cf4aa4f82ef59e2ff6e  \n",
      "24  md5:19c86e2b79bde505112fb6b3d6e38eef  \n",
      "26  md5:3da085b997006205c694b023aefafe7c  \n",
      "15  md5:f9eb5a1bb86caf6a4f2a7563453fc6df  \n",
      "28  md5:7864a0ddb0b676bb053bbcdf12a12525  \n",
      "10  md5:ffd2537b08c58d78eea4bc23a99b3c07  \n",
      "29  md5:66a7e88ddda08626cbf26e182f7eafea  \n",
      "14  md5:6ffaf325257d9af5e43daa68505b1797  \n",
      "4   md5:a79573a02f2c9a9d65c33b3f3a2eaab9  \n",
      "19  md5:95442e14703ba5db573cbf12296d76f4  \n",
      "18  md5:68c5839a6355ea26f979ba781bc26a56  \n",
      "21  md5:d770fcd862d5d7d4d918c59f028e24da  \n",
      "20  md5:35482782eb621df9ec7365d34ccf3d07  \n",
      "13  md5:213c3f1ac83454ec01560d683a26362d  \n",
      "30  md5:10362f805616e391d350b395d9ae30a3  \n",
      "32  md5:a3b9edbd956aee813cdfe97655c3fc9e  \n",
      "33  md5:618fbecca154af288dc1c92246b3d73b  \n",
      "31  md5:4422b9a3be4a1e6cad61da46044a0ca2  \n",
      "3   md5:d1416c058b3961483aac340750ea8726  \n",
      "36  md5:e82f580135522acd2da7277ea9389718  \n",
      "17  md5:b677758820184053d9c1e6714394dd70  \n",
      "34  md5:6543f5ead539a8bbad04593b641af0a1  \n",
      "16  md5:696f8b509a12c0bc898e1b9040a53790  \n",
      "35  md5:178b5a944133ded65d741ea5dc2c5990  \n",
      "2   md5:302e3844ebd041c5f4ed94505eb9a285  \n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame to hold the tar files information for later use.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df_tar_files = pd.DataFrame(tar_files)\n",
    "\n",
    "# Sort the DataFrame by filename alphabetically\n",
    "df_tar_files = df_tar_files.sort_values(by='filename')\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df_tar_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tar files:\n",
      "    index                    filename      size\n",
      "0       5             argo-france.tar   0.00 GB\n",
      "1       8                  aurora.tar   1.73 GB\n",
      "2      22                  beopen.tar   0.20 GB\n",
      "3       6                  civica.tar   0.23 GB\n",
      "4       7                covid-19.tar   2.03 GB\n",
      "5      23                  dariah.tar   0.02 GB\n",
      "6       9                   dh-ch.tar   1.16 GB\n",
      "7      11                     dth.tar   0.01 GB\n",
      "8       1            edih-adria_1.tar   5.86 GB\n",
      "9      12                  egrise.tar   0.02 GB\n",
      "10     25               elixir-gr.tar   0.01 GB\n",
      "11      0       energy-planning_1.tar   6.99 GB\n",
      "12     27                enermaps.tar   1.59 GB\n",
      "13     24              eu-conexus.tar   0.18 GB\n",
      "14     26                     eut.tar   0.21 GB\n",
      "15     15                 eutopia.tar   1.60 GB\n",
      "16     28                 forthem.tar   0.91 GB\n",
      "17     10        heritage-science.tar   0.03 GB\n",
      "18     29                   inria.tar   0.27 GB\n",
      "19     14               iperionhs.tar   0.00 GB\n",
      "20      4               knowmad_1.tar  10.08 GB\n",
      "21     19               knowmad_2.tar  10.07 GB\n",
      "22     18               knowmad_3.tar  10.06 GB\n",
      "23     21               knowmad_4.tar  10.04 GB\n",
      "24     20               knowmad_5.tar   5.37 GB\n",
      "25     13          lifewatch-eric.tar   0.05 GB\n",
      "26     30                     mes.tar   0.38 GB\n",
      "27     32     neanias-atmospheric.tar   0.57 GB\n",
      "28     33           neanias-space.tar   0.73 GB\n",
      "29     31      neanias-underwater.tar   0.02 GB\n",
      "30      3           netherlands_1.tar   3.91 GB\n",
      "31     36                      ni.tar   3.01 GB\n",
      "32     17  north-american-studies.tar   0.36 GB\n",
      "33     34    rural-digital-europe.tar   0.83 GB\n",
      "34     16                 sdsn-gr.tar   0.05 GB\n",
      "35     35                   tunet.tar   0.05 GB\n",
      "36      2               uarctic_1.tar   9.75 GB\n",
      "Selected file: argo-france.tar\n",
      "Download link: https://zenodo.org/api/records/14887484/files/argo-france.tar/content\n",
      "Checksum: md5:2ce6b0fcc6f876b600207759a0dc9758\n"
     ]
    }
   ],
   "source": [
    "# Print a reindexed list of available tar files\n",
    "print(\"Available tar files:\")\n",
    "print(df_tar_files[['filename', 'size']].reset_index())\n",
    "\n",
    "import signal\n",
    "\n",
    "# Function to handle timeout\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutError\n",
    "\n",
    "# Set the timeout handler for the input\n",
    "signal.signal(signal.SIGALRM, timeout_handler)\n",
    "signal.alarm(10)  # Set the timeout to 10 seconds\n",
    "\n",
    "try:\n",
    "    # Ask the user to select a tar file by its index\n",
    "    selected_index = int(input(\"Enter the index of the tar file you want to download: \"))\n",
    "except TimeoutError:\n",
    "    print(\"No response received. Defaulting to index 1.\")\n",
    "    selected_index = 1\n",
    "finally:\n",
    "    signal.alarm(0)  # Disable the alarm\n",
    "\n",
    "# Get the selected tar file's download link and checksum\n",
    "selected_file = df_tar_files.iloc[selected_index]\n",
    "downloadlink = selected_file['downloadlink']\n",
    "checksum = selected_file['checksum']\n",
    "\n",
    "print(f\"Selected file: {selected_file['filename']}\")\n",
    "print(f\"Download link: {downloadlink}\")\n",
    "print(f\"Checksum: {checksum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name: argo-france.tar\n",
      "Download Path File: ./data/2025-02-19/01_input/argo-france.tar\n",
      "Folder Name: argo-france\n",
      "Extraction Path Folder: ./data/2025-02-19/02_extracted/argo-france\n"
     ]
    }
   ],
   "source": [
    "# Path Variables\n",
    "\n",
    "# Extract the file name from the selected file\n",
    "file_name = selected_file['filename']    \n",
    "\n",
    "# Path to save the downloaded tar file using file_name variable\n",
    "download_path = f\"./data/{publication_date}/01_input/{file_name}\"\n",
    "\n",
    "# Create the folder name by removing the .tar extension\n",
    "folder_name = selected_file['filename'].replace('.tar', '')\n",
    "\n",
    "# Path to save the extracted files using the file_name variable without the .tar extension\n",
    "extraction_path = f\"./data/{publication_date}/02_extracted/{folder_name}\"\n",
    "\n",
    "\n",
    "print(f\"File Name: {file_name}\")\n",
    "print(f\"Download Path File: {download_path}\")\n",
    "print(f\"Folder Name: {folder_name}\")\n",
    "print(f\"Extraction Path Folder: {extraction_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the tar file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists: ./data/2025-02-19/01_input/argo-france.tar\n",
      "Download URL: https://zenodo.org/api/records/14887484/files/argo-france.tar/content\n",
      "Download complete: ./data/2025-02-19/01_input/argo-france.tar\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Ensure the directory for the download path exists\n",
    "os.makedirs(os.path.dirname(download_path), exist_ok=True)\n",
    "\n",
    "# Check if the file already exists\n",
    "if not os.path.exists(download_path):\n",
    "    # Get the file size in bytes\n",
    "    file_size_bytes = float(selected_file['size'].split()[0]) * (1024**3)  # Convert GB to bytes\n",
    "    print(f\"Downloading file: {selected_file['filename']} ({selected_file['size']})\")\n",
    "    print(f\"Download URL: {downloadlink}\")\n",
    "    \n",
    "    # Estimate download duration assuming an average speed of 10 MB/s\n",
    "    avg_speed = 10 * (1024**2)  # 10 MB/s in bytes\n",
    "    estimated_duration = file_size_bytes / avg_speed\n",
    "    print(f\"Estimated download time: {estimated_duration:.2f} seconds\")\n",
    "    \n",
    "    # Download the selected tar file\n",
    "    response = requests.get(downloadlink, stream=True)\n",
    "    with open(download_path, 'wb') as file:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            file.write(chunk)\n",
    "else:\n",
    "    print(f\"File already exists: {download_path}\")\n",
    "    print(f\"Download URL: {downloadlink}\")\n",
    "\n",
    "print(f\"Download complete: {download_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checksum verification passed.\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "# Function to calculate the checksum of a file\n",
    "def calculate_checksum(file_path, algorithm):\n",
    "    hash_func = hashlib.new(algorithm)\n",
    "    with open(file_path, 'rb') as f:\n",
    "        while chunk := f.read(8192):\n",
    "            hash_func.update(chunk)\n",
    "    return hash_func.hexdigest()\n",
    "\n",
    "# Extract the checksum algorithm and value\n",
    "checksum_parts = checksum.split(':', 1)\n",
    "checksum_algorithm = checksum_parts[0]\n",
    "expected_checksum = checksum_parts[1]\n",
    "\n",
    "# Calculate the checksum of the downloaded file\n",
    "calculated_checksum = calculate_checksum(download_path, algorithm=checksum_algorithm)\n",
    "\n",
    "# Compare the calculated checksum with the provided checksum\n",
    "if calculated_checksum == expected_checksum:\n",
    "    print(\"Checksum verification passed.\")\n",
    "else:\n",
    "    print(\"Checksum verification failed.\")\n",
    "    print(f\"Expected: {expected_checksum}\")\n",
    "    print(f\"Calculated: {calculated_checksum}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract the tar file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tar file has already been extracted.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if the extraction directory already exists and contains files\n",
    "if os.path.exists(extraction_path) and os.listdir(extraction_path):\n",
    "    print(\"The tar file has already been extracted.\")\n",
    "else:\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(extraction_path, exist_ok=True)\n",
    "\n",
    "    # Extract the tar file in the parent directory of the extraction_path - because the tar file contains a folder structure repeating the name of the tar file\n",
    "    print(f\"Extracting {download_path} to {extraction_path}...\")\n",
    "    parent_extraction_path = os.path.dirname(extraction_path)\n",
    "    with tarfile.open(download_path, 'r') as tar:\n",
    "        if testing_mode:\n",
    "            # Extract only the first 10 files for testing\n",
    "            members = tar.getmembers()[:10]\n",
    "            tar.extractall(path=parent_extraction_path, members=members)\n",
    "            print(\"Extracted only the first 10 files for testing mode.\")\n",
    "        else:\n",
    "            tar.extractall(path=parent_extraction_path)\n",
    "\n",
    "    print(\"Extraction complete.\")\n",
    "    print(f\"Files extracted to: {extraction_path}\")\n",
    "    # print the number of files extracted\n",
    "    extracted_files = os.listdir(extraction_path)\n",
    "    print(f\"Number of files extracted: {len(extracted_files)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 285\n",
      "First 5 files:\n",
      "part-00000-2c0de614-bb18-4931-bd6a-64f101a27baf-c000.json.gz\n",
      "part-00001-2c0de614-bb18-4931-bd6a-64f101a27baf-c000.json.gz\n",
      "part-00002-2c0de614-bb18-4931-bd6a-64f101a27baf-c000.json.gz\n",
      "part-00003-2c0de614-bb18-4931-bd6a-64f101a27baf-c000.json.gz\n",
      "part-00004-2c0de614-bb18-4931-bd6a-64f101a27baf-c000.json.gz\n",
      "                                              filename\n",
      "0    part-00000-2c0de614-bb18-4931-bd6a-64f101a27ba...\n",
      "1    part-00001-2c0de614-bb18-4931-bd6a-64f101a27ba...\n",
      "2    part-00002-2c0de614-bb18-4931-bd6a-64f101a27ba...\n",
      "3    part-00003-2c0de614-bb18-4931-bd6a-64f101a27ba...\n",
      "4    part-00004-2c0de614-bb18-4931-bd6a-64f101a27ba...\n",
      "..                                                 ...\n",
      "280  part-00580-2c0de614-bb18-4931-bd6a-64f101a27ba...\n",
      "281  part-00583-2c0de614-bb18-4931-bd6a-64f101a27ba...\n",
      "282  part-00592-2c0de614-bb18-4931-bd6a-64f101a27ba...\n",
      "283  part-00618-2c0de614-bb18-4931-bd6a-64f101a27ba...\n",
      "284  part-00736-2c0de614-bb18-4931-bd6a-64f101a27ba...\n",
      "\n",
      "[285 rows x 1 columns]\n",
      "DataFrame dimensions: (285, 1)\n",
      "Randomly selected files with full paths for testing:\n",
      "./data/2025-02-19/02_extracted/argo-france/part-00163-2c0de614-bb18-4931-bd6a-64f101a27baf-c000.json.gz\n",
      "./data/2025-02-19/02_extracted/argo-france/part-00273-2c0de614-bb18-4931-bd6a-64f101a27baf-c000.json.gz\n",
      "./data/2025-02-19/02_extracted/argo-france/part-00000-2c0de614-bb18-4931-bd6a-64f101a27baf-c000.json.gz\n",
      "./data/2025-02-19/02_extracted/argo-france/part-00171-2c0de614-bb18-4931-bd6a-64f101a27baf-c000.json.gz\n",
      "./data/2025-02-19/02_extracted/argo-france/part-00341-2c0de614-bb18-4931-bd6a-64f101a27baf-c000.json.gz\n",
      "Random file selected for later use: part-00046-2c0de614-bb18-4931-bd6a-64f101a27baf-c000.json.gz\n",
      "Path to the random file: ./data/2025-02-19/02_extracted/argo-france/part-00046-2c0de614-bb18-4931-bd6a-64f101a27baf-c000.json.gz\n",
      "The random file exists: ./data/2025-02-19/02_extracted/argo-france/part-00046-2c0de614-bb18-4931-bd6a-64f101a27baf-c000.json.gz\n"
     ]
    }
   ],
   "source": [
    "# List the extracted files\n",
    "extracted_files = os.listdir(extraction_path)\n",
    "\n",
    "# if testing_mode is True, limit the number of files to 10 for testing purposes\n",
    "if testing_mode:\n",
    "    extracted_files = extracted_files[:10]\n",
    "\n",
    "# add the path to the extracted files\n",
    "extracted_files_with_path = [os.path.join(extraction_path, file) for file in extracted_files]\n",
    "\n",
    "# count the number of files in the extracted folder\n",
    "num_files = len(extracted_files)\n",
    "print(f\"Number of files: {num_files}\")\n",
    "\n",
    "# print the first 5 files\n",
    "print(\"First 5 files:\")\n",
    "for file in extracted_files[:5]:\n",
    "    print(file) \n",
    "\n",
    "# make a DataFrame for the extracted files\n",
    "df_extracted_files = pd.DataFrame(extracted_files, columns=['filename'])\n",
    "# Sort the DataFrame by filename alphabetically\n",
    "df_extracted_files = df_extracted_files.sort_values(by='filename')\n",
    "# Print the DataFrame\n",
    "print(df_extracted_files)\n",
    "\n",
    "# print the dimensions of the DataFrame\n",
    "print(f\"DataFrame dimensions: {df_extracted_files.shape}\")\n",
    "\n",
    "# print a random 5 files, to be used for testing, and use in a variable for later use\n",
    "import random\n",
    "random_files = random.sample(extracted_files, 5)\n",
    "random_files_with_path = [os.path.join(extraction_path, file) for file in random_files]\n",
    "print(\"Randomly selected files with full paths for testing:\")\n",
    "for file in random_files_with_path:\n",
    "    print(file)\n",
    "\n",
    "# one random file for later use\n",
    "random_file = random.choice(extracted_files)\n",
    "print(f\"Random file selected for later use: {random_file}\")\n",
    "# Define the path to the random file\n",
    "random_file_path = os.path.join(extraction_path, random_file)\n",
    "print(f\"Path to the random file: {random_file_path}\")\n",
    "# Check if the random file exists\n",
    "if os.path.exists(random_file_path):\n",
    "    print(f\"The random file exists: {random_file_path}\")\n",
    "else:\n",
    "    print(f\"The random file does not exist: {random_file_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Get a data sample to generate parquetfile and the SQL schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master file path: ./data/2025-02-19/03_transformed/argo-france/argo-france-master.parquet\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "transformation_folder_path = f\"./data/{publication_date}/03_transformed/{folder_name}\"\n",
    "\n",
    "# Ensure the target directory exists\n",
    "os.makedirs(transformation_folder_path, exist_ok=True)\n",
    "\n",
    "# define and print the target output master file for all extracted files\n",
    "master_file = f\"{transformation_folder_path}/{folder_name}-master.parquet\"\n",
    "print(f\"Master file path: {master_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing extracted files into one master parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master file already exists: ./data/2025-02-19/03_transformed/argo-france/argo-france-master.parquet\n",
      "Do you want to overwrite it? (y/n) [Default: n, timeout 10s]:\n",
      "Using the existing master file.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import signal\n",
    "\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutError\n",
    "\n",
    "# Check if the master file already exists\n",
    "if os.path.exists(master_file):\n",
    "    print(f\"Master file already exists: {master_file}\")\n",
    "    print(\"Do you want to overwrite it? (y/n) [Default: n, timeout 10s]:\")\n",
    "    signal.signal(signal.SIGALRM, timeout_handler)\n",
    "    signal.alarm(10)\n",
    "    try:\n",
    "        user_input = input()\n",
    "        overwrite = user_input.strip().lower() == 'y'\n",
    "    except TimeoutError:\n",
    "        print(\"No response received. Continuing with the existing master file.\")\n",
    "        overwrite = False\n",
    "    finally:\n",
    "        signal.alarm(0)\n",
    "    if not overwrite:\n",
    "        print(\"Using the existing master file.\")\n",
    "    else:\n",
    "        # Overwrite: regenerate the master file\n",
    "        con = duckdb.connect()\n",
    "        file_paths = ','.join(f\"'{file}'\" for file in extracted_files_with_path)\n",
    "        con.sql(f'''\n",
    "            COPY (\n",
    "                SELECT *\n",
    "                FROM read_json([{file_paths}], sample_size=-1, union_by_name=true)\n",
    "            )\n",
    "            TO '{master_file}' (FORMAT parquet, COMPRESSION gzip)\n",
    "        ''')\n",
    "        print(f\"Transformed data saved to: {master_file}\")\n",
    "        print(f\"File size: {os.path.getsize(master_file) / (1024**2):.2f} MB\")\n",
    "        con.close()\n",
    "else:\n",
    "    # Master file does not exist, create it\n",
    "    con = duckdb.connect()\n",
    "    file_paths = ','.join(f\"'{file}'\" for file in extracted_files_with_path)\n",
    "    con.sql(f'''\n",
    "        COPY (\n",
    "            SELECT *\n",
    "            FROM read_json([{file_paths}], sample_size=-1, union_by_name=true)\n",
    "        )\n",
    "        TO '{master_file}' (FORMAT parquet, COMPRESSION gzip)\n",
    "    ''')\n",
    "    print(f\"Transformed data saved to: {master_file}\")\n",
    "    print(f\"File size: {os.path.getsize(master_file) / (1024**2):.2f} MB\")\n",
    "    con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema file path: ./data/2025-02-19/03_transformed/argo-france/argo-france-schema.sql\n",
      "Schema file exists: ./data/2025-02-19/03_transformed/argo-france/argo-france-schema.sql\n",
      "Schema file content:\n",
      "column_name,column_type,null,key,default,extra\n",
      "authors,\"STRUCT(fullName VARCHAR, \"\"name\"\" VARCHAR, pid STRUCT(id STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR), provenance STRUCT(provenance VARCHAR, trust VARCHAR)), rank BIGINT, surname VARCHAR)[]\",YES,,,\n",
      "bestAccessRight,\"STRUCT(code VARCHAR, \"\"label\"\" VARCHAR, scheme VARCHAR)\",YES,,,\n",
      "collectedFrom,\"STRUCT(\"\"key\"\" VARCHAR, \"\"value\"\" VARCHAR)[]\",YES,,,\n",
      "communities,\"STRUCT(code VARCHAR, \"\"label\"\" VARCHAR, provenance STRUCT(provenance VARCHAR, trust VARCHAR)[])[]\",YES,,,\n",
      "container,\"STRUCT(ep VARCHAR, issnOnline VARCHAR, issnPrinted VARCHAR, \"\"name\"\" VARCHAR, sp VARCHAR, vol VARCHAR, edition VARCHAR, iss VARCHAR, issnLinking VARCHAR)\",YES,,,\n",
      "contributors,VARCHAR[],YES,,,\n",
      "countries,\"STRUCT(code VARCHAR, \"\"label\"\" VARCHAR, provenance STRUCT(provenance VARCHAR, trust VARCHAR))[]\",YES,,,\n",
      "coverages,JSON[],YES,,,\n",
      "dateOfCollection,VARCHAR,YES,,,\n",
      "descriptions,VARCHAR[],YES,,,\n",
      "formats,VARCHAR[],YES,,,\n",
      "id,VARCHAR,YES,,,\n",
      "indicators,\"STRUCT(citationImpact STRUCT(citationClass VARCHAR, citationCount DOUBLE, impulse DOUBLE, impulseClass VARCHAR, influence DOUBLE, influenceClass VARCHAR, popularity DOUBLE, popularityClass VARCHAR), usageCounts STRUCT(downloads BIGINT, \"\"views\"\" BIGINT))\",YES,,,\n",
      "instances,\"STRUCT(accessRight STRUCT(code VARCHAR, \"\"label\"\" VARCHAR, openAccessRoute VARCHAR, scheme VARCHAR), alternateIdentifiers STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR)[], collectedFrom STRUCT(\"\"key\"\" VARCHAR, \"\"value\"\" VARCHAR), hostedBy STRUCT(\"\"key\"\" VARCHAR, \"\"value\"\" VARCHAR), license VARCHAR, pids STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR)[], publicationDate DATE, refereed VARCHAR, \"\"type\"\" VARCHAR, urls VARCHAR[], articleProcessingCharge STRUCT(amount VARCHAR, currency VARCHAR))[]\",YES,,,\n",
      "isGreen,BOOLEAN,YES,,,\n",
      "isInDiamondJournal,BOOLEAN,YES,,,\n",
      "language,\"STRUCT(code VARCHAR, \"\"label\"\" VARCHAR)\",YES,,,\n",
      "lastUpdateTimeStamp,BIGINT,YES,,,\n",
      "mainTitle,VARCHAR,YES,,,\n",
      "openAccessColor,VARCHAR,YES,,,\n",
      "organizations,\"STRUCT(id VARCHAR, legalName VARCHAR, pids STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR)[])[]\",YES,,,\n",
      "originalIds,VARCHAR[],YES,,,\n",
      "pids,\"STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR)[]\",YES,,,\n",
      "projects,\"STRUCT(acronym VARCHAR, code VARCHAR, funder STRUCT(fundingStream VARCHAR, jurisdiction VARCHAR, \"\"name\"\" VARCHAR, shortName VARCHAR), id VARCHAR, provenance STRUCT(provenance VARCHAR, trust VARCHAR), title VARCHAR, validated STRUCT(validatedByFunder BOOLEAN, validationDate DATE))[]\",YES,,,\n",
      "publicationDate,DATE,YES,,,\n",
      "publiclyFunded,BOOLEAN,YES,,,\n",
      "publisher,VARCHAR,YES,,,\n",
      "sources,VARCHAR[],YES,,,\n",
      "subjects,\"STRUCT(provenance STRUCT(provenance VARCHAR, trust VARCHAR), subject STRUCT(scheme VARCHAR, \"\"value\"\" VARCHAR))[]\",YES,,,\n",
      "type,VARCHAR,YES,,,\n",
      "subTitle,VARCHAR,YES,,,\n",
      "embargoEndDate,DATE,YES,,,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# schema path\n",
    "schema_file_path = f\"{transformation_folder_path}/{folder_name}-schema.sql\"\n",
    "\n",
    "#print the schema file path\n",
    "print(f\"Schema file path: {schema_file_path}\")\n",
    "\n",
    "duckdb.sql(f'''\n",
    "    COPY (\n",
    "        SELECT *\n",
    "        FROM (DESCRIBE '{master_file}')\n",
    "    )\n",
    "    TO '{schema_file_path}'\n",
    "''')\n",
    "# check if the schema file exists\n",
    "if os.path.exists(schema_file_path):\n",
    "    print(f\"Schema file exists: {schema_file_path}\")\n",
    "else:\n",
    "    print(f\"Schema file does not exist: {schema_file_path}\")\n",
    "# Print the schema file content\n",
    "with open(schema_file_path, 'r') as schema_file:\n",
    "    schema_content = schema_file.read()\n",
    "    print(\"Schema file content:\")\n",
    "    print(schema_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in the Parquet file: 674\n"
     ]
    }
   ],
   "source": [
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Count the number of records in the Parquet file\n",
    "record_count = con.sql(f'''\n",
    "    SELECT COUNT(*)\n",
    "    FROM read_parquet('{master_file}')\n",
    "''').fetchone()[0]\n",
    "\n",
    "# Print the record count\n",
    "print(f\"Number of records in the Parquet file: {record_count}\")\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Random Titles in the Parquet file:\n",
      "Argo DAC profile cookbook\n",
      "Uncertainties in Steric Sea Level Change Estimation During the Satellite Altimeter Era: Concepts and Practices\n",
      "New insights into SMOS Sea Surface Salinity retrievals in the Arctic Ocean.\n",
      "Modeling the intense 2012-2013 dense water formation event in the northwestern Mediterranean Sea: Evaluation with an ensemble simulation approach\n",
      "OSNAP mooring data recovered during MSM54 (data from August 2014 to May 2016)\n",
      "Ocean productivity south of Australia during spring and summer\n",
      "Ocean circulation causes the largest freshening event for 120 years in eastern subpolar North Atlantic\n",
      "BGC-Argo float Deep Chlorophyll Maximum data combined with mesoscale eddies detection from TOEddies algorithm\n",
      "Modelling deep-water formation in the north-west Mediterranean Sea with a new airâ€“sea coupled model: sensitivity to turbulent flux parameterizations\n",
      "Characterizing, modelling and understanding the climate variability of the deep water formation in the North-Western Mediterranean Sea\n",
      "Number of unique titles in the Parquet file: 666\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Query all titles from the Parquet file\n",
    "titles = con.sql(f'''\n",
    "    SELECT mainTitle\n",
    "    FROM read_parquet('{master_file}')\n",
    "''').fetchall()\n",
    "\n",
    "# Select 10 random titles\n",
    "random_titles = random.sample([title[0] for title in titles if title[0]], min(10, len(titles)))\n",
    "\n",
    "print(\"10 Random Titles in the Parquet file:\")\n",
    "for title in random_titles:\n",
    "    print(title)\n",
    "\n",
    "# print the number of unique titles\n",
    "unique_titles = set(title[0] for title in titles if title[0])\n",
    "print(f\"Number of unique titles in the Parquet file: {len(unique_titles)}\")\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Random DOIs:\n",
      "10.1029/2011pa002184\n",
      "10.1029/2011gl050798\n",
      "10.1029/2018jc014394\n",
      "10.1029/2020jc017123\n",
      "10.1002/2016jc011935\n",
      "10.1126/science.aay1790\n",
      "10.5194/os-12-647-2016\n",
      "10.5194/bg-16-1321-2019\n",
      "10.5194/os-15-1489-2019\n",
      "10.5194/bg-2021-201\n",
      "Total number of DOIs: 709\n",
      "Number of unique DOIs: 709\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "# Query to extract DOIs from the pids column\n",
    "dois = con.sql(f'''\n",
    "    SELECT unnest.value AS doi\n",
    "    FROM read_parquet('{master_file}')\n",
    "    CROSS JOIN UNNEST(pids) AS unnest\n",
    "    WHERE unnest.scheme = 'doi'\n",
    "''').fetchall()\n",
    "\n",
    "# Select 10 random DOIs\n",
    "random_dois = random.sample([doi[0] for doi in dois if doi[0]], min(10, len(dois)))\n",
    "\n",
    "# Print the 10 random DOIs\n",
    "print(\"10 Random DOIs:\")\n",
    "for doi in random_dois:\n",
    "    print(doi)\n",
    "\n",
    "# print total number of DOIs\n",
    "print(f\"Total number of DOIs: {len(dois)}\")\n",
    "\n",
    "# print the number of unique DOIs\n",
    "unique_dois = set(doi[0] for doi in dois if doi[0])\n",
    "print(f\"Number of unique DOIs: {len(unique_dois)}\")\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct PID schemes in the master table:\n",
      "doi\n",
      "mag_id\n",
      "arXiv\n",
      "handle\n",
      "pmid\n",
      "pmc\n"
     ]
    }
   ],
   "source": [
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Query to extract distinct PID schemes from the master file\n",
    "pid_schemes = con.sql(f'''\n",
    "    SELECT DISTINCT unnest.scheme AS scheme\n",
    "    FROM read_parquet('{master_file}')\n",
    "    CROSS JOIN UNNEST(pids) AS unnest\n",
    "''').fetchall()\n",
    "\n",
    "# Print the distinct PID schemes\n",
    "print(\"Distinct PID schemes in the master table:\")\n",
    "for scheme in pid_schemes:\n",
    "    print(scheme[0])\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIDs grouped by schemes:\n",
      "   scheme                                               pids\n",
      "0  handle  [20.500.14243/381862, 1871/48380, 1912/27589, ...\n",
      "1    pmid  [31996687, 32978152, 35865129, 31875863, 30659...\n",
      "2     pmc  [PMC6989661, PMC7518875, PMC9287098, PMC691659...\n",
      "3     doi  [10.1175/jpo-d-16-0107.1, 10.1002/2016jc012629...\n",
      "4  mag_id  [2497128190, 2601592524, 2332474861, 205755597...\n",
      "5   arXiv                  [http://arxiv.org/abs/1607.08469]\n"
     ]
    }
   ],
   "source": [
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Query to extract PIDs grouped by their schemes\n",
    "pids_by_scheme = con.sql(f'''\n",
    "    SELECT unnest.scheme AS scheme, LIST(unnest.value) AS pids\n",
    "    FROM read_parquet('{master_file}')\n",
    "    CROSS JOIN UNNEST(pids) AS unnest\n",
    "    GROUP BY unnest.scheme\n",
    "''').fetchall()\n",
    "\n",
    "# dataframe to hold the PIDs grouped by their schemes\n",
    "df_pids_by_scheme = pd.DataFrame(pids_by_scheme, columns=['scheme', 'pids'])\n",
    "# Print the DataFrame of PIDs grouped by their schemes\n",
    "print(\"PIDs grouped by schemes:\")  \n",
    "print(df_pids_by_scheme)\n",
    " \n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geting ready for further processing the master data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create a folder for processed data\n",
    "processing_folder_path = f\"./data/{publication_date}/04_processed/{folder_name}\"\n",
    "\n",
    "# Ensure the target directory exists\n",
    "os.makedirs(processing_folder_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Get the DOI's and other identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined id and pid:\n",
      "                                           record_id pid_scheme  \\\n",
      "0     doi_dedup___::fe2c8a61b8ccaa6515d3c2996b2144c9        doi   \n",
      "1     doi_dedup___::fe2c8a61b8ccaa6515d3c2996b2144c9     mag_id   \n",
      "2     doi_dedup___::7bd738ce5851f7e450ebf6388ad51522        doi   \n",
      "3     doi_dedup___::7bd738ce5851f7e450ebf6388ad51522     mag_id   \n",
      "4     doi_dedup___::7bd738ce5851f7e450ebf6388ad51522     handle   \n",
      "...                                              ...        ...   \n",
      "1392  doi_dedup___::da9603d028d4bd4ad2d9c980917fd5ac        doi   \n",
      "1393  doi_dedup___::3da738372eaf79174655e4ef24d74cd2        doi   \n",
      "1394  doi_dedup___::3da738372eaf79174655e4ef24d74cd2        doi   \n",
      "1395  doi_dedup___::3da738372eaf79174655e4ef24d74cd2        doi   \n",
      "1396  doi_dedup___::3da738372eaf79174655e4ef24d74cd2        doi   \n",
      "\n",
      "                    pid_value  \\\n",
      "0     10.1175/jpo-d-16-0107.1   \n",
      "1                  2497128190   \n",
      "2        10.1002/2016jc012629   \n",
      "3                  2601592524   \n",
      "4         20.500.14243/381862   \n",
      "...                       ...   \n",
      "1392   10.5281/zenodo.1137795   \n",
      "1393   10.5281/zenodo.4009263   \n",
      "1394   10.5281/zenodo.4010160   \n",
      "1395   10.5281/zenodo.6343858   \n",
      "1396   10.5281/zenodo.4009264   \n",
      "\n",
      "                                        combined_id_pid  \n",
      "0     doi_dedup___::fe2c8a61b8ccaa6515d3c2996b2144c9...  \n",
      "1     doi_dedup___::fe2c8a61b8ccaa6515d3c2996b2144c9...  \n",
      "2     doi_dedup___::7bd738ce5851f7e450ebf6388ad51522...  \n",
      "3     doi_dedup___::7bd738ce5851f7e450ebf6388ad51522...  \n",
      "4     doi_dedup___::7bd738ce5851f7e450ebf6388ad51522...  \n",
      "...                                                 ...  \n",
      "1392  doi_dedup___::da9603d028d4bd4ad2d9c980917fd5ac...  \n",
      "1393  doi_dedup___::3da738372eaf79174655e4ef24d74cd2...  \n",
      "1394  doi_dedup___::3da738372eaf79174655e4ef24d74cd2...  \n",
      "1395  doi_dedup___::3da738372eaf79174655e4ef24d74cd2...  \n",
      "1396  doi_dedup___::3da738372eaf79174655e4ef24d74cd2...  \n",
      "\n",
      "[1397 rows x 4 columns]\n",
      "Combined data saved to: ./data/2025-02-19/04_processed/argo-france/argo-france-combined-id-pid.parquet\n",
      "File size: 0.08 MB\n"
     ]
    }
   ],
   "source": [
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Query to combine the id with each pid\n",
    "combined_data = con.sql(f'''\n",
    "    SELECT \n",
    "        id AS record_id,\n",
    "        unnest.scheme AS pid_scheme,\n",
    "        unnest.value AS pid_value,\n",
    "        CONCAT(id, '_', unnest.value) AS combined_id_pid\n",
    "    FROM read_parquet('{master_file}')\n",
    "    CROSS JOIN UNNEST(pids) AS unnest\n",
    "''').fetchdf()\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(\"Combined id and pid:\")\n",
    "print(combined_data)\n",
    "\n",
    "\n",
    "# Save the combined data to a new Parquet file for later use\n",
    "combined_file_path = f\"{processing_folder_path}/{folder_name}-combined-id-pid.parquet\"\n",
    "combined_data.to_parquet(combined_file_path, index=False)\n",
    "print(f\"Combined data saved to: {combined_file_path}\")\n",
    "print(f\"File size: {os.path.getsize(combined_file_path) / (1024**2):.2f} MB\")\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Get Altmetric data\n",
    "\n",
    "a. use the PIDS (df_pids_by_scheme) along with the record id (to be used as primary keys, connecting the tables later on), \n",
    "\n",
    "b. get mention data by parsing the pids over the altmetric API,\n",
    "\n",
    "c. save the outcomes in a separate parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records to be processed: 940\n",
      "Requesting Altmetric count for record doi_dedup___::fe2c8a61b8ccaa6515d3c2996b2144c9: https://api.altmetric.com/v1/doi/10.1175/jpo-d-16-0107.1\n",
      "Record doi_dedup___::fe2c8a61b8ccaa6515d3c2996b2144c9 for doi:10.1175/jpo-d-16-0107.1 returned 404 Not Found: Altmetric doesnâ€™t have any details for the research output or set of research outputs you requested.\n",
      "Requesting Altmetric count for record doi_dedup___::7bd738ce5851f7e450ebf6388ad51522: https://api.altmetric.com/v1/doi/10.1002/2016jc012629\n",
      "Record doi_dedup___::7bd738ce5851f7e450ebf6388ad51522 for doi:10.1002/2016jc012629 returned 200 OK: Data received.\n",
      "  Altmetric score: 8.08\n",
      "Requesting Altmetric count for record doi_dedup___::7bd738ce5851f7e450ebf6388ad51522: https://api.altmetric.com/v1/handle/20.500.14243/381862\n",
      "Record doi_dedup___::7bd738ce5851f7e450ebf6388ad51522 for handle:20.500.14243/381862 returned 404 Not Found: Altmetric doesnâ€™t have any details for the research output or set of research outputs you requested.\n",
      "Requesting Altmetric count for record doi_dedup___::bf1098713a38f89cb9c67a7f59401107: https://api.altmetric.com/v1/doi/10.1080/03091929.2016.1164148\n",
      "Record doi_dedup___::bf1098713a38f89cb9c67a7f59401107 for doi:10.1080/03091929.2016.1164148 returned 404 Not Found: Altmetric doesnâ€™t have any details for the research output or set of research outputs you requested.\n",
      "Requesting Altmetric count for record doi_dedup___::bf2bceb2795ae668b763e656619c8b7f: https://api.altmetric.com/v1/doi/10.1175/jpo-d-13-030.1\n",
      "Record doi_dedup___::bf2bceb2795ae668b763e656619c8b7f for doi:10.1175/jpo-d-13-030.1 returned 200 OK: Data received.\n",
      "  Altmetric score: 0.5\n",
      "Requesting Altmetric count for record doi_dedup___::d010a9bab7e29a2c7f97175b48c9aa84: https://api.altmetric.com/v1/doi/10.5194/os-12-257-2016\n",
      "Record doi_dedup___::d010a9bab7e29a2c7f97175b48c9aa84 for doi:10.5194/os-12-257-2016 returned 200 OK: Data received.\n",
      "  Altmetric score: 1.25\n",
      "Requesting Altmetric count for record doi_dedup___::d010a9bab7e29a2c7f97175b48c9aa84: https://api.altmetric.com/v1/doi/10.5194/osd-12-1145-2015\n",
      "Record doi_dedup___::d010a9bab7e29a2c7f97175b48c9aa84 for doi:10.5194/osd-12-1145-2015 returned 404 Not Found: Altmetric doesnâ€™t have any details for the research output or set of research outputs you requested.\n",
      "Requesting Altmetric count for record doi_dedup___::7a4fa727222eff5f8c33186fd4029b54: https://api.altmetric.com/v1/doi/10.1029/2020gl091649\n",
      "Record doi_dedup___::7a4fa727222eff5f8c33186fd4029b54 for doi:10.1029/2020gl091649 returned 200 OK: Data received.\n",
      "  Altmetric score: 0.25\n",
      "Requesting Altmetric count for record doi_dedup___::c0361a74fd77ddff77b52eddcfa088e8: https://api.altmetric.com/v1/doi/10.1029/2011pa002184\n",
      "Record doi_dedup___::c0361a74fd77ddff77b52eddcfa088e8 for doi:10.1029/2011pa002184 returned 200 OK: Data received.\n",
      "  Altmetric score: 3\n",
      "Requesting Altmetric count for record doi_dedup___::c0361a74fd77ddff77b52eddcfa088e8: https://api.altmetric.com/v1/handle/1871/48380\n",
      "Record doi_dedup___::c0361a74fd77ddff77b52eddcfa088e8 for handle:1871/48380 returned 404 Not Found: Altmetric doesnâ€™t have any details for the research output or set of research outputs you requested.\n",
      "First few rows of the Altmetric results DataFrame:\n",
      "                                        record_id pid_scheme  \\\n",
      "0  doi_dedup___::fe2c8a61b8ccaa6515d3c2996b2144c9        doi   \n",
      "1  doi_dedup___::7bd738ce5851f7e450ebf6388ad51522        doi   \n",
      "2  doi_dedup___::7bd738ce5851f7e450ebf6388ad51522     handle   \n",
      "3  doi_dedup___::bf1098713a38f89cb9c67a7f59401107        doi   \n",
      "4  doi_dedup___::bf2bceb2795ae668b763e656619c8b7f        doi   \n",
      "\n",
      "                       pid_value  \\\n",
      "0        10.1175/jpo-d-16-0107.1   \n",
      "1           10.1002/2016jc012629   \n",
      "2            20.500.14243/381862   \n",
      "3  10.1080/03091929.2016.1164148   \n",
      "4         10.1175/jpo-d-13-030.1   \n",
      "\n",
      "                                      altmetric_data  \n",
      "0                                                 {}  \n",
      "1  {\"title\": \"Bio\\u2010optical anomalies in the w...  \n",
      "2                                                 {}  \n",
      "3                                                 {}  \n",
      "4  {\"title\": \"The Ocean General Circulation near ...  \n",
      "Altmetric data saved to: ./data/2025-02-19/04_processed/argo-france/argo-france-altmetric-data.parquet\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Define your Altmetric API key and base URL\n",
    "altmetric_subscription = \"False\"  # Change to \"True\" if you have a Paid subscription  https://details-page-api-docs.altmetric.com/licensing.html#licensing\n",
    "altmetric_api_key = \"YOUR_API_KEY\"  # Replace with your actual Altmetric API key\n",
    "base_altmetric_url = \"https://api.altmetric.com/v1/\"\n",
    "\n",
    "# Load the combined data (record_id, pid_scheme, pid_value) generated previously\n",
    "combined_data = pd.read_parquet(combined_file_path)\n",
    "\n",
    "# filter out rows where pid_value is NaN or empty\n",
    "combined_data = combined_data[combined_data['pid_value'].notna() & (combined_data['pid_value'] != '')]\n",
    "# Ensure pid_scheme is in lowercase for consistency\n",
    "combined_data['pid_scheme'] = combined_data['pid_scheme'].str.lower()\n",
    "# Ensure pid_value is in lowercase for consistency\n",
    "combined_data['pid_value'] = combined_data['pid_value'].str.lower()\n",
    "# filter out rows where pid_scheme is not in the list of supported schemes\n",
    "supported_schemes = [\n",
    "    'dimensions_publication_id',  # Dimensions Publication Identifier\n",
    "    'doi',                       # Digital Object Identifier\n",
    "    'pmid',                      # PubMed Identifier\n",
    "    'handle',                    # Handle\n",
    "    'arxiv',                     # arXiv Identifier\n",
    "    'ads',                       # ADS Bibcode\n",
    "    'ssrn',                      # Social Science Research Network identifier\n",
    "    'repec',                     # RePEc ID\n",
    "    'isbn',                      # International Standard Book Number\n",
    "    'id',                        # Altmetric Internal Identifier\n",
    "    'nct_id',                    # ClinicalTrials.gov ID\n",
    "    'urn'                        # Uniform Resource Name\n",
    "]\n",
    "combined_data = combined_data[combined_data['pid_scheme'].isin(supported_schemes)]\n",
    "# Print the number of records to be processed\n",
    "print(f\"Number of records to be processed: {len(combined_data)}\")\n",
    "\n",
    "# Set testing_mode to True to only use the first 10 records\n",
    "if testing_mode:\n",
    "    combined_data = combined_data.head(10)\n",
    "\n",
    "# Prepare a list to collect results\n",
    "altmetric_results = []\n",
    "\n",
    "# Define a delay between requests (in seconds) to respect rate limits\n",
    "if altmetric_subscription == \"True\":\n",
    "    delay_between_requests = 0  # No delay for paid subscription\n",
    "else:\n",
    "    delay_between_requests = 1  # 1 second delay for free API\n",
    "    # Intelligent rate limiting based on Altmetric API response headers\n",
    "    if altmetric_subscription != \"True\" and response is not None:\n",
    "        hourly_remaining = response.headers.get(\"X-HourlyRateLimit-Remaining\")\n",
    "        daily_remaining = response.headers.get(\"X-DailyRateLimit-Remaining\")\n",
    "        # If headers are present, adjust delay if close to limit\n",
    "        if hourly_remaining is not None and daily_remaining is not None:\n",
    "            try:\n",
    "                hourly_remaining = int(hourly_remaining)\n",
    "                daily_remaining = int(daily_remaining)\n",
    "                # If less than 10 hourly requests left, slow down to 10 seconds/request\n",
    "                if hourly_remaining < 10:\n",
    "                    delay_between_requests = 10\n",
    "                # If less than 100 daily requests left, slow down to 60 seconds/request\n",
    "                elif daily_remaining < 100:\n",
    "                    delay_between_requests = 60\n",
    "                else:\n",
    "                    delay_between_requests = 1\n",
    "            except Exception:\n",
    "                delay_between_requests = 1\n",
    "        else:\n",
    "            delay_between_requests = 1\n",
    "\n",
    "# Loop over each row in the combined data\n",
    "for idx, row in combined_data.iterrows():\n",
    "    record_id = row['record_id']\n",
    "    pid_scheme = row['pid_scheme']\n",
    "    pid_value = row['pid_value']\n",
    "  \n",
    "    # Construct the API endpoint URL\n",
    "    # Example endpoint: https://api.altmetric.com/v1/doi/10.1038/news.2011.490?key=YOUR_API_KEY\n",
    "    if altmetric_subscription == \"True\":\n",
    "        request_url = f\"{base_altmetric_url}{pid_scheme}/{pid_value}?key={altmetric_api_key}\"\n",
    "    else:\n",
    "        request_url = f\"{base_altmetric_url}{pid_scheme}/{pid_value}\"\n",
    "    print(f\"Requesting Altmetric count for record {record_id}: {request_url}\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(request_url)\n",
    "        # Check for response status 200; otherwise, set result as None\n",
    "        if response.status_code == 200:\n",
    "            result_json = response.json()\n",
    "            print(f\"Record {record_id} for {pid_scheme}:{pid_value} returned 200 OK: Data received.\")\n",
    "            # Optionally, print a summary of the data (e.g., Altmetric score if present)\n",
    "            if isinstance(result_json, dict):\n",
    "                altmetric_score = result_json.get('score')\n",
    "            if altmetric_score is not None:\n",
    "                print(f\"  Altmetric score: {altmetric_score}\")\n",
    "            else:\n",
    "                print(\"  Altmetric score not found in response.\")\n",
    "        else:\n",
    "            if response.status_code == 403:\n",
    "                print(f\"Record {record_id} for {pid_scheme}:{pid_value} returned 403 Forbidden: You arenâ€™t authorized for this call. Some requests require an API key.\")\n",
    "            elif response.status_code == 404:\n",
    "                print(f\"Record {record_id} for {pid_scheme}:{pid_value} returned 404 Not Found: Altmetric doesnâ€™t have any details for the research output or set of research outputs you requested.\")\n",
    "            elif response.status_code == 429:\n",
    "                print(f\"Record {record_id} for {pid_scheme}:{pid_value} returned 429 Too Many Requests: You are being rate limited. If you havenâ€™t already then apply for an API key.\")\n",
    "            elif response.status_code == 502:\n",
    "                print(f\"Record {record_id} for {pid_scheme}:{pid_value} returned 502 Bad Gateway: The API version you are using is currently down for maintenance.\")\n",
    "            else:\n",
    "                print(f\"Record {record_id} for {pid_scheme}:{pid_value} returned status code {response.status_code}\")\n",
    "            result_json = None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing record {record_id} for {pid_scheme}:{pid_value}: {e}\")\n",
    "        result_json = None\n",
    "    \n",
    "    altmetric_results.append({\n",
    "        \"record_id\": record_id,\n",
    "        \"pid_scheme\": pid_scheme,\n",
    "        \"pid_value\": pid_value,\n",
    "        \"altmetric_data\": result_json\n",
    "    })\n",
    "    \n",
    "    time.sleep(delay_between_requests)\n",
    "\n",
    "# Convert the results into a DataFrame and save as a parquet file\n",
    "altmetric_results_df = pd.DataFrame(altmetric_results)\n",
    "\n",
    "import json\n",
    "\n",
    "# Ensure altmetric_data column contains only dicts (replace None, lists, or any other type with {})\n",
    "def sanitize_altmetric_data(x):\n",
    "    # Only allow dicts, everything else (including lists, None, etc.) becomes {}\n",
    "    if isinstance(x, dict):\n",
    "        return x\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "# Convert all altmetric_data to dict, then to JSON string for Parquet compatibility\n",
    "altmetric_results_df['altmetric_data'] = altmetric_results_df['altmetric_data'].apply(sanitize_altmetric_data)\n",
    "altmetric_results_df['altmetric_data'] = altmetric_results_df['altmetric_data'].apply(json.dumps)\n",
    "\n",
    "# print the first few rows of the DataFrame\n",
    "print(\"First few rows of the Altmetric results DataFrame:\")\n",
    "print(altmetric_results_df.head())\n",
    "\n",
    "altmetric_file_path = f\"{processing_folder_path}/{folder_name}-altmetric-data.parquet\"\n",
    "altmetric_results_df.to_parquet(altmetric_file_path, index=False)\n",
    "\n",
    "print(f\"Altmetric data saved to: {altmetric_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Get Overton data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Get SDG classification labels\n",
    "\n",
    "a. Query the abstracts first along with the id (to be used as primary keys, connecting the tables later on), \n",
    "\n",
    "b. get sdg data by parsing the abstracts with more than 100 tokens over an LLM API with system prompt, \n",
    "\n",
    "c. save the outcomes in a separate parquet file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### step 7a: Get the abstracts, including the record id and the number of tokens i nthe abstract\n",
    "\n",
    "Number of tokens are important later on, less then 100 tokens in the abstract deliver low quality SDG classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptions with token counts:\n",
      "                                          record_id  \\\n",
      "0    doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef   \n",
      "1    doi_dedup___::e246801fc9ed25782358bac694517f8f   \n",
      "2    doi_dedup___::fe2c8a61b8ccaa6515d3c2996b2144c9   \n",
      "3    doi_dedup___::7bd738ce5851f7e450ebf6388ad51522   \n",
      "4    doi_dedup___::bf1098713a38f89cb9c67a7f59401107   \n",
      "..                                              ...   \n",
      "650  doi_dedup___::3894f0d63b65411c0d289bf831716e48   \n",
      "651  doi_dedup___::a96d857fd60b818f7bde9aa0c99bfc3f   \n",
      "652  doi_dedup___::551f2ca097a75326cc2e7561f831d38b   \n",
      "653  doi_dedup___::da9603d028d4bd4ad2d9c980917fd5ac   \n",
      "654  doi_dedup___::3da738372eaf79174655e4ef24d74cd2   \n",
      "\n",
      "                                           description  token_count  \n",
      "0    Abstract</jats:title><jats:p>The Black Sea, th...          161  \n",
      "1     The early twenty-first centuryâ€™s warming tren...          247  \n",
      "2    Abstract</jats:title><jats:p>The semienclosed ...          226  \n",
      "3    Abstract</jats:title><jats:p>Identification of...          249  \n",
      "4    This study focuses on the interaction between ...          165  \n",
      "..                                                 ...          ...  \n",
      "650  Development of innovative 3D web based Argo Da...            9  \n",
      "651                      Data Management Plan document            4  \n",
      "652         Report of the 2nd Ocean Observers workshop            7  \n",
      "653  Provides Matlab functions as well as example w...           31  \n",
      "654  Argo is a real-time global ocean in situ obser...          120  \n",
      "\n",
      "[655 rows x 3 columns]\n",
      "Description data saved to: ./data/2025-02-19/04_processed/argo-france/argo-france-descriptions-with-tokens.parquet\n",
      "File size: 0.47 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "# Query to extract the ID, description, remove XML tags, and calculate the number of tokens in the description\n",
    "description_data = con.sql(f'''\n",
    "    SELECT \n",
    "        id AS record_id,\n",
    "        regexp_replace(descriptions[1], '<[^>]+>', '') AS description,  -- Remove XML tags\n",
    "        array_length(split(regexp_replace(descriptions[1], '<[^>]+>', ''), ' ')) AS token_count\n",
    "    FROM read_parquet('{master_file}')\n",
    "    WHERE descriptions IS NOT NULL AND array_length(descriptions) > 0\n",
    "''').fetchdf()\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(\"Descriptions with token counts:\")\n",
    "print(description_data)\n",
    "\n",
    "# Save the data to a new Parquet file for later use\n",
    "description_file_path = f\"{processing_folder_path}/{folder_name}-descriptions-with-tokens.parquet\"\n",
    "description_data.to_parquet(description_file_path, index=False)\n",
    "print(f\"Description data saved to: {description_file_path}\")\n",
    "print(f\"File size: {os.path.getsize(description_file_path) / (1024**2):.2f} MB\")\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 7b-1:  Aurora SDG Classifier\n",
    "In this step we use the Aurora SDG classifier to classify all the abstracts.\n",
    "\n",
    "First we set a test_mode parameter, so that the first 3 abstracts with more than 100 tokens are used. If testing mode is False, then use all abstracts with more than 100 tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed record_id: doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef, SDGs: [{'goal_code': '1', 'goal_name': 'No poverty', 'prediction_score': 0.00683957338}, {'goal_code': '2', 'goal_name': 'Zero hunger', 'prediction_score': 0.00627630949}, {'goal_code': '3', 'goal_name': 'Good health and well-being', 'prediction_score': 0.0135991275}, {'goal_code': '4', 'goal_name': 'Quality Education', 'prediction_score': 0.0116534233}, {'goal_code': '5', 'goal_name': 'Gender equality', 'prediction_score': 0.0107629895}, {'goal_code': '6', 'goal_name': 'Clean water and sanitation', 'prediction_score': 0.0340154171}, {'goal_code': '7', 'goal_name': 'Affordable and clean energy', 'prediction_score': 0.0119103789}, {'goal_code': '8', 'goal_name': 'Decent work and economic growth', 'prediction_score': 0.0123335421}, {'goal_code': '9', 'goal_name': 'Industry, innovation and infrastructure', 'prediction_score': 0.00996026397}, {'goal_code': '10', 'goal_name': 'Reduced inequalities', 'prediction_score': 0.011392504}, {'goal_code': '11', 'goal_name': 'Sustainable cities and communities', 'prediction_score': 0.0179706514}, {'goal_code': '12', 'goal_name': 'Responsible consumption and production', 'prediction_score': 0.0220328867}, {'goal_code': '13', 'goal_name': 'Climate action', 'prediction_score': 0.0219024122}, {'goal_code': '14', 'goal_name': 'Life below water', 'prediction_score': 0.881027758}, {'goal_code': '15', 'goal_name': 'Life in Land', 'prediction_score': 0.0209139884}, {'goal_code': '16', 'goal_name': 'Peace, Justice and strong institutions', 'prediction_score': 0.0105369985}, {'goal_code': '17', 'goal_name': 'Partnerships for the goals', 'prediction_score': 0.00933539867}]\n",
      "Processed record_id: doi_dedup___::e246801fc9ed25782358bac694517f8f, SDGs: [{'goal_code': '1', 'goal_name': 'No poverty', 'prediction_score': 0.00652289391}, {'goal_code': '2', 'goal_name': 'Zero hunger', 'prediction_score': 0.0060801506}, {'goal_code': '3', 'goal_name': 'Good health and well-being', 'prediction_score': 0.0121947229}, {'goal_code': '4', 'goal_name': 'Quality Education', 'prediction_score': 0.0106155574}, {'goal_code': '5', 'goal_name': 'Gender equality', 'prediction_score': 0.00980517268}, {'goal_code': '6', 'goal_name': 'Clean water and sanitation', 'prediction_score': 0.0357260406}, {'goal_code': '7', 'goal_name': 'Affordable and clean energy', 'prediction_score': 0.014480859}, {'goal_code': '8', 'goal_name': 'Decent work and economic growth', 'prediction_score': 0.0119721}, {'goal_code': '9', 'goal_name': 'Industry, innovation and infrastructure', 'prediction_score': 0.010145992}, {'goal_code': '10', 'goal_name': 'Reduced inequalities', 'prediction_score': 0.0115697384}, {'goal_code': '11', 'goal_name': 'Sustainable cities and communities', 'prediction_score': 0.0157734454}, {'goal_code': '12', 'goal_name': 'Responsible consumption and production', 'prediction_score': 0.0224746466}, {'goal_code': '13', 'goal_name': 'Climate action', 'prediction_score': 0.0268632174}, {'goal_code': '14', 'goal_name': 'Life below water', 'prediction_score': 0.870481133}, {'goal_code': '15', 'goal_name': 'Life in Land', 'prediction_score': 0.0175649822}, {'goal_code': '16', 'goal_name': 'Peace, Justice and strong institutions', 'prediction_score': 0.0106834173}, {'goal_code': '17', 'goal_name': 'Partnerships for the goals', 'prediction_score': 0.00932398438}]\n",
      "Processed record_id: doi_dedup___::fe2c8a61b8ccaa6515d3c2996b2144c9, SDGs: [{'goal_code': '1', 'goal_name': 'No poverty', 'prediction_score': 0.00672459602}, {'goal_code': '2', 'goal_name': 'Zero hunger', 'prediction_score': 0.00688648224}, {'goal_code': '3', 'goal_name': 'Good health and well-being', 'prediction_score': 0.0140247941}, {'goal_code': '4', 'goal_name': 'Quality Education', 'prediction_score': 0.0122556388}, {'goal_code': '5', 'goal_name': 'Gender equality', 'prediction_score': 0.0110270381}, {'goal_code': '6', 'goal_name': 'Clean water and sanitation', 'prediction_score': 0.0529985726}, {'goal_code': '7', 'goal_name': 'Affordable and clean energy', 'prediction_score': 0.0133106709}, {'goal_code': '8', 'goal_name': 'Decent work and economic growth', 'prediction_score': 0.0123882}, {'goal_code': '9', 'goal_name': 'Industry, innovation and infrastructure', 'prediction_score': 0.00946700573}, {'goal_code': '10', 'goal_name': 'Reduced inequalities', 'prediction_score': 0.0118457675}, {'goal_code': '11', 'goal_name': 'Sustainable cities and communities', 'prediction_score': 0.0157861412}, {'goal_code': '12', 'goal_name': 'Responsible consumption and production', 'prediction_score': 0.0219151378}, {'goal_code': '13', 'goal_name': 'Climate action', 'prediction_score': 0.0188716352}, {'goal_code': '14', 'goal_name': 'Life below water', 'prediction_score': 0.877728343}, {'goal_code': '15', 'goal_name': 'Life in Land', 'prediction_score': 0.018901974}, {'goal_code': '16', 'goal_name': 'Peace, Justice and strong institutions', 'prediction_score': 0.010047555}, {'goal_code': '17', 'goal_name': 'Partnerships for the goals', 'prediction_score': 0.00888702273}]\n",
      "Processed record_id: doi_dedup___::7bd738ce5851f7e450ebf6388ad51522, SDGs: [{'goal_code': '1', 'goal_name': 'No poverty', 'prediction_score': 0.00678217411}, {'goal_code': '2', 'goal_name': 'Zero hunger', 'prediction_score': 0.00621497631}, {'goal_code': '3', 'goal_name': 'Good health and well-being', 'prediction_score': 0.0123209059}, {'goal_code': '4', 'goal_name': 'Quality Education', 'prediction_score': 0.00958263874}, {'goal_code': '5', 'goal_name': 'Gender equality', 'prediction_score': 0.00914633274}, {'goal_code': '6', 'goal_name': 'Clean water and sanitation', 'prediction_score': 0.0291617513}, {'goal_code': '7', 'goal_name': 'Affordable and clean energy', 'prediction_score': 0.0126612782}, {'goal_code': '8', 'goal_name': 'Decent work and economic growth', 'prediction_score': 0.0109635592}, {'goal_code': '9', 'goal_name': 'Industry, innovation and infrastructure', 'prediction_score': 0.00953891873}, {'goal_code': '10', 'goal_name': 'Reduced inequalities', 'prediction_score': 0.0108856857}, {'goal_code': '11', 'goal_name': 'Sustainable cities and communities', 'prediction_score': 0.0154951513}, {'goal_code': '12', 'goal_name': 'Responsible consumption and production', 'prediction_score': 0.0229218}, {'goal_code': '13', 'goal_name': 'Climate action', 'prediction_score': 0.0370459259}, {'goal_code': '14', 'goal_name': 'Life below water', 'prediction_score': 0.860107183}, {'goal_code': '15', 'goal_name': 'Life in Land', 'prediction_score': 0.019131422}, {'goal_code': '16', 'goal_name': 'Peace, Justice and strong institutions', 'prediction_score': 0.0101343989}, {'goal_code': '17', 'goal_name': 'Partnerships for the goals', 'prediction_score': 0.00944107771}]\n",
      "Processed record_id: doi_dedup___::bf1098713a38f89cb9c67a7f59401107, SDGs: [{'goal_code': '1', 'goal_name': 'No poverty', 'prediction_score': 0.0063097775}, {'goal_code': '2', 'goal_name': 'Zero hunger', 'prediction_score': 0.00583729148}, {'goal_code': '3', 'goal_name': 'Good health and well-being', 'prediction_score': 0.0126335621}, {'goal_code': '4', 'goal_name': 'Quality Education', 'prediction_score': 0.0107073784}, {'goal_code': '5', 'goal_name': 'Gender equality', 'prediction_score': 0.0101864338}, {'goal_code': '6', 'goal_name': 'Clean water and sanitation', 'prediction_score': 0.0381637514}, {'goal_code': '7', 'goal_name': 'Affordable and clean energy', 'prediction_score': 0.013869375}, {'goal_code': '8', 'goal_name': 'Decent work and economic growth', 'prediction_score': 0.0126271546}, {'goal_code': '9', 'goal_name': 'Industry, innovation and infrastructure', 'prediction_score': 0.0103320479}, {'goal_code': '10', 'goal_name': 'Reduced inequalities', 'prediction_score': 0.0112560689}, {'goal_code': '11', 'goal_name': 'Sustainable cities and communities', 'prediction_score': 0.0175408125}, {'goal_code': '12', 'goal_name': 'Responsible consumption and production', 'prediction_score': 0.0216282904}, {'goal_code': '13', 'goal_name': 'Climate action', 'prediction_score': 0.0235070586}, {'goal_code': '14', 'goal_name': 'Life below water', 'prediction_score': 0.874863863}, {'goal_code': '15', 'goal_name': 'Life in Land', 'prediction_score': 0.0185729861}, {'goal_code': '16', 'goal_name': 'Peace, Justice and strong institutions', 'prediction_score': 0.0102155805}, {'goal_code': '17', 'goal_name': 'Partnerships for the goals', 'prediction_score': 0.00965943933}]\n",
      "Processed record_id: doi_dedup___::bf2bceb2795ae668b763e656619c8b7f, SDGs: [{'goal_code': '1', 'goal_name': 'No poverty', 'prediction_score': 0.00670775771}, {'goal_code': '2', 'goal_name': 'Zero hunger', 'prediction_score': 0.00618097186}, {'goal_code': '3', 'goal_name': 'Good health and well-being', 'prediction_score': 0.0140991807}, {'goal_code': '4', 'goal_name': 'Quality Education', 'prediction_score': 0.01149562}, {'goal_code': '5', 'goal_name': 'Gender equality', 'prediction_score': 0.010594964}, {'goal_code': '6', 'goal_name': 'Clean water and sanitation', 'prediction_score': 0.0370538533}, {'goal_code': '7', 'goal_name': 'Affordable and clean energy', 'prediction_score': 0.01577878}, {'goal_code': '8', 'goal_name': 'Decent work and economic growth', 'prediction_score': 0.0120371282}, {'goal_code': '9', 'goal_name': 'Industry, innovation and infrastructure', 'prediction_score': 0.00982818}, {'goal_code': '10', 'goal_name': 'Reduced inequalities', 'prediction_score': 0.0118320882}, {'goal_code': '11', 'goal_name': 'Sustainable cities and communities', 'prediction_score': 0.0174311697}, {'goal_code': '12', 'goal_name': 'Responsible consumption and production', 'prediction_score': 0.0209509134}, {'goal_code': '13', 'goal_name': 'Climate action', 'prediction_score': 0.0251804292}, {'goal_code': '14', 'goal_name': 'Life below water', 'prediction_score': 0.880335689}, {'goal_code': '15', 'goal_name': 'Life in Land', 'prediction_score': 0.0182872415}, {'goal_code': '16', 'goal_name': 'Peace, Justice and strong institutions', 'prediction_score': 0.0108811259}, {'goal_code': '17', 'goal_name': 'Partnerships for the goals', 'prediction_score': 0.0094015}]\n",
      "Processed record_id: doi_dedup___::d010a9bab7e29a2c7f97175b48c9aa84, SDGs: [{'goal_code': '1', 'goal_name': 'No poverty', 'prediction_score': 0.00714117289}, {'goal_code': '2', 'goal_name': 'Zero hunger', 'prediction_score': 0.00610893965}, {'goal_code': '3', 'goal_name': 'Good health and well-being', 'prediction_score': 0.0147537887}, {'goal_code': '4', 'goal_name': 'Quality Education', 'prediction_score': 0.0122347176}, {'goal_code': '5', 'goal_name': 'Gender equality', 'prediction_score': 0.0109245479}, {'goal_code': '6', 'goal_name': 'Clean water and sanitation', 'prediction_score': 0.0328349471}, {'goal_code': '7', 'goal_name': 'Affordable and clean energy', 'prediction_score': 0.0132573247}, {'goal_code': '8', 'goal_name': 'Decent work and economic growth', 'prediction_score': 0.0129851103}, {'goal_code': '9', 'goal_name': 'Industry, innovation and infrastructure', 'prediction_score': 0.0100620389}, {'goal_code': '10', 'goal_name': 'Reduced inequalities', 'prediction_score': 0.0117383}, {'goal_code': '11', 'goal_name': 'Sustainable cities and communities', 'prediction_score': 0.0172130167}, {'goal_code': '12', 'goal_name': 'Responsible consumption and production', 'prediction_score': 0.0211188793}, {'goal_code': '13', 'goal_name': 'Climate action', 'prediction_score': 0.0229455829}, {'goal_code': '14', 'goal_name': 'Life below water', 'prediction_score': 0.879254}, {'goal_code': '15', 'goal_name': 'Life in Land', 'prediction_score': 0.0183603466}, {'goal_code': '16', 'goal_name': 'Peace, Justice and strong institutions', 'prediction_score': 0.0110609531}, {'goal_code': '17', 'goal_name': 'Partnerships for the goals', 'prediction_score': 0.0096603334}]\n",
      "Processed record_id: doi_dedup___::024e02533911ba7f1bcb52de1e16495c, SDGs: [{'goal_code': '1', 'goal_name': 'No poverty', 'prediction_score': 0.00695750117}, {'goal_code': '2', 'goal_name': 'Zero hunger', 'prediction_score': 0.00633755326}, {'goal_code': '3', 'goal_name': 'Good health and well-being', 'prediction_score': 0.0134236515}, {'goal_code': '4', 'goal_name': 'Quality Education', 'prediction_score': 0.0118724108}, {'goal_code': '5', 'goal_name': 'Gender equality', 'prediction_score': 0.0104109347}, {'goal_code': '6', 'goal_name': 'Clean water and sanitation', 'prediction_score': 0.0320121348}, {'goal_code': '7', 'goal_name': 'Affordable and clean energy', 'prediction_score': 0.0128987432}, {'goal_code': '8', 'goal_name': 'Decent work and economic growth', 'prediction_score': 0.0125187635}, {'goal_code': '9', 'goal_name': 'Industry, innovation and infrastructure', 'prediction_score': 0.0101955235}, {'goal_code': '10', 'goal_name': 'Reduced inequalities', 'prediction_score': 0.011863023}, {'goal_code': '11', 'goal_name': 'Sustainable cities and communities', 'prediction_score': 0.0170882642}, {'goal_code': '12', 'goal_name': 'Responsible consumption and production', 'prediction_score': 0.0223079026}, {'goal_code': '13', 'goal_name': 'Climate action', 'prediction_score': 0.0264196098}, {'goal_code': '14', 'goal_name': 'Life below water', 'prediction_score': 0.881905854}, {'goal_code': '15', 'goal_name': 'Life in Land', 'prediction_score': 0.0193151832}, {'goal_code': '16', 'goal_name': 'Peace, Justice and strong institutions', 'prediction_score': 0.0109666586}, {'goal_code': '17', 'goal_name': 'Partnerships for the goals', 'prediction_score': 0.00980708}]\n",
      "Processed record_id: doi_dedup___::f56993cc50f407e192d0001e44e4b563, SDGs: [{'goal_code': '1', 'goal_name': 'No poverty', 'prediction_score': 0.00664180517}, {'goal_code': '2', 'goal_name': 'Zero hunger', 'prediction_score': 0.00437763333}, {'goal_code': '3', 'goal_name': 'Good health and well-being', 'prediction_score': 0.00730240345}, {'goal_code': '4', 'goal_name': 'Quality Education', 'prediction_score': 0.0049059689}, {'goal_code': '5', 'goal_name': 'Gender equality', 'prediction_score': 0.0042873323}, {'goal_code': '6', 'goal_name': 'Clean water and sanitation', 'prediction_score': 0.0174248219}, {'goal_code': '7', 'goal_name': 'Affordable and clean energy', 'prediction_score': 0.0431557298}, {'goal_code': '8', 'goal_name': 'Decent work and economic growth', 'prediction_score': 0.00918406248}, {'goal_code': '9', 'goal_name': 'Industry, innovation and infrastructure', 'prediction_score': 0.0193417966}, {'goal_code': '10', 'goal_name': 'Reduced inequalities', 'prediction_score': 0.011061281}, {'goal_code': '11', 'goal_name': 'Sustainable cities and communities', 'prediction_score': 0.0214499831}, {'goal_code': '12', 'goal_name': 'Responsible consumption and production', 'prediction_score': 0.0219768584}, {'goal_code': '13', 'goal_name': 'Climate action', 'prediction_score': 0.312328517}, {'goal_code': '14', 'goal_name': 'Life below water', 'prediction_score': 0.347239912}, {'goal_code': '15', 'goal_name': 'Life in Land', 'prediction_score': 0.0105567276}, {'goal_code': '16', 'goal_name': 'Peace, Justice and strong institutions', 'prediction_score': 0.0104779005}, {'goal_code': '17', 'goal_name': 'Partnerships for the goals', 'prediction_score': 0.0186977983}]\n",
      "Processed record_id: doi_dedup___::6d01ec16c6ac92e8f2ae61b5fad6712b, SDGs: [{'goal_code': '1', 'goal_name': 'No poverty', 'prediction_score': 0.00580260158}, {'goal_code': '2', 'goal_name': 'Zero hunger', 'prediction_score': 0.00641900301}, {'goal_code': '3', 'goal_name': 'Good health and well-being', 'prediction_score': 0.010175854}, {'goal_code': '4', 'goal_name': 'Quality Education', 'prediction_score': 0.00827544928}, {'goal_code': '5', 'goal_name': 'Gender equality', 'prediction_score': 0.00811800361}, {'goal_code': '6', 'goal_name': 'Clean water and sanitation', 'prediction_score': 0.0375110805}, {'goal_code': '7', 'goal_name': 'Affordable and clean energy', 'prediction_score': 0.0136908293}, {'goal_code': '8', 'goal_name': 'Decent work and economic growth', 'prediction_score': 0.0105186403}, {'goal_code': '9', 'goal_name': 'Industry, innovation and infrastructure', 'prediction_score': 0.0104342103}, {'goal_code': '10', 'goal_name': 'Reduced inequalities', 'prediction_score': 0.00921034813}, {'goal_code': '11', 'goal_name': 'Sustainable cities and communities', 'prediction_score': 0.0150004923}, {'goal_code': '12', 'goal_name': 'Responsible consumption and production', 'prediction_score': 0.0268063843}, {'goal_code': '13', 'goal_name': 'Climate action', 'prediction_score': 0.0379301}, {'goal_code': '14', 'goal_name': 'Life below water', 'prediction_score': 0.856715083}, {'goal_code': '15', 'goal_name': 'Life in Land', 'prediction_score': 0.0201815367}, {'goal_code': '16', 'goal_name': 'Peace, Justice and strong institutions', 'prediction_score': 0.0090097487}, {'goal_code': '17', 'goal_name': 'Partnerships for the goals', 'prediction_score': 0.00905320048}]\n",
      "SDG classification results:\n",
      "                                        record_id top_predicted_sdgs\n",
      "0  doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef               [14]\n",
      "1  doi_dedup___::e246801fc9ed25782358bac694517f8f               [14]\n",
      "2  doi_dedup___::fe2c8a61b8ccaa6515d3c2996b2144c9               [14]\n",
      "3  doi_dedup___::7bd738ce5851f7e450ebf6388ad51522               [14]\n",
      "4  doi_dedup___::bf1098713a38f89cb9c67a7f59401107               [14]\n",
      "5  doi_dedup___::bf2bceb2795ae668b763e656619c8b7f               [14]\n",
      "6  doi_dedup___::d010a9bab7e29a2c7f97175b48c9aa84               [14]\n",
      "7  doi_dedup___::024e02533911ba7f1bcb52de1e16495c               [14]\n",
      "8  doi_dedup___::f56993cc50f407e192d0001e44e4b563           [13, 14]\n",
      "9  doi_dedup___::6d01ec16c6ac92e8f2ae61b5fad6712b               [14]\n",
      "SDG classification results saved to: ./data/2025-02-19/04_processed/argo-france/argo-france-sdg-results-aurora-sdg-multi.parquet\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Set the testing mode to True for limited processing\n",
    "testing_mode = True\n",
    "\n",
    "# define the models\n",
    "model = \"aurora-sdg-multi\"  # Use the multi-label model for SDG classification (faster, Aurora definition of SDG's, 104 languages)\n",
    "\n",
    "# other available models:\n",
    "# model = \"aurora-sdg\"  # Use the single-label model for classification of each SDG in the Aurora definition (slower, Aurora definition of SDG's, 104 languages)\n",
    "# model = \"elsevier-multi\"  # Elsevier SDG multi-label mBERT model (fast, Elsevier definition of SDG's, 104 languages)\n",
    "# model = \"osdg\"  # OSDG model (alternative, OSDG definition of SDG's, 15 languages)\n",
    "\n",
    "# Set the base URL for the Aurora SDG classifier\n",
    "base_url = \"https://aurora-sdg.labs.vu.nl/classifier/classify/\" + model\n",
    "\n",
    "# Load the descriptions with token counts\n",
    "description_df = pd.read_parquet(f\"{processing_folder_path}/{folder_name}-descriptions-with-tokens.parquet\")\n",
    "\n",
    "# Filter abstracts with at least 100 tokens\n",
    "description_df = description_df[description_df['token_count'] >= 100]\n",
    "\n",
    "# Set testing mode to limit the number of abstracts\n",
    "if testing_mode:\n",
    "    description_df = description_df.head(10)  # Limit to 10 records for testing\n",
    "\n",
    "# Prepare results list\n",
    "sdg_results = []\n",
    "\n",
    "# Rate limit settings\n",
    "rate_limit = 5  # 5 requests per second\n",
    "delay_between_requests = 1 / rate_limit\n",
    "\n",
    "# Loop through each abstract\n",
    "for idx, row in description_df.iterrows():\n",
    "    record_id = row['record_id']\n",
    "    abstract = row['description']\n",
    "\n",
    "    # Prepare the payload for the API\n",
    "    payload = json.dumps({\"text\": abstract})\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "    try:\n",
    "        # Make the API call\n",
    "        response = requests.post(base_url, headers=headers, data=payload)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the response\n",
    "        result = response.json()\n",
    "        predictions = result.get(\"predictions\", [])\n",
    "\n",
    "        # Extract SDG predictions\n",
    "        sdgs = [\n",
    "            {\n",
    "                \"goal_code\": pred[\"sdg\"][\"code\"],\n",
    "                \"goal_name\": pred[\"sdg\"][\"name\"],\n",
    "                \"prediction_score\": pred[\"prediction\"]\n",
    "            }\n",
    "            for pred in predictions\n",
    "        ]\n",
    "\n",
    "        # Append the result to the list\n",
    "        sdg_results.append({\n",
    "            \"record_id\": record_id,\n",
    "            \"abstract\": abstract,\n",
    "            \"sdgs\": sdgs\n",
    "        })\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Processed record_id: {record_id}, SDGs: {sdgs}\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error processing record_id {record_id}: {e}\")\n",
    "\n",
    "    # Add a delay to respect the rate limit\n",
    "    time.sleep(delay_between_requests)\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "sdg_results_df = pd.DataFrame(sdg_results)\n",
    "\n",
    "# calculate the 90th percentile of the prediction scores for each SDG\n",
    "sdg_scores = []\n",
    "for sdg in sdg_results_df['sdgs']:\n",
    "    for prediction in sdg:\n",
    "        sdg_scores.append(prediction['prediction_score'])  \n",
    "# Calculate the 90th percentile\n",
    "percentile_90 = pd.Series(sdg_scores).quantile(0.9)\n",
    "# Filter the results and append a column top_predicted_sdgs, to include only SDGs (as list of goal_codes) with a prediction score above the 90th percentile\n",
    "sdg_results_df['top_predicted_sdgs'] = sdg_results_df['sdgs'].apply(\n",
    "    lambda x: [sdg['goal_code'] for sdg in x if sdg['prediction_score'] >= percentile_90 and sdg['prediction_score'] > 0.1]\n",
    ")\n",
    "\n",
    "# Print the DataFrame with SDG results\n",
    "print(\"SDG classification results:\")\n",
    "print(sdg_results_df[['record_id', 'top_predicted_sdgs']])\n",
    "\n",
    "# Save the results to a Parquet file including the top predicted SDGs\n",
    "sdg_results_path = f\"{processing_folder_path}/{folder_name}-sdg-results-{model}.parquet\"\n",
    "sdg_results_df.to_parquet(sdg_results_path, index=False)\n",
    "print(f\"SDG classification results saved to: {sdg_results_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### step 7b Get the official definitions of the SDG's from https://metadata.un.org/sdg/ using the Accept header application/rdf+xml\n",
    "\n",
    "First we get the links to the top level goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDG definitions saved to: ./data/2025-02-19/04_processed/argo-france/sdg_definitions.rdf\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL for the SDG metadata\n",
    "sdg_metadata_url = \"https://metadata.un.org/sdg/\"\n",
    "\n",
    "# Set the headers to request RDF/XML format\n",
    "headers = {\n",
    "    \"Accept\": \"application/rdf+xml\"\n",
    "}\n",
    "\n",
    "# Send the GET request\n",
    "response = requests.get(sdg_metadata_url, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Save the RDF/XML content to a file\n",
    "    rdf_file_path = f\"{processing_folder_path}/sdg_definitions.rdf\"\n",
    "    with open(rdf_file_path, \"wb\") as rdf_file:\n",
    "        rdf_file.write(response.content)\n",
    "    print(f\"SDG definitions saved to: {rdf_file_path}\")\n",
    "else:\n",
    "    print(f\"Failed to fetch SDG definitions. Status code: {response.status_code}\")\n",
    "    print(f\"Response: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top concept URLs found in the RDF/XML:\n",
      "http://metadata.un.org/sdg/1\n",
      "http://metadata.un.org/sdg/2\n",
      "http://metadata.un.org/sdg/3\n",
      "http://metadata.un.org/sdg/4\n",
      "http://metadata.un.org/sdg/5\n",
      "http://metadata.un.org/sdg/6\n",
      "http://metadata.un.org/sdg/7\n",
      "http://metadata.un.org/sdg/8\n",
      "http://metadata.un.org/sdg/9\n",
      "http://metadata.un.org/sdg/10\n",
      "http://metadata.un.org/sdg/11\n",
      "http://metadata.un.org/sdg/12\n",
      "http://metadata.un.org/sdg/13\n",
      "http://metadata.un.org/sdg/14\n",
      "http://metadata.un.org/sdg/15\n",
      "http://metadata.un.org/sdg/16\n",
      "http://metadata.un.org/sdg/17\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Parse the RDF/XML file\n",
    "tree = ET.parse(rdf_file_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "# Find all skos:hasTopConcept elements and extract their rdf:resource attribute\n",
    "top_concept_urls = []\n",
    "for elem in root.findall('.//{http://www.w3.org/2004/02/skos/core#}hasTopConcept'):\n",
    "    url = elem.attrib.get('{http://www.w3.org/1999/02/22-rdf-syntax-ns#}resource')\n",
    "    if url:\n",
    "        top_concept_urls.append(url)\n",
    "\n",
    "# sort the URLs based on the integer in the last part of the URL\n",
    "top_concept_urls.sort(key=lambda x: int(x.split('/')[-1]))\n",
    "\n",
    "print(\"Top concept URLs found in the RDF/XML:\")\n",
    "for url in top_concept_urls:\n",
    "    print(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we get the goal number, goal name and goal description for each top level goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   goal_number                               goal_title  \\\n",
      "0            1                               No poverty   \n",
      "1            2                              Zero hunger   \n",
      "2            3               Good health and well-being   \n",
      "3            4                        Quality education   \n",
      "4            5                          Gender equality   \n",
      "5            6               Clean water and sanitation   \n",
      "6            7              Affordable and clean energy   \n",
      "7            8          Decent work and economic growth   \n",
      "8            9  Industry, innovation and infrastructure   \n",
      "9           10                     Reduced inequalities   \n",
      "10          11       Sustainable cities and communities   \n",
      "11          12   Responsible consumption and production   \n",
      "12          13                           Climate action   \n",
      "13          14                         Life below water   \n",
      "14          15                             Life on land   \n",
      "15          16   Peace, justice and strong institutions   \n",
      "16          17               Partnerships for the goals   \n",
      "\n",
      "                                     goal_description  \\\n",
      "0             End poverty in all its forms everywhere   \n",
      "1   End hunger, achieve food security and improved...   \n",
      "2   Ensure healthy lives and promote well-being fo...   \n",
      "3   Ensure inclusive and equitable quality educati...   \n",
      "4   Achieve gender equality and empower all women ...   \n",
      "5   Ensure availability and sustainable management...   \n",
      "6   Ensure access to affordable, reliable, sustain...   \n",
      "7   Promote sustained, inclusive and sustainable e...   \n",
      "8   Build resilient infrastructure, promote inclus...   \n",
      "9        Reduce inequality within and among countries   \n",
      "10  Make cities and human settlements inclusive, s...   \n",
      "11  Ensure sustainable consumption and production ...   \n",
      "12  Take urgent action to combat climate change an...   \n",
      "13  Conserve and sustainably use the oceans, seas ...   \n",
      "14  Protect, restore and promote sustainable use o...   \n",
      "15  Promote peaceful and inclusive societies for s...   \n",
      "16  Strengthen the means of implementation and rev...   \n",
      "\n",
      "                         goal_url  \n",
      "0    http://metadata.un.org/sdg/1  \n",
      "1    http://metadata.un.org/sdg/2  \n",
      "2    http://metadata.un.org/sdg/3  \n",
      "3    http://metadata.un.org/sdg/4  \n",
      "4    http://metadata.un.org/sdg/5  \n",
      "5    http://metadata.un.org/sdg/6  \n",
      "6    http://metadata.un.org/sdg/7  \n",
      "7    http://metadata.un.org/sdg/8  \n",
      "8    http://metadata.un.org/sdg/9  \n",
      "9   http://metadata.un.org/sdg/10  \n",
      "10  http://metadata.un.org/sdg/11  \n",
      "11  http://metadata.un.org/sdg/12  \n",
      "12  http://metadata.un.org/sdg/13  \n",
      "13  http://metadata.un.org/sdg/14  \n",
      "14  http://metadata.un.org/sdg/15  \n",
      "15  http://metadata.un.org/sdg/16  \n",
      "16  http://metadata.un.org/sdg/17  \n",
      "SDG goals saved to: ./data/2025-02-19/04_processed/argo-france/sdg_goals.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Prepare lists to store the results\n",
    "goal_codes = []\n",
    "goal_names = []\n",
    "goal_descriptions = []\n",
    "goal_urls = []\n",
    "\n",
    "# Loop through each top concept URL\n",
    "for url in top_concept_urls:\n",
    "    try:\n",
    "        # Fetch the RDF/XML content\n",
    "        resp = requests.get(url, headers={\"Accept\": \"application/rdf+xml\"})\n",
    "        resp.raise_for_status()\n",
    "        root = ET.fromstring(resp.content)\n",
    "        # Find the main Description element\n",
    "        desc = root.find('.//{http://www.w3.org/1999/02/22-rdf-syntax-ns#}Description')\n",
    "        if desc is None:\n",
    "            continue\n",
    "        # Extract <skos:note xml:lang=\"en\">Goal N</skos:note>\n",
    "        goal_code = None\n",
    "        for note in desc.findall('{http://www.w3.org/2004/02/skos/core#}note'):\n",
    "            if note.attrib.get('{http://www.w3.org/XML/1998/namespace}lang') == 'en' and note.text and note.text.startswith('Goal'):\n",
    "                goal_code = note.text.replace('Goal ', '').strip()\n",
    "                break\n",
    "        # Extract <skos:altLabel xml:lang=\"en\">...</skos:altLabel>\n",
    "        goal_name = None\n",
    "        for alt in desc.findall('{http://www.w3.org/2004/02/skos/core#}altLabel'):\n",
    "            if alt.attrib.get('{http://www.w3.org/XML/1998/namespace}lang') == 'en':\n",
    "                goal_name = alt.text.strip()\n",
    "                break\n",
    "        # Extract <skos:prefLabel xml:lang=\"en\">...</skos:prefLabel>\n",
    "        goal_description = None\n",
    "        for pref in desc.findall('{http://www.w3.org/2004/02/skos/core#}prefLabel'):\n",
    "            if pref.attrib.get('{http://www.w3.org/XML/1998/namespace}lang') == 'en':\n",
    "                goal_description = pref.text.strip()\n",
    "                break\n",
    "        # Store results\n",
    "        goal_codes.append(goal_code)\n",
    "        goal_names.append(goal_name)\n",
    "        goal_descriptions.append(goal_description)\n",
    "        goal_urls.append(url)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {e}\")\n",
    "\n",
    "# Create DataFrame\n",
    "df_sdg_goals = pd.DataFrame({\n",
    "    \"goal_code\": goal_codes,\n",
    "    \"goal_name\": goal_names,\n",
    "    \"goal_description\": goal_descriptions,\n",
    "    \"goal_url\": goal_urls\n",
    "})\n",
    "\n",
    "print(df_sdg_goals)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "sdg_goals_csv_path = f\"{processing_folder_path}/sdg_goals.csv\"\n",
    "df_sdg_goals.to_csv(sdg_goals_csv_path, index=False)\n",
    "print(f\"SDG goals saved to: {sdg_goals_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 7c Here we prepare the System and User prompts to be used by an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text to classify:\n",
      "\n",
      "The United Nations Sustainable Development Goals (SDGs) are a universal call to action to end poverty, protect the planet, and ensure prosperity for all by 2030. They address global challenges such as inequality, climate change, environmental degradation, peace, and justice. The SDGs consist of 17 goals and 169 targets that aim to achieve a better and more sustainable future for all.\n",
      "\n",
      "Example Output Format:\n",
      "\n",
      "{\n",
      "    \"sdgs\": [2, 6, 17],\n",
      "    \"explanation\": \"This text is related to SDG 2 (Zero hunger) because it discusses food security, SDG 6 (Clean water and sanitation) due to references to environmental protection, and SDG 17 (Partnerships for the goals) as it mentions global cooperation.\"\n",
      "}\n",
      "\n",
      "System Prompt:\n",
      "\n",
      "You are an intelligent multi-label classification system designed to map texts to their relevant Sustainable Development Goals.\n",
      "Take the text delimited by triple quotation marks and return a JSON list of relevant SDGs. \n",
      "Example output format: \n",
      "{\n",
      "    \"sdgs\": [2, 6, 17],\n",
      "    \"explanation\": \"This text is related to SDG 2 (Zero hunger) because it discusses food security, SDG 6 (Clean water and sanitation) due to references to environmental protection, and SDG 17 (Partnerships for the goals) as it mentions global cooperation.\"\n",
      "}\n",
      "\n",
      "\n",
      "Here are the SDG goals and their descriptions:\n",
      "1: No poverty - End poverty in all its forms everywhere\n",
      "2: Zero hunger - End hunger, achieve food security and improved nutrition and promote sustainable agriculture\n",
      "3: Good health and well-being - Ensure healthy lives and promote well-being for all at all ages\n",
      "4: Quality education - Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all\n",
      "5: Gender equality - Achieve gender equality and empower all women and girls\n",
      "6: Clean water and sanitation - Ensure availability and sustainable management of water and sanitation for all\n",
      "7: Affordable and clean energy - Ensure access to affordable, reliable, sustainable and modern energy for all\n",
      "8: Decent work and economic growth - Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all\n",
      "9: Industry, innovation and infrastructure - Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation\n",
      "10: Reduced inequalities - Reduce inequality within and among countries\n",
      "11: Sustainable cities and communities - Make cities and human settlements inclusive, safe, resilient and sustainable\n",
      "12: Responsible consumption and production - Ensure sustainable consumption and production patterns\n",
      "13: Climate action - Take urgent action to combat climate change and its impacts\n",
      "14: Life below water - Conserve and sustainably use the oceans, seas and marine resources for sustainable development\n",
      "15: Life on land - Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss\n",
      "16: Peace, justice and strong institutions - Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels\n",
      "17: Partnerships for the goals - Strengthen the means of implementation and revitalize the Global Partnership for Sustainable Development\n",
      "\n",
      "\n",
      "User Prompt:\n",
      "\n",
      "\"Classify the following text in terms of its relevance to the Sustainable Development Goals:\",\n",
      "Text: '''\n",
      "The United Nations Sustainable Development Goals (SDGs) are a universal call to action to end poverty, protect the planet, and ensure prosperity for all by 2030. They address global challenges such as inequality, climate change, environmental degradation, peace, and justice. The SDGs consist of 17 goals and 169 targets that aim to achieve a better and more sustainable future for all.\n",
      "'''\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the text to classify\n",
    "text = \"\"\"\n",
    "The United Nations Sustainable Development Goals (SDGs) are a universal call to action to end poverty, protect the planet, and ensure prosperity for all by 2030. They address global challenges such as inequality, climate change, environmental degradation, peace, and justice. The SDGs consist of 17 goals and 169 targets that aim to achieve a better and more sustainable future for all.\n",
    "\"\"\"\n",
    "# Print the text to classify\n",
    "print(\"Text to classify:\")\n",
    "print(text)\n",
    "\n",
    "# Define the expected output format, now including an explanation field\n",
    "example_output_format = \"\"\"\n",
    "{\n",
    "    \"sdgs\": [2, 6, 17],\n",
    "    \"explanation\": \"This text is related to SDG 2 (Zero hunger) because it discusses food security, SDG 6 (Clean water and sanitation) due to references to environmental protection, and SDG 17 (Partnerships for the goals) as it mentions global cooperation.\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Print the example output format\n",
    "print(\"Example Output Format:\")\n",
    "print(example_output_format)\n",
    "\n",
    "# system_prompt\n",
    "# Build SDG goal info string from df_sdg_goals\n",
    "sdg_goal_info = \"\\n\".join(\n",
    "    f\"{row.goal_code}: {row.goal_name} - {row.goal_description}\"\n",
    "    for _, row in df_sdg_goals.iterrows()\n",
    ")\n",
    "\n",
    "sdg_system_prompt = f\"\"\"\n",
    "You are an intelligent multi-label classification system designed to map texts to their relevant Sustainable Development Goals.\n",
    "Take the text delimited by triple quotation marks and return a JSON list of relevant SDGs. \n",
    "Example output format: {example_output_format}\n",
    "\n",
    "Here are the SDG goals and their descriptions:\n",
    "{sdg_goal_info}\n",
    "\n",
    "\"\"\"\n",
    "# Print the system prompt\n",
    "print(\"System Prompt:\")\n",
    "print(sdg_system_prompt)\n",
    "# user_prompt\n",
    "sdg_user_prompt = f\"\"\"\n",
    "\"Classify the following text in terms of its relevance to the Sustainable Development Goals:\",\n",
    "Text: '''{text}'''\n",
    "\"\"\"\n",
    "# Print the user prompt\n",
    "print(\"User Prompt:\")\n",
    "print(sdg_user_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 7d: Get the LLM API prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenWebUI API configuration\n",
    "openwebui_base_url = \"https://nebula.cs.vu.nl\"  # Replace with your actual OpenWebUI API base URL\n",
    "openwebui_api_key = \"sk-5b5a024888c14a019c0e9b4857df9329\"  # Replace with your actual API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first get the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curl -X GET 'https://nebula.cs.vu.nl/api/models' -H 'Authorization: Bearer sk-5b5a024888c14a019c0e9b4857df9329'\n",
      "Available models:\n",
      "- id: deepseek-r1:1.5b, name: deepseek-r1:1.5b, parameter_size: 1.8B\n",
      "- id: deepseek-r1:8b, name: deepseek-r1:8b, parameter_size: 8.0B\n",
      "- id: llama3.1:8b, name: llama3.1:8b, parameter_size: 8.0B\n",
      "- id: qwen2.5:1.5b, name: qwen2.5:1.5b, parameter_size: 1.5B\n",
      "- id: qwen2.5:7b, name: qwen2.5:7b, parameter_size: 7.6B\n"
     ]
    }
   ],
   "source": [
    "# This script fetches the list of available models from the OpenWebUI API\n",
    "# and prints their IDs, names, and parameter sizes.\n",
    "\n",
    "import requests\n",
    "\n",
    "# Use the existing variables openwebui_base_url and openwebui_api_key\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {openwebui_api_key}\"\n",
    "}\n",
    "\n",
    "# Ensure the base URL does not end with a slash\n",
    "api_url = openwebui_base_url.rstrip('/') + \"/api/models\"\n",
    "\n",
    "# print the request in curl\n",
    "print(f\"curl -X GET '{api_url}' -H 'Authorization: Bearer {openwebui_api_key}'\")\n",
    "\n",
    "response = requests.get(api_url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    models_json = response.json()\n",
    "    models = models_json.get(\"data\", [])\n",
    "    print(\"Available models:\")\n",
    "    for model in models:\n",
    "        print(f\"- id: {model.get('id')}, name: {model.get('name')}, parameter_size: {model.get('ollama', {}).get('details', {}).get('parameter_size')}\")\n",
    "else:\n",
    "    print(f\"Failed to fetch models. Status code: {response.status_code}\")\n",
    "    print(f\"Response: {response.text}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the model to use, when no model is chosen, deepseek-r1:1.5b will be the default (faser & cheaper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "0: deepseek-r1:1.5b\n",
      "1: deepseek-r1:8b\n",
      "2: llama3.1:8b\n",
      "3: qwen2.5:1.5b\n",
      "4: qwen2.5:7b\n",
      "Select the model index to use (default: 2, llama3.1:8b) [timeout 10s]:\n",
      "No valid input, using default model.\n",
      "Model selected: llama3.1:8b\n"
     ]
    }
   ],
   "source": [
    "import signal\n",
    "\n",
    "# Select the model to use, when no model is chosen, llama3.1:8b will be the default\n",
    "model = \"llama3.1:8b\"  # Replace with your actual model name\n",
    "\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutError\n",
    "\n",
    "print(\"Available models:\")\n",
    "for i, m in enumerate(models):\n",
    "    print(f\"{i}: {m['id']}\")\n",
    "\n",
    "print(\"Select the model index to use (default: 2, llama3.1:8b) [timeout 10s]:\")\n",
    "signal.signal(signal.SIGALRM, timeout_handler)\n",
    "signal.alarm(10)\n",
    "try:\n",
    "    user_input = input()\n",
    "    if user_input.strip().isdigit():\n",
    "        selected_model_index = int(user_input.strip())\n",
    "        if 0 <= selected_model_index < len(models):\n",
    "            model = models[selected_model_index]['id']\n",
    "        else:\n",
    "            print(\"Invalid index, using default model.\")\n",
    "            model = \"llama3.1:8b\"\n",
    "    else:\n",
    "        print(\"No valid input, using default model.\")\n",
    "        model = \"llama3.1:8b\"\n",
    "except TimeoutError:\n",
    "    print(\"No response received. Using default model.\")\n",
    "    model = \"llama3.1:8b\"\n",
    "finally:\n",
    "    signal.alarm(0)\n",
    "\n",
    "print(f\"Model selected: {model}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for each abstract, run the system and user prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for record_id doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef: {'model': 'llama3.1:8b', 'messages': [{'role': 'system', 'content': '\\nYou are an intelligent multi-label classification system designed to map texts to their relevant Sustainable Development Goals.\\nTake the text delimited by triple quotation marks and return a JSON list of relevant SDGs. \\nExample output format: \\n{\\n    \"sdgs\": [2, 6, 17],\\n    \"explanation\": \"This text is related to SDG 2 (Zero hunger) because it discusses food security, SDG 6 (Clean water and sanitation) due to references to environmental protection, and SDG 17 (Partnerships for the goals) as it mentions global cooperation.\"\\n}\\n\\n\\nHere are the SDG goals and their descriptions:\\n1: No poverty - End poverty in all its forms everywhere\\n2: Zero hunger - End hunger, achieve food security and improved nutrition and promote sustainable agriculture\\n3: Good health and well-being - Ensure healthy lives and promote well-being for all at all ages\\n4: Quality education - Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all\\n5: Gender equality - Achieve gender equality and empower all women and girls\\n6: Clean water and sanitation - Ensure availability and sustainable management of water and sanitation for all\\n7: Affordable and clean energy - Ensure access to affordable, reliable, sustainable and modern energy for all\\n8: Decent work and economic growth - Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all\\n9: Industry, innovation and infrastructure - Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation\\n10: Reduced inequalities - Reduce inequality within and among countries\\n11: Sustainable cities and communities - Make cities and human settlements inclusive, safe, resilient and sustainable\\n12: Responsible consumption and production - Ensure sustainable consumption and production patterns\\n13: Climate action - Take urgent action to combat climate change and its impacts\\n14: Life below water - Conserve and sustainably use the oceans, seas and marine resources for sustainable development\\n15: Life on land - Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss\\n16: Peace, justice and strong institutions - Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels\\n17: Partnerships for the goals - Strengthen the means of implementation and revitalize the Global Partnership for Sustainable Development\\n\\n'}, {'role': 'user', 'content': \"Classify the following text in terms of its relevance to the Sustainable Development Goals:\\nText: '''Abstract</jats:title><jats:p>The Black Sea, the largest semienclosed anoxic basin on Earth, can be considered as an excellent natural laboratory for oxic and anoxic biogeochemical processes. The suboxic zone, a thin interface between oxic and anoxic waters, still remains poorly understood because it has been undersampled. This has led to alternative concepts regarding the underlying processes that create it. Existing hypotheses suggest that the interface originates either by isopycnal intrusions that introduce oxygen or the dynamics of manganese redox cycling that are associated with the sinking of particles or chemosynthetic bacteria. Here we reexamine these concepts using highâ€resolution oxygen, sulfide, nitrate, and particle concentration profiles obtained with sensors deployed on profiling floats. Our results show an extremely stable structure in density space over the entire basin with the exception of areas near the Bosporus plume and in the southern areas dominated by coastal anticyclones. The absence of largeâ€scale horizontal intrusive signatures in the openâ€sea supports a hypothesis prioritizing the role of biogeochemical processes.</jats:p>'''\"}]}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData for record_id \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecord_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Make the API call\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopenwebui_base_url\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/api/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAuthorization\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBearer \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mopenwebui_api_key\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mContent-Type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapplication/json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Check if the request was successful\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "File \u001b[0;32m~/data/ori_storage/OpenAIRE-tools/.venv/lib/python3.10/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/data/ori_storage/OpenAIRE-tools/.venv/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/data/ori_storage/OpenAIRE-tools/.venv/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/data/ori_storage/OpenAIRE-tools/.venv/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/data/ori_storage/OpenAIRE-tools/.venv/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/data/ori_storage/OpenAIRE-tools/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/data/ori_storage/OpenAIRE-tools/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/data/ori_storage/OpenAIRE-tools/.venv/lib/python3.10/site-packages/urllib3/connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    513\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1303\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1300\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1301\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1302\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1159\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Add a testing method to limit the number of abstracts\n",
    "if testing_mode:\n",
    "    # Load only the first 3 abstracts for testing\n",
    "    description_df = pd.read_parquet(f\"{processing_folder_path}/{folder_name}-descriptions-with-tokens.parquet\").head(3)\n",
    "else:\n",
    "    # Load all abstracts for production\n",
    "    description_df = pd.read_parquet(f\"{processing_folder_path}/{folder_name}-descriptions-with-tokens.parquet\")                                                             \n",
    "\n",
    "# Filter abstracts with at least 100 tokens\n",
    "description_df = description_df[description_df['token_count'] >= 100]\n",
    "\n",
    "# Prepare results list\n",
    "sdg_results = []\n",
    "\n",
    "# Loop through each abstract\n",
    "for idx, row in description_df.iterrows():\n",
    "    record_id = row['record_id']\n",
    "    abstract = row['description']\n",
    "\n",
    "    # Prepare the messages for the API\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": sdg_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Classify the following text in terms of its relevance to the Sustainable Development Goals:\\nText: '''{abstract}'''\"}\n",
    "    ]\n",
    "\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages\n",
    "    }\n",
    "\n",
    "    # Print the data variable for debugging\n",
    "    print(f\"Data for record_id {record_id}: {data}\")\n",
    "\n",
    "    # Make the API call\n",
    "    response = requests.post(\n",
    "        openwebui_base_url.rstrip('/') + \"/api/chat/completions\",\n",
    "        headers={\"Authorization\": f\"Bearer {openwebui_api_key}\", \"Content-Type\": \"application/json\"},\n",
    "        json=data\n",
    "    )\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error processing record_id {record_id}: {response.status_code} - {response.text}\")\n",
    "        continue\n",
    "\n",
    "    # Print the response for debugging\n",
    "    print(f\"Response for record_id {record_id}: {response.json()}\")\n",
    "    \n",
    "    # Parse the response\n",
    "    try:\n",
    "        result = response.json()\n",
    "        # Try to extract the SDG list from the response\n",
    "        content = result['choices'][0]['message']['content']\n",
    "        # Try to parse the JSON from the model output\n",
    "        try:\n",
    "            sdg_json = eval(content) if isinstance(content, str) else content\n",
    "            sdgs = sdg_json.get(\"sdgs\", [])\n",
    "            explanation = sdg_json.get(\"explanation\", \"\")\n",
    "        except Exception:\n",
    "            sdgs = []\n",
    "            explanation = \"\"\n",
    "    except Exception:\n",
    "        sdgs = []\n",
    "        explanation = \"\"\n",
    "\n",
    "    # Append to results, including the explanation if available\n",
    "    sdg_results.append({\n",
    "        \"record_id\": record_id,\n",
    "        \"abstract\": abstract,\n",
    "        \"sdgs\": sdgs,\n",
    "        \"explanation\": explanation\n",
    "    })\n",
    "\n",
    "    # Optional: print progress\n",
    "    print(f\"Processed record_id: {record_id}, SDGs: {sdgs}\")\n",
    "\n",
    "    # Optional: delay to avoid rate limits\n",
    "    time.sleep(1)\n",
    "\n",
    "# Print the number of results\n",
    "print(f\"Number of SDG results collected: {len(sdg_results)}\")\n",
    "\n",
    "# Make the value of the model variable suitable for using in the file names\n",
    "model_filename = model.replace(\":\", \"-\").replace(\" \", \"_\")\n",
    "\n",
    "# Save results to parquet\n",
    "sdg_results_df = pd.DataFrame(sdg_results)\n",
    "sdg_results_path = f\"{processing_folder_path}/{folder_name}-sdg-results-{model_filename}.parquet\"\n",
    "sdg_results_df.to_parquet(sdg_results_path, index=False)\n",
    "print(f\"SDG LLM results saved to: {sdg_results_path}\")\n",
    "print(f\"File size: {os.path.getsize(sdg_results_path) / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Get Genderize data\n",
    "a. First Query the authors with country of the affiliation along with the record id (to be used as primary keys, connecting the tables later on), \n",
    "\n",
    "b. get gender data by parsing the author names with country label over an API, \n",
    "\n",
    "c. save the outcomes in a separate parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authors with full names and ORCID IDs:\n",
      "                                           record_id             full_name  \\\n",
      "0     doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef        Emil V. Stanev   \n",
      "1     doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef  Pierreâ€Marie Poulain   \n",
      "2     doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef      Sebastian Grayek   \n",
      "3     doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef    Kenneth S. Johnson   \n",
      "4     doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef        HervÃ© Claustre   \n",
      "...                                              ...                   ...   \n",
      "5846  doi_dedup___::53e879864d1a55d055a8f2385005f5e0         Xiaogang Xing   \n",
      "5847  doi_dedup___::53e879864d1a55d055a8f2385005f5e0        Antoine Poteau   \n",
      "5848  doi_dedup___::53e879864d1a55d055a8f2385005f5e0     Giorgio Dall'Olmo   \n",
      "5849  doi_dedup___::53e879864d1a55d055a8f2385005f5e0        Annick Bricaud   \n",
      "5850  doi_dedup___::f872f5c02d3bce9bc6ae9962dbec5083                  Argo   \n",
      "\n",
      "        first_name last_name                orcid country_name country_code  \n",
      "0          Emil V.    Stanev  0000-0002-1110-8645       France           FR  \n",
      "1     Pierreâ€Marie   Poulain  0000-0003-1342-8463       France           FR  \n",
      "2        Sebastian    Grayek  0000-0002-2461-757x       France           FR  \n",
      "3       Kenneth S.   Johnson  0000-0001-5513-5584       France           FR  \n",
      "4            HervÃ©  Claustre  0000-0001-6243-0258       France           FR  \n",
      "...            ...       ...                  ...          ...          ...  \n",
      "5846          None      None                 None        Italy           IT  \n",
      "5847          None      None  0000-0002-0519-5180        Italy           IT  \n",
      "5848          None      None  0000-0003-3931-4675        Italy           IT  \n",
      "5849          None      None                 None        Italy           IT  \n",
      "5850                                         None       Canada           CA  \n",
      "\n",
      "[5851 rows x 7 columns]\n",
      "Authors data saved to: ./data/2025-02-19/04_processed/argo-france/argo-france-authors.parquet\n",
      "File size: 0.14 MB\n"
     ]
    }
   ],
   "source": [
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Query to extract authors along with their full names and record IDs\n",
    "authors = con.sql(f'''\n",
    "    SELECT \n",
    "        id AS record_id,\n",
    "        unnest.fullName AS full_name,\n",
    "        unnest.name AS first_name,\n",
    "        unnest.surname AS last_name,\n",
    "        unnest.pid.id.value AS orcid,\n",
    "        countries[1].label AS country_name,\n",
    "        countries[1].code AS country_code\n",
    "    FROM read_parquet('{master_file}')\n",
    "    CROSS JOIN UNNEST(authors) AS unnest\n",
    "    WHERE countries IS NOT NULL AND array_length(countries) > 0\n",
    "''').fetchall()\n",
    "\n",
    "# convert the result to a DataFrame\n",
    "import pandas as pd\n",
    "authors_df = pd.DataFrame(authors, columns=['record_id', 'full_name', 'first_name', 'last_name', 'orcid', 'country_name', 'country_code'])\n",
    "# Print the authors DataFrame\n",
    "print(\"Authors with full names and ORCID IDs:\")\n",
    "print(authors_df)  \n",
    "\n",
    "# Save the authors data to a new Parquet file for later use\n",
    "authors_file_path = f\"{processing_folder_path}/{folder_name}-authors.parquet\"\n",
    "authors_df.to_parquet(authors_file_path, index=False)\n",
    "print(f\"Authors data saved to: {authors_file_path}\")\n",
    "print(f\"File size: {os.path.getsize(authors_file_path) / (1024**2):.2f} MB\")\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique authors linked to record IDs:\n",
      "                                           record_id    first_name  \\\n",
      "0     doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef          Emil   \n",
      "1     doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef  Pierreâ€Marie   \n",
      "2     doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef     Sebastian   \n",
      "3     doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef       Kenneth   \n",
      "4     doi_dedup___::9e973d60bf13b4e8b28c199e27dea4ef         HervÃ©   \n",
      "...                                              ...           ...   \n",
      "5834  doi_dedup___::46a4623c5543eb6a03c0389effc47b8d       Nicolas   \n",
      "5835  doi_dedup___::46a4623c5543eb6a03c0389effc47b8d         Sally   \n",
      "5836  doi_dedup___::46a4623c5543eb6a03c0389effc47b8d       Thierry   \n",
      "5837  doi_dedup___::46a4623c5543eb6a03c0389effc47b8d     Jean-Marc   \n",
      "5838  doi_dedup___::46a4623c5543eb6a03c0389effc47b8d       Laurent   \n",
      "\n",
      "     country_code  \n",
      "0              FR  \n",
      "1              FR  \n",
      "2              FR  \n",
      "3              FR  \n",
      "4              FR  \n",
      "...           ...  \n",
      "5834           FR  \n",
      "5835           FR  \n",
      "5836           FR  \n",
      "5837           FR  \n",
      "5838           FR  \n",
      "\n",
      "[2747 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Filter the authors DataFrame to get unique names, countries, and record IDs\n",
    "# Only use the first occurrence of each first name\n",
    "unique_authors = authors_df[['record_id', 'first_name', 'country_code']].copy()\n",
    "unique_authors['first_name'] = unique_authors['first_name'].str.split().str[0]  # Keep only the first word\n",
    "# Remove one-letter names (e.g., \"L.\", \"S.\") that often end with a dot\n",
    "unique_authors = unique_authors[~unique_authors['first_name'].str.match(r'^[A-Z]\\.$', na=False)]\n",
    "# Drop rows where 'first_name' is None or NaN\n",
    "unique_authors = unique_authors.dropna(subset=['first_name'])\n",
    "unique_authors = unique_authors[unique_authors['first_name'] != 'None']\n",
    "unique_authors = unique_authors.drop_duplicates(subset=['first_name', 'record_id'], keep='first')\n",
    "\n",
    "# Print unique authors with record IDs\n",
    "print(\"Unique authors linked to record IDs:\")\n",
    "print(unique_authors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paid Subscription: False\n",
      "Testing Mode: True\n",
      "Rate Limit: 10 requests per second\n",
      "Delay between requests: 0.50 seconds\n"
     ]
    }
   ],
   "source": [
    "# Adding variables to handle rate limiting and API key for Genderize API\n",
    "\n",
    "# Check if the user has a paid subscription\n",
    "paid_subscription = False  # Set this to True if you have a paid subscription\n",
    "\n",
    "testing_mode = True  # Set to True for testing, False for production\n",
    "\n",
    "# Set the rate limit based on the testing mode\n",
    "if testing_mode:\n",
    "    rate_limit = 10  # Reduced rate limit for testing\n",
    "else:\n",
    "    rate_limit = 1000 if paid_subscription else 100 # Adjust rate limit based on subscription, setting a default for free users\n",
    "\n",
    "# delay between requests in seconds\n",
    "delay_between_requests = 0.5  # Calculate delay based on rate limit\n",
    "\n",
    "# Genderize API key\n",
    "genderize_api_key= \"da1a264b9bab63b46f27ac635dd7d2df\"  # Replace with your actual API key\n",
    "\n",
    "# Initialize request count\n",
    "request_count = 0  # Initialize request count\n",
    "\n",
    "# Base URL for Genderize API\n",
    "base_url = \"https://api.genderize.io\"\n",
    "\n",
    "# print all the above variables\n",
    "print(f\"Paid Subscription: {paid_subscription}\")\n",
    "print(f\"Testing Mode: {testing_mode}\")\n",
    "print(f\"Rate Limit: {rate_limit} requests per second\")\n",
    "print(f\"Delay between requests: {delay_between_requests:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Stopping for the day.\n",
      "Gender data saved to: ./data/2025-02-19/04_processed/argo-france/argo-france-gender-data.parquet\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "# Initialize the list to store gender results\n",
    "gender_results = []\n",
    "\n",
    "# Iterate over the unique authors\n",
    "for _, row in unique_authors.iterrows():\n",
    "    if request_count >= rate_limit: # type: ignore\n",
    "        print(\"Rate limit reached. Stopping for the day.\")\n",
    "        break\n",
    "\n",
    "    first_name = row['first_name']\n",
    "    country_code = row['country_code']\n",
    "    record_id = row['record_id']  # Add the record ID\n",
    "\n",
    "    # Skip if the first name is missing\n",
    "    if pd.isna(first_name):\n",
    "        continue\n",
    "\n",
    "    # Prepare the API request\n",
    "    params = {\n",
    "        \"name\": first_name,\n",
    "        \"country_id\": country_code\n",
    "    }\n",
    "    if paid_subscription:\n",
    "        params[\"apikey\"] = genderize_api_key\n",
    "\n",
    "    try:\n",
    "        # Send the request to the Genderize API\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        # Append the result to the list\n",
    "        gender_results.append({\n",
    "            \"first_name\": first_name,\n",
    "            \"country_code\": country_code,\n",
    "            \"gender\": data.get(\"gender\"),\n",
    "            \"probability\": data.get(\"probability\"),\n",
    "            \"count\": data.get(\"count\")\n",
    "        })\n",
    "\n",
    "        # Increment the request count\n",
    "        request_count += 1\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Processed: {first_name} ({country_code}) - Gender: {data.get('gender')}\")\n",
    "\n",
    "        # Add a delay between requests to avoid overwhelming the API\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error processing {first_name} ({country_code}): {e}\")\n",
    "\n",
    "        # Increment the request count\n",
    "        request_count += 1\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Processed: {first_name} ({country_code}) - Gender: {data.get('gender')}\")\n",
    "\n",
    "        # Add a small delay to avoid overwhelming the API\n",
    "        time.sleep(1)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error processing {first_name} ({country_code}): {e}\")\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "gender_df = pd.DataFrame(gender_results)\n",
    "\n",
    "# Save the results to a Parquet file\n",
    "gender_file_path = f\"{processing_folder_path}/{folder_name}-gender-data.parquet\"\n",
    "gender_df.to_parquet(gender_file_path, index=False)\n",
    "print(f\"Gender data saved to: {gender_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Get Citizen Science classification labels\n",
    "\n",
    "a. Query the abstracts first along with the id (to be used as primary keys, connecting the tables later on), \n",
    "\n",
    "b. get citizen science labels by parsing the abstract over an LLM API with system prompt, \n",
    "\n",
    "c. save the outcomes in a separate parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 10: Generate SQL schemas for all the parquet files in the processed folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import duckdb\n",
    "\n",
    "# List all .parquet files in the processing folder\n",
    "parquet_files = [\n",
    "    f for f in os.listdir(processing_folder_path/folder_name)\n",
    "    if f.endswith('.parquet')\n",
    "]\n",
    "\n",
    "for parquet_file in parquet_files:\n",
    "    parquet_path = os.path.join(processing_folder_path/folder_name, parquet_file)\n",
    "    schema_file_name = os.path.splitext(parquet_file)[0] + '.sql'\n",
    "    schema_file_path = os.path.join(processing_folder_path/folder_name, schema_file_name)\n",
    "    \n",
    "    print(f\"Generating schema for: {parquet_path}\")\n",
    "    print(f\"Schema file path: {schema_file_path}\")\n",
    "\n",
    "    duckdb.sql(f'''\n",
    "        COPY (\n",
    "            SELECT *\n",
    "            FROM (DESCRIBE '{parquet_path}')\n",
    "        )\n",
    "        TO '{schema_file_path}'\n",
    "    ''')\n",
    "\n",
    "    if os.path.exists(schema_file_path):\n",
    "        print(f\"Schema file exists: {schema_file_path}\")\n",
    "        with open(schema_file_path, 'r') as schema_file:\n",
    "            schema_content = schema_file.read()\n",
    "            print(\"Schema file content:\")\n",
    "            print(schema_content)\n",
    "    else:\n",
    "        print(f\"Schema file does not exist: {schema_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
