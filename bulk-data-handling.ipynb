{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAIRE Community Data Dump Handling: Extraction, Tranformation and Enrichment\n",
    "\n",
    "In this notebook we will start with one of the [OpenAIRE Community Subgraphs](https://graph.openaire.eu/docs/downloads/subgraphs) to enrich that informatino for further analysis.\n",
    "\n",
    "This data process will extract an [OpenAIRE community data dump from Zenodo](https://doi.org/10.5281/zenodo.3974604), transforms it in to a portable file format .parquet (and updatable with changes for time seires analysis), that can be used to query with DuckDB, to enrich this with additional data  (also in .parquet, for join queries).\n",
    "\n",
    "This additional data can be societal impact data from [Altmetric.com](https://details-page-api-docs.altmetric.com/) or [Overton.io](https://app.overton.io/swagger.php), Gender data using [genderize.io](https://genderize.io/documentation), sdg classification using [aurora-sdg](https://aurora-universities.eu/sdg-research/sdg-api/)\n",
    "\n",
    "This script needs to be written in a way so that it can run every month using  the latest data.\n",
    "\n",
    "## Processing steps\n",
    "\n",
    "* the folder ./data/ is put in .gitignore to prevent that bulk datais sent to a code repository. So make sure that folder exists, and mkdir if not exists. \n",
    "* The script downloads the lastest Data Dump Tar file from one selected community. See https://doi.org/10.5281/zenodo.3974604 for the latest list. In our case the Aurora tar file. https://zenodo.org/records/14887484/files/aurora.tar?download=1\n",
    "  * Use the json record of zenodo to get to the latest record, and fetch the download link of the aurora.tar file. for example : https://zenodo.org/records/14887484/export/json or https://zenodo.org/api/records/14887484/versions/latest \n",
    "  Make the tar filename a variable, so it can be used for multiple community dumps.\n",
    "  Download the tar file in a target folder ./data/{filename+timestamp}/ where a subfolder is created using the filename and the timestamp. Make this also as a  variable to use later on.\n",
    "* Extract the tar file, to the compressed .json.gz files and put these in target folder ./data/{filename+timestamp}/01-extracted/\n",
    "* Transform the compressed .json.gz files into a single .parquet file in target folder ./data/{filename+timestamp}/02-transformed/\n",
    "Use instructions in sections \"Processing JSON files with DuckDB\" and \"Full dataset, bit by bit\" and \"Splitting and Processing JSON Files in Batches\" https://github.com/mosart/OpenAIRE-tools/blob/main/duckdb-querying.ipynb to start with. (be aware of error messages, and fix the issues to get all the data in)\n",
    "* Extract the SQL schema (schema-datadump.sql) from the .parquet file and put it in target folder ./data/{filename+timestamp}/02-transformed/ This is needed for further processing of the records with DuckDB later on.\n",
    "Use instructions in section \"Extracting Schema from Parquet File\" https://github.com/mosart/OpenAIRE-tools/blob/main/duckdb-querying.ipynb to start with.\n",
    "* Query to get all identifiers: openaire id, doi, isbn, hdl, etc.\n",
    "* **Get Altmetric data:**\n",
    "* Extract the Altmetric data using the Identifiers. put that in target folder ./data/{filename+timestamp}/03-altmetric-extracted/\n",
    "* Transform the Altmetric data to a single .parquet file, with the identifiers. put that in target folder ./data/{filename+timestamp}/04-altmetric-transformed/ This way duckDB can make a join when querying over multiple parquet files.\n",
    "* Extract the SQL schema (schema-altmetric.sql) from the .parquet file and put it in target folder ./data/{filename+timestamp}/04-altmetric-transformed/\n",
    "* **Get Overton data:** Repeat the altmetric steps, bun than for Overton.\n",
    "* **Get Gender data** query for the Author names and country codes, and run them over the gerderize api\n",
    "* **Get SDG data** query for the abstracts, and run abstracs larger than 100 tokens over the aurora-SDG api.\n",
    "\n",
    "ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Get the latest Community Dump File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37 .tar files in the dataset.\n",
      "Details of .tar files:\n",
      "[{'filename': 'energy-planning_1.tar', 'size': '6.99 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/energy-planning_1.tar/content', 'checksum': 'md5:0a2f551db46a9e629bb1d0a0098ae5cd'}, {'filename': 'edih-adria_1.tar', 'size': '5.86 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/edih-adria_1.tar/content', 'checksum': 'md5:23559bed5a9023398b431777bdc8a126'}, {'filename': 'uarctic_1.tar', 'size': '9.75 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/uarctic_1.tar/content', 'checksum': 'md5:302e3844ebd041c5f4ed94505eb9a285'}, {'filename': 'netherlands_1.tar', 'size': '3.91 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/netherlands_1.tar/content', 'checksum': 'md5:d1416c058b3961483aac340750ea8726'}, {'filename': 'knowmad_1.tar', 'size': '10.08 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/knowmad_1.tar/content', 'checksum': 'md5:a79573a02f2c9a9d65c33b3f3a2eaab9'}, {'filename': 'argo-france.tar', 'size': '0.00 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/argo-france.tar/content', 'checksum': 'md5:2ce6b0fcc6f876b600207759a0dc9758'}, {'filename': 'civica.tar', 'size': '0.23 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/civica.tar/content', 'checksum': 'md5:d2f24bbef06809a91d124f0b07cb1034'}, {'filename': 'covid-19.tar', 'size': '2.03 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/covid-19.tar/content', 'checksum': 'md5:3b741e8138f39932ca6c13ca106fe5d3'}, {'filename': 'aurora.tar', 'size': '1.73 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/aurora.tar/content', 'checksum': 'md5:9b6a8f38cd6f0ce16a85dfc020c220bf'}, {'filename': 'dh-ch.tar', 'size': '1.16 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/dh-ch.tar/content', 'checksum': 'md5:dbebdcc8ad7fd1dc7894fe03ebe2a978'}, {'filename': 'heritage-science.tar', 'size': '0.03 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/heritage-science.tar/content', 'checksum': 'md5:ffd2537b08c58d78eea4bc23a99b3c07'}, {'filename': 'dth.tar', 'size': '0.01 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/dth.tar/content', 'checksum': 'md5:643894810ac8bfce0f8273cf40d05a7a'}, {'filename': 'egrise.tar', 'size': '0.02 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/egrise.tar/content', 'checksum': 'md5:2f52b49fa8bd983bcf6884d6c4f5e952'}, {'filename': 'lifewatch-eric.tar', 'size': '0.05 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/lifewatch-eric.tar/content', 'checksum': 'md5:213c3f1ac83454ec01560d683a26362d'}, {'filename': 'iperionhs.tar', 'size': '0.00 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/iperionhs.tar/content', 'checksum': 'md5:6ffaf325257d9af5e43daa68505b1797'}, {'filename': 'eutopia.tar', 'size': '1.60 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/eutopia.tar/content', 'checksum': 'md5:f9eb5a1bb86caf6a4f2a7563453fc6df'}, {'filename': 'sdsn-gr.tar', 'size': '0.05 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/sdsn-gr.tar/content', 'checksum': 'md5:696f8b509a12c0bc898e1b9040a53790'}, {'filename': 'north-american-studies.tar', 'size': '0.36 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/north-american-studies.tar/content', 'checksum': 'md5:b677758820184053d9c1e6714394dd70'}, {'filename': 'knowmad_3.tar', 'size': '10.06 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/knowmad_3.tar/content', 'checksum': 'md5:68c5839a6355ea26f979ba781bc26a56'}, {'filename': 'knowmad_2.tar', 'size': '10.07 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/knowmad_2.tar/content', 'checksum': 'md5:95442e14703ba5db573cbf12296d76f4'}, {'filename': 'knowmad_5.tar', 'size': '5.37 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/knowmad_5.tar/content', 'checksum': 'md5:35482782eb621df9ec7365d34ccf3d07'}, {'filename': 'knowmad_4.tar', 'size': '10.04 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/knowmad_4.tar/content', 'checksum': 'md5:d770fcd862d5d7d4d918c59f028e24da'}, {'filename': 'beopen.tar', 'size': '0.20 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/beopen.tar/content', 'checksum': 'md5:447dbe25eaedc20a75568fd340f8e25d'}, {'filename': 'dariah.tar', 'size': '0.02 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/dariah.tar/content', 'checksum': 'md5:814e3a79b29da6b605018009f8575a8c'}, {'filename': 'eu-conexus.tar', 'size': '0.18 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/eu-conexus.tar/content', 'checksum': 'md5:19c86e2b79bde505112fb6b3d6e38eef'}, {'filename': 'elixir-gr.tar', 'size': '0.01 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/elixir-gr.tar/content', 'checksum': 'md5:e1dfe593d40c498e31a6ff5132da5fa4'}, {'filename': 'eut.tar', 'size': '0.21 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/eut.tar/content', 'checksum': 'md5:3da085b997006205c694b023aefafe7c'}, {'filename': 'enermaps.tar', 'size': '1.59 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/enermaps.tar/content', 'checksum': 'md5:b157900f8cb97cf4aa4f82ef59e2ff6e'}, {'filename': 'forthem.tar', 'size': '0.91 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/forthem.tar/content', 'checksum': 'md5:7864a0ddb0b676bb053bbcdf12a12525'}, {'filename': 'inria.tar', 'size': '0.27 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/inria.tar/content', 'checksum': 'md5:66a7e88ddda08626cbf26e182f7eafea'}, {'filename': 'mes.tar', 'size': '0.38 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/mes.tar/content', 'checksum': 'md5:10362f805616e391d350b395d9ae30a3'}, {'filename': 'neanias-underwater.tar', 'size': '0.02 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/neanias-underwater.tar/content', 'checksum': 'md5:4422b9a3be4a1e6cad61da46044a0ca2'}, {'filename': 'neanias-atmospheric.tar', 'size': '0.57 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/neanias-atmospheric.tar/content', 'checksum': 'md5:a3b9edbd956aee813cdfe97655c3fc9e'}, {'filename': 'neanias-space.tar', 'size': '0.73 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/neanias-space.tar/content', 'checksum': 'md5:618fbecca154af288dc1c92246b3d73b'}, {'filename': 'rural-digital-europe.tar', 'size': '0.83 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/rural-digital-europe.tar/content', 'checksum': 'md5:6543f5ead539a8bbad04593b641af0a1'}, {'filename': 'tunet.tar', 'size': '0.05 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/tunet.tar/content', 'checksum': 'md5:178b5a944133ded65d741ea5dc2c5990'}, {'filename': 'ni.tar', 'size': '3.01 GB', 'downloadlink': 'https://zenodo.org/api/records/14887484/files/ni.tar/content', 'checksum': 'md5:e82f580135522acd2da7277ea9389718'}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Fetch the JSON data from the URL\n",
    "url = \"https://zenodo.org/api/records/14887484/versions/latest\"\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "# Extract the files information\n",
    "files = data.get(\"files\", [])\n",
    "\n",
    "# Create a list of dictionaries for the .tar files\n",
    "tar_files = []\n",
    "for file in files:\n",
    "    if file[\"key\"].endswith(\".tar\"):\n",
    "        tar_files.append({\n",
    "            \"filename\": file[\"key\"],\n",
    "            \"size\": f\"{file['size'] / (1024**3):.2f} GB\",  # Convert bytes to GB\n",
    "            \"downloadlink\": file[\"links\"][\"self\"],\n",
    "            \"checksum\": file[\"checksum\"]\n",
    "        })\n",
    "\n",
    "# print the tar files\n",
    "# If no tar files found, print a message\n",
    "if not tar_files:\n",
    "    print(\"No .tar files found in the dataset.\")\n",
    "else:\n",
    "    print(f\"Found {len(tar_files)} .tar files in the dataset.\")\n",
    "    print(\"Details of .tar files:\")\n",
    "    print(tar_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      filename      size  \\\n",
      "5              argo-france.tar   0.00 GB   \n",
      "8                   aurora.tar   1.73 GB   \n",
      "22                  beopen.tar   0.20 GB   \n",
      "6                   civica.tar   0.23 GB   \n",
      "7                 covid-19.tar   2.03 GB   \n",
      "23                  dariah.tar   0.02 GB   \n",
      "9                    dh-ch.tar   1.16 GB   \n",
      "11                     dth.tar   0.01 GB   \n",
      "1             edih-adria_1.tar   5.86 GB   \n",
      "12                  egrise.tar   0.02 GB   \n",
      "25               elixir-gr.tar   0.01 GB   \n",
      "0        energy-planning_1.tar   6.99 GB   \n",
      "27                enermaps.tar   1.59 GB   \n",
      "24              eu-conexus.tar   0.18 GB   \n",
      "26                     eut.tar   0.21 GB   \n",
      "15                 eutopia.tar   1.60 GB   \n",
      "28                 forthem.tar   0.91 GB   \n",
      "10        heritage-science.tar   0.03 GB   \n",
      "29                   inria.tar   0.27 GB   \n",
      "14               iperionhs.tar   0.00 GB   \n",
      "4                knowmad_1.tar  10.08 GB   \n",
      "19               knowmad_2.tar  10.07 GB   \n",
      "18               knowmad_3.tar  10.06 GB   \n",
      "21               knowmad_4.tar  10.04 GB   \n",
      "20               knowmad_5.tar   5.37 GB   \n",
      "13          lifewatch-eric.tar   0.05 GB   \n",
      "30                     mes.tar   0.38 GB   \n",
      "32     neanias-atmospheric.tar   0.57 GB   \n",
      "33           neanias-space.tar   0.73 GB   \n",
      "31      neanias-underwater.tar   0.02 GB   \n",
      "3            netherlands_1.tar   3.91 GB   \n",
      "36                      ni.tar   3.01 GB   \n",
      "17  north-american-studies.tar   0.36 GB   \n",
      "34    rural-digital-europe.tar   0.83 GB   \n",
      "16                 sdsn-gr.tar   0.05 GB   \n",
      "35                   tunet.tar   0.05 GB   \n",
      "2                uarctic_1.tar   9.75 GB   \n",
      "\n",
      "                                         downloadlink  \\\n",
      "5   https://zenodo.org/api/records/14887484/files/...   \n",
      "8   https://zenodo.org/api/records/14887484/files/...   \n",
      "22  https://zenodo.org/api/records/14887484/files/...   \n",
      "6   https://zenodo.org/api/records/14887484/files/...   \n",
      "7   https://zenodo.org/api/records/14887484/files/...   \n",
      "23  https://zenodo.org/api/records/14887484/files/...   \n",
      "9   https://zenodo.org/api/records/14887484/files/...   \n",
      "11  https://zenodo.org/api/records/14887484/files/...   \n",
      "1   https://zenodo.org/api/records/14887484/files/...   \n",
      "12  https://zenodo.org/api/records/14887484/files/...   \n",
      "25  https://zenodo.org/api/records/14887484/files/...   \n",
      "0   https://zenodo.org/api/records/14887484/files/...   \n",
      "27  https://zenodo.org/api/records/14887484/files/...   \n",
      "24  https://zenodo.org/api/records/14887484/files/...   \n",
      "26  https://zenodo.org/api/records/14887484/files/...   \n",
      "15  https://zenodo.org/api/records/14887484/files/...   \n",
      "28  https://zenodo.org/api/records/14887484/files/...   \n",
      "10  https://zenodo.org/api/records/14887484/files/...   \n",
      "29  https://zenodo.org/api/records/14887484/files/...   \n",
      "14  https://zenodo.org/api/records/14887484/files/...   \n",
      "4   https://zenodo.org/api/records/14887484/files/...   \n",
      "19  https://zenodo.org/api/records/14887484/files/...   \n",
      "18  https://zenodo.org/api/records/14887484/files/...   \n",
      "21  https://zenodo.org/api/records/14887484/files/...   \n",
      "20  https://zenodo.org/api/records/14887484/files/...   \n",
      "13  https://zenodo.org/api/records/14887484/files/...   \n",
      "30  https://zenodo.org/api/records/14887484/files/...   \n",
      "32  https://zenodo.org/api/records/14887484/files/...   \n",
      "33  https://zenodo.org/api/records/14887484/files/...   \n",
      "31  https://zenodo.org/api/records/14887484/files/...   \n",
      "3   https://zenodo.org/api/records/14887484/files/...   \n",
      "36  https://zenodo.org/api/records/14887484/files/...   \n",
      "17  https://zenodo.org/api/records/14887484/files/...   \n",
      "34  https://zenodo.org/api/records/14887484/files/...   \n",
      "16  https://zenodo.org/api/records/14887484/files/...   \n",
      "35  https://zenodo.org/api/records/14887484/files/...   \n",
      "2   https://zenodo.org/api/records/14887484/files/...   \n",
      "\n",
      "                                checksum  \n",
      "5   md5:2ce6b0fcc6f876b600207759a0dc9758  \n",
      "8   md5:9b6a8f38cd6f0ce16a85dfc020c220bf  \n",
      "22  md5:447dbe25eaedc20a75568fd340f8e25d  \n",
      "6   md5:d2f24bbef06809a91d124f0b07cb1034  \n",
      "7   md5:3b741e8138f39932ca6c13ca106fe5d3  \n",
      "23  md5:814e3a79b29da6b605018009f8575a8c  \n",
      "9   md5:dbebdcc8ad7fd1dc7894fe03ebe2a978  \n",
      "11  md5:643894810ac8bfce0f8273cf40d05a7a  \n",
      "1   md5:23559bed5a9023398b431777bdc8a126  \n",
      "12  md5:2f52b49fa8bd983bcf6884d6c4f5e952  \n",
      "25  md5:e1dfe593d40c498e31a6ff5132da5fa4  \n",
      "0   md5:0a2f551db46a9e629bb1d0a0098ae5cd  \n",
      "27  md5:b157900f8cb97cf4aa4f82ef59e2ff6e  \n",
      "24  md5:19c86e2b79bde505112fb6b3d6e38eef  \n",
      "26  md5:3da085b997006205c694b023aefafe7c  \n",
      "15  md5:f9eb5a1bb86caf6a4f2a7563453fc6df  \n",
      "28  md5:7864a0ddb0b676bb053bbcdf12a12525  \n",
      "10  md5:ffd2537b08c58d78eea4bc23a99b3c07  \n",
      "29  md5:66a7e88ddda08626cbf26e182f7eafea  \n",
      "14  md5:6ffaf325257d9af5e43daa68505b1797  \n",
      "4   md5:a79573a02f2c9a9d65c33b3f3a2eaab9  \n",
      "19  md5:95442e14703ba5db573cbf12296d76f4  \n",
      "18  md5:68c5839a6355ea26f979ba781bc26a56  \n",
      "21  md5:d770fcd862d5d7d4d918c59f028e24da  \n",
      "20  md5:35482782eb621df9ec7365d34ccf3d07  \n",
      "13  md5:213c3f1ac83454ec01560d683a26362d  \n",
      "30  md5:10362f805616e391d350b395d9ae30a3  \n",
      "32  md5:a3b9edbd956aee813cdfe97655c3fc9e  \n",
      "33  md5:618fbecca154af288dc1c92246b3d73b  \n",
      "31  md5:4422b9a3be4a1e6cad61da46044a0ca2  \n",
      "3   md5:d1416c058b3961483aac340750ea8726  \n",
      "36  md5:e82f580135522acd2da7277ea9389718  \n",
      "17  md5:b677758820184053d9c1e6714394dd70  \n",
      "34  md5:6543f5ead539a8bbad04593b641af0a1  \n",
      "16  md5:696f8b509a12c0bc898e1b9040a53790  \n",
      "35  md5:178b5a944133ded65d741ea5dc2c5990  \n",
      "2   md5:302e3844ebd041c5f4ed94505eb9a285  \n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame to hold the tar files information for later use.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df_tar_files = pd.DataFrame(tar_files)\n",
    "\n",
    "# Sort the DataFrame by filename alphabetically\n",
    "df_tar_files = df_tar_files.sort_values(by='filename')\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df_tar_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tar files:\n",
      "    index                    filename      size\n",
      "0       5             argo-france.tar   0.00 GB\n",
      "1       8                  aurora.tar   1.73 GB\n",
      "2      22                  beopen.tar   0.20 GB\n",
      "3       6                  civica.tar   0.23 GB\n",
      "4       7                covid-19.tar   2.03 GB\n",
      "5      23                  dariah.tar   0.02 GB\n",
      "6       9                   dh-ch.tar   1.16 GB\n",
      "7      11                     dth.tar   0.01 GB\n",
      "8       1            edih-adria_1.tar   5.86 GB\n",
      "9      12                  egrise.tar   0.02 GB\n",
      "10     25               elixir-gr.tar   0.01 GB\n",
      "11      0       energy-planning_1.tar   6.99 GB\n",
      "12     27                enermaps.tar   1.59 GB\n",
      "13     24              eu-conexus.tar   0.18 GB\n",
      "14     26                     eut.tar   0.21 GB\n",
      "15     15                 eutopia.tar   1.60 GB\n",
      "16     28                 forthem.tar   0.91 GB\n",
      "17     10        heritage-science.tar   0.03 GB\n",
      "18     29                   inria.tar   0.27 GB\n",
      "19     14               iperionhs.tar   0.00 GB\n",
      "20      4               knowmad_1.tar  10.08 GB\n",
      "21     19               knowmad_2.tar  10.07 GB\n",
      "22     18               knowmad_3.tar  10.06 GB\n",
      "23     21               knowmad_4.tar  10.04 GB\n",
      "24     20               knowmad_5.tar   5.37 GB\n",
      "25     13          lifewatch-eric.tar   0.05 GB\n",
      "26     30                     mes.tar   0.38 GB\n",
      "27     32     neanias-atmospheric.tar   0.57 GB\n",
      "28     33           neanias-space.tar   0.73 GB\n",
      "29     31      neanias-underwater.tar   0.02 GB\n",
      "30      3           netherlands_1.tar   3.91 GB\n",
      "31     36                      ni.tar   3.01 GB\n",
      "32     17  north-american-studies.tar   0.36 GB\n",
      "33     34    rural-digital-europe.tar   0.83 GB\n",
      "34     16                 sdsn-gr.tar   0.05 GB\n",
      "35     35                   tunet.tar   0.05 GB\n",
      "36      2               uarctic_1.tar   9.75 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the index of the tar file you want to download:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected file: aurora.tar\n",
      "Download link: https://zenodo.org/api/records/14887484/files/aurora.tar/content\n",
      "Checksum: md5:9b6a8f38cd6f0ce16a85dfc020c220bf\n"
     ]
    }
   ],
   "source": [
    "# Ask the user to select a tar file by its index\n",
    "print(\"Available tar files:\")\n",
    "print(df_tar_files[['filename', 'size']].reset_index())\n",
    "\n",
    "selected_index = int(input(\"Enter the index of the tar file you want to download: \"))\n",
    "\n",
    "# Get the selected tar file's download link and checksum\n",
    "selected_file = df_tar_files.iloc[selected_index]\n",
    "downloadlink = selected_file['downloadlink']\n",
    "checksum = selected_file['checksum']\n",
    "\n",
    "print(f\"Selected file: {selected_file['filename']}\")\n",
    "print(f\"Download link: {downloadlink}\")\n",
    "print(f\"Checksum: {checksum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://zenodo.org/records/14887484/files/aurora.tar?download=1\n",
      "File Name: aurora.tar\n",
      "Download Path: ./data/01_input/aurora.tar\n",
      "Extraction Path: ./data/02_extracted\n"
     ]
    }
   ],
   "source": [
    "# Path Variables\n",
    "\n",
    "# Path to save the downloaded tar file using file_name variable\n",
    "download_path = f\"./data/01_input/{selected_file}\"\n",
    "\n",
    "# Path to save the extracted files\n",
    "extraction_path = \"./data/02_extracted\"\n",
    "\n",
    "print(f\"Download Path File: {download_path}\")\n",
    "print(f\"Extraction Path Folder: {extraction_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the tar file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:191\u001b[39m, in \u001b[36m_pydevd_sys_monitoring_cython._get_unhandled_exception_frame\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mAttributeError\u001b[39m: Not the same exception",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m os.makedirs(os.path.dirname(download_path), exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Download the tar file\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(download_path, \u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m     10\u001b[39m     file.write(response.content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/OpenAIRE-tools/env/lib/python3.12/site-packages/requests/api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/OpenAIRE-tools/env/lib/python3.12/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/OpenAIRE-tools/env/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/OpenAIRE-tools/env/lib/python3.12/site-packages/requests/sessions.py:746\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    743\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    745\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[32m--> \u001b[39m\u001b[32m746\u001b[39m     \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/OpenAIRE-tools/env/lib/python3.12/site-packages/requests/models.py:902\u001b[39m, in \u001b[36mResponse.content\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    900\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    901\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m902\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[33;43mb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    904\u001b[39m \u001b[38;5;28mself\u001b[39m._content_consumed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    905\u001b[39m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[32m    906\u001b[39m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/OpenAIRE-tools/env/lib/python3.12/site-packages/requests/models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/OpenAIRE-tools/env/lib/python3.12/site-packages/urllib3/response.py:1066\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1064\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1065\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1066\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1068\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[32m   1069\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/OpenAIRE-tools/env/lib/python3.12/site-packages/urllib3/response.py:942\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt, decode_content, cache_content)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread\u001b[39m(\n\u001b[32m    917\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    918\u001b[39m     amt: \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    919\u001b[39m     decode_content: \u001b[38;5;28mbool\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    920\u001b[39m     cache_content: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    921\u001b[39m ) -> \u001b[38;5;28mbytes\u001b[39m:\n\u001b[32m    922\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    923\u001b[39m \u001b[33;03m    Similar to :meth:`http.client.HTTPResponse.read`, but with two additional\u001b[39;00m\n\u001b[32m    924\u001b[39m \u001b[33;03m    parameters: ``decode_content`` and ``cache_content``.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m \u001b[33;03m        set.)\u001b[39;00m\n\u001b[32m    941\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_decoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    943\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m decode_content \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    944\u001b[39m         decode_content = \u001b[38;5;28mself\u001b[39m.decode_content\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/OpenAIRE-tools/env/lib/python3.12/site-packages/urllib3/response.py:456\u001b[39m, in \u001b[36mBaseHTTPResponse._init_decoder\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    451\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    452\u001b[39m \u001b[33;03mSet-up the _decoder attribute if necessary.\u001b[39;00m\n\u001b[32m    453\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    454\u001b[39m \u001b[38;5;66;03m# Note: content-encoding value should be case-insensitive, per RFC 7230\u001b[39;00m\n\u001b[32m    455\u001b[39m \u001b[38;5;66;03m# Section 3.2\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m content_encoding = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent-encoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.lower()\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.CONTENT_DECODERS:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen _collections_abc>:807\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(self, key, default)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<stringsource>:69\u001b[39m, in \u001b[36mcfunc.to_py.__Pyx_CFunc_4904d5__29_pydevd_sys_monitoring_cython_object__lParen__etc_to_py_4code_11instruction_3exc.wrap\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:920\u001b[39m, in \u001b[36m_pydevd_sys_monitoring_cython._unwind_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:198\u001b[39m, in \u001b[36m_pydevd_sys_monitoring_cython._get_unhandled_exception_frame\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen posixpath>:125\u001b[39m, in \u001b[36msplitext\u001b[39m\u001b[34m(p)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen genericpath>:133\u001b[39m, in \u001b[36m_splitext\u001b[39m\u001b[34m(p, sep, altsep, extsep)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Ensure the directory for the download path exists\n",
    "os.makedirs(os.path.dirname(download_path), exist_ok=True)\n",
    "\n",
    "# Download the selected tar file\n",
    "response = requests.get(downloadlink, stream=True)\n",
    "with open(download_path, 'wb') as file:\n",
    "    for chunk in response.iter_content(chunk_size=8192):\n",
    "        file.write(chunk)\n",
    "\n",
    "print(f\"Download complete: {download_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "# Function to calculate the checksum of a file\n",
    "def calculate_checksum(file_path, algorithm):\n",
    "    hash_func = hashlib.new(algorithm)\n",
    "    with open(file_path, 'rb') as f:\n",
    "        while chunk := f.read(8192):\n",
    "            hash_func.update(chunk)\n",
    "    return hash_func.hexdigest()\n",
    "\n",
    "# Extract the checksum algorithm and value\n",
    "checksum_parts = checksum.split(':', 1)\n",
    "checksum_algorithm = checksum_parts[0]\n",
    "expected_checksum = checksum_parts[1]\n",
    "\n",
    "# Calculate the checksum of the downloaded file\n",
    "calculated_checksum = calculate_checksum(download_path, algorithm=checksum_algorithm)\n",
    "\n",
    "# Compare the calculated checksum with the provided checksum\n",
    "if calculated_checksum == expected_checksum:\n",
    "    print(\"Checksum verification passed.\")\n",
    "else:\n",
    "    print(\"Checksum verification failed.\")\n",
    "    print(f\"Expected: {expected_checksum}\")\n",
    "    print(f\"Calculated: {calculated_checksum}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract the tar file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(extraction_path, exist_ok=True)\n",
    "\n",
    "# Extract the tar file\n",
    "with tarfile.open(download_path, 'r') as tar:\n",
    "    tar.extractall(path=extraction_path)\n",
    "\n",
    "print(\"Extraction complete.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 19\n",
      "First 5 files:\n",
      "aurora\n",
      "Subdirectories:\n",
      "aurora\n",
      "Latest subdirectory: aurora\n",
      "Latest extraction path: ./data/02_extracted/aurora\n"
     ]
    }
   ],
   "source": [
    "# List the extracted files\n",
    "extracted_files = os.listdir(extraction_path)\n",
    "\n",
    "# count he number of files in the extracted folder\n",
    "num_files = len(extracted_path)\n",
    "print(f\"Number of files: {num_files}\")\n",
    "\n",
    "# print the first 5 files\n",
    "print(\"First 5 files:\")\n",
    "for file in extracted_files[:5]:\n",
    "    print(file) \n",
    "\n",
    "# print the added subdirectories\n",
    "subdirectories = [file for file in extracted_files if os.path.isdir(os.path.join(extraction_path, file))]\n",
    "print(\"Subdirectories:\")\n",
    "for subdirectory in subdirectories:\n",
    "    print(subdirectory)\n",
    "\n",
    "# print the latest added subdirectory based on date modified\n",
    "latest_subdirectory = sorted(subdirectories, key=lambda x: os.path.getmtime(os.path.join(extraction_path, x)))[-1]\n",
    "print(f\"Latest subdirectory: {latest_subdirectory}\")\n",
    "\n",
    "# make varable for the path to the latest subdirectory\n",
    "latest_extraction_path = os.path.join(extraction_path, latest_subdirectory)\n",
    "\n",
    "# print the path of the latest extraction path\n",
    "print(f\"Latest extraction path: {latest_extraction_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL Schema:\n",
      "authors: STRUCT(fullName VARCHAR, \"name\" VARCHAR, rank BIGINT, surname VARCHAR, pid STRUCT(id STRUCT(scheme VARCHAR, \"value\" VARCHAR), provenance STRUCT(provenance VARCHAR, trust VARCHAR)))[]\n",
      "collectedFrom: STRUCT(\"key\" VARCHAR, \"value\" VARCHAR)[]\n",
      "communities: STRUCT(code VARCHAR, \"label\" VARCHAR, provenance STRUCT(provenance VARCHAR, trust VARCHAR)[])[]\n",
      "contributors: JSON[]\n",
      "countries: JSON[]\n",
      "coverages: JSON[]\n",
      "dateOfCollection: VARCHAR\n",
      "descriptions: VARCHAR[]\n",
      "documentationUrls: JSON[]\n",
      "formats: JSON[]\n",
      "id: VARCHAR\n",
      "indicators: STRUCT(citationImpact STRUCT(citationClass VARCHAR, citationCount DOUBLE, impulse DOUBLE, impulseClass VARCHAR, influence DOUBLE, influenceClass VARCHAR, popularity DOUBLE, popularityClass VARCHAR), usageCounts STRUCT(downloads BIGINT, \"views\" BIGINT))\n",
      "instances: STRUCT(alternateIdentifiers STRUCT(scheme VARCHAR, \"value\" VARCHAR)[], collectedFrom STRUCT(\"key\" VARCHAR, \"value\" VARCHAR), hostedBy STRUCT(\"key\" VARCHAR, \"value\" VARCHAR), pids STRUCT(scheme VARCHAR, \"value\" VARCHAR)[], publicationDate DATE, refereed VARCHAR, \"type\" VARCHAR, urls VARCHAR[], accessRight STRUCT(code VARCHAR, \"label\" VARCHAR, scheme VARCHAR), license VARCHAR)[]\n",
      "language: STRUCT(code VARCHAR, \"label\" VARCHAR)\n",
      "lastUpdateTimeStamp: BIGINT\n",
      "mainTitle: VARCHAR\n",
      "organizations: STRUCT(id VARCHAR, legalName VARCHAR, pids STRUCT(scheme VARCHAR, \"value\" VARCHAR)[])[]\n",
      "originalIds: VARCHAR[]\n",
      "pids: STRUCT(scheme VARCHAR, \"value\" VARCHAR)[]\n",
      "programmingLanguage: VARCHAR\n",
      "publicationDate: DATE\n",
      "publisher: VARCHAR\n",
      "sources: JSON[]\n",
      "subjects: STRUCT(subject STRUCT(scheme VARCHAR, \"value\" VARCHAR))[]\n",
      "type: VARCHAR\n",
      "bestAccessRight: STRUCT(code VARCHAR, \"label\" VARCHAR, scheme VARCHAR)\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import os\n",
    "\n",
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Get the first 3 gzipped JSON files from the latest_extraction_path\n",
    "gz_files = [file for file in os.listdir(latest_extraction_path) if file.endswith(\".gz\")][:3]\n",
    "\n",
    "# Load the gzipped JSON files directly into DuckDB\n",
    "for gz_file in gz_files:\n",
    "    gz_file_path = os.path.join(latest_extraction_path, gz_file)\n",
    "    con.execute(\"CREATE OR REPLACE TEMP TABLE temp_table AS SELECT * FROM read_json_auto(?, compression='gzip')\", [gz_file_path])\n",
    "\n",
    "# Generate the SQL schema from the temporary table\n",
    "schema = con.execute(\"DESCRIBE temp_table\").fetchall()\n",
    "\n",
    "# Print the schema\n",
    "print(\"SQL Schema:\")\n",
    "for column in schema:\n",
    "    print(f\"{column[0]}: {column[1]}\")\n",
    "\n",
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
